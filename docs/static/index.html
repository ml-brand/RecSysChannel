<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Рекомендательная [RecSys Channel] — статическая версия (стр. 1/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-07T07%3A21%3A08Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-07T07%3A21%3A08Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-07T07%3A21%3A08Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Рекомендательная [RecSys Channel]</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+3NrSk0BmQ-QzZTMy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link disabled" href="#">←</a>
        <a class="page-link current" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-2.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="218" data-search="onerec-think: in-text reasoning for generative recommendation сегодня обсудим работу, в которой продолжается история с генеративными рекомендациями от kuaishou. авторы по-прежнему хотят заменить классический рекомендательный стек одной генеративной моделью, но теперь ещё и добавить туда llm-ный ризонинг и диалог. onerec хорошо предсказывает следующий айтем по истории пользователя, но остаётся узкодоменной моделью: у неё нет широкого world knowledge, как у llm, и нет развитых механизмов следования инструкциям и рассуждения. поэтому авторы добавляют в onerec-think ризонинг, рассчитывая улучшить точность рекомендаций. причём он используется непосредственно в процессе предсказания следующего айтема. тут возникают две сложности. во-первых, llm изначально не знает, что такое рекомендательные айтемы (видео, треки и прочее). во-вторых, даже если заставить её «думать», она не умеет думать именно в рекомендательном домене: длинные и шумные истории пользователей ломают красивый ризонинг. авторы решают эти проблемы в три этапа. сначала делают itemic alignment. в словарь добавляют айтем-токены (3×8к = 24к новых токенов) и учат модель понимать айтем-токены в одном контексте с текстовыми. делают это аккуратно: сначала замораживают бэкбон и обучают только эмбеддинги новых токенов, чтобы сохранить языковые способности модели, а затем размораживают все параметры и обучают модель совместно. используют несколько задач, включая интерпретацию пользовательской истории, sequential next-item prediction и декодирование айтемов в текстовые описания. дальше — reasoning activation. просто взять полную историю и попросить «подумай» не работает: слишком много шума и длинный контекст. поэтому ризонинг-траектории извлекают хитрее. берут таргет-айтем и с помощью внешней модели близости айтемов g(·,·) достают top-k (k=10) самых релевантных айтемов из истории пользователя. на этом подмножестве модель способна сгенерировать осмысленное объяснение того, почему пользователь взаимодействовал с таргетным айтемом. эти объяснения затем используют как sft-данные: уже на полной истории учат сначала генерировать ризонинг-трейс, а потом — следующий айтем. и финальный этап — reasoning enhancement. модель сэмплит несколько объяснений, а дальше под каждое считают reward — не в бинарной форме «угадал / не угадал», а на основе степени совпадения семантических токенов предсказанных кандидатов с таргетным айтемом. для этого используется beam search по продолжениям. в результате ризонинг-траектории, ведущие к более точным предсказаниям, получают больший вес и становятся более вероятными. в статье обсуждают, как такую модель можно внедрить при больших rps. авторы предлагают схему think-ahead: вычислительно тяжёлую часть — генерацию ризонинга и первых шагов декодирования айтем-токенов — считают офлайн и сохраняют для пользователя набор возможных префиксов. в онлайне обычный onerec ограничивается этим множеством и быстро достраивает финальный айтем. за счёт этого снижается стоимость инференса и одновременно в продакшн-систему переносятся знания llm, зашитые в ризонинг-префиксы. в результате модель не только генерирует объяснения и учитывает текстовые ограничения, но и сохраняет качество предсказания следующего айтема, что подтверждают онлайн-эксперименты. @recsyschannel разбор подготовил ❣ артём матвеев onerec-think: in-text reasoning for generative recommendation сегодня обсудим работу , в которой продолжается история с генеративными рекомендациями от kuaishou. авторы по-прежнему хотят заменить классический рекомендательный стек одной генеративной моделью, но теперь ещё и добавить туда llm-ный ризонинг и диалог. onerec хорошо предсказывает следующий айтем по истории пользователя, но остаётся узкодоменной моделью: у неё нет широкого world knowledge, как у llm, и нет развитых механизмов следования инструкциям и рассуждения. поэтому авторы добавляют в onerec-think ризонинг, рассчитывая улучшить точность рекомендаций. причём он используется непосредственно в процессе предсказания следующего айтема. тут возникают две сложности. во-первых, llm изначально не знает, что такое рекомендательные айтемы (видео, треки и прочее). во-вторых, даже если заставить её «думать», она не умеет думать именно в рекомендательном домене: длинные и шумные истории пользователей ломают красивый ризонинг. авторы решают эти проблемы в три этапа. сначала делают itemic alignment . в словарь добавляют айтем-токены (3×8к = 24к новых токенов) и учат модель понимать айтем-токены в одном контексте с текстовыми. делают это аккуратно: сначала замораживают бэкбон и обучают только эмбеддинги новых токенов, чтобы сохранить языковые способности модели, а затем размораживают все параметры и обучают модель совместно. используют несколько задач, включая интерпретацию пользовательской истории, sequential next-item prediction и декодирование айтемов в текстовые описания. дальше — reasoning activation . просто взять полную историю и попросить «подумай» не работает: слишком много шума и длинный контекст. поэтому ризонинг-траектории извлекают хитрее. берут таргет-айтем и с помощью внешней модели близости айтемов g(·,·) достают top-k (k=10) самых релевантных айтемов из истории пользователя. на этом подмножестве модель способна сгенерировать осмысленное объяснение того, почему пользователь взаимодействовал с таргетным айтемом. эти объяснения затем используют как sft-данные: уже на полной истории учат сначала генерировать ризонинг-трейс, а потом — следующий айтем. и финальный этап — reasoning enhancement. модель сэмплит несколько объяснений, а дальше под каждое считают reward — не в бинарной форме «угадал / не угадал», а на основе степени совпадения семантических токенов предсказанных кандидатов с таргетным айтемом. для этого используется beam search по продолжениям. в результате ризонинг-траектории, ведущие к более точным предсказаниям, получают больший вес и становятся более вероятными. в статье обсуждают, как такую модель можно внедрить при больших rps. авторы предлагают схему think-ahead: вычислительно тяжёлую часть — генерацию ризонинга и первых шагов декодирования айтем-токенов — считают офлайн и сохраняют для пользователя набор возможных префиксов. в онлайне обычный onerec ограничивается этим множеством и быстро достраивает финальный айтем. за счёт этого снижается стоимость инференса и одновременно в продакшн-систему переносятся знания llm, зашитые в ризонинг-префиксы. в результате модель не только генерирует объяснения и учитывает текстовые ограничения, но и сохраняет качество предсказания следующего айтема, что подтверждают онлайн-эксперименты. @recsyschannel разбор подготовил ❣ артём матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-02-04T11:02:16+00:00" href="./posts/218.html">2026-02-04 11:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>OneRec-Think: In-Text Reasoning for Generative Recommendation</strong><br><br>Сегодня обсудим <a href="https://arxiv.org/abs/2510.11639v1" rel="nofollow noopener noreferrer">работу</a>, в которой продолжается история с генеративными рекомендациями от Kuaishou. Авторы по-прежнему хотят заменить классический рекомендательный стек одной генеративной моделью, но теперь ещё и добавить туда LLM-ный ризонинг и диалог.<br><br>OneRec хорошо предсказывает следующий айтем по истории пользователя, но остаётся узкодоменной моделью: у неё нет широкого world knowledge, как у LLM, и нет развитых механизмов следования инструкциям и рассуждения. Поэтому авторы добавляют в OneRec-Think ризонинг, рассчитывая улучшить точность рекомендаций. Причём он используется непосредственно в процессе предсказания следующего айтема.<br><br>Тут возникают две сложности. Во-первых, LLM изначально не знает, что такое рекомендательные айтемы (видео, треки и прочее). Во-вторых, даже если заставить её «думать», она не умеет думать именно в рекомендательном домене: длинные и шумные истории пользователей ломают красивый ризонинг.<br><br>Авторы решают эти проблемы в три этапа.<br><br>Сначала делают <strong>Itemic Alignment</strong>. В словарь добавляют айтем-токены (3×8К = 24К новых токенов) и учат модель понимать айтем-токены в одном контексте с текстовыми. Делают это аккуратно: сначала замораживают бэкбон и обучают только эмбеддинги новых токенов, чтобы сохранить языковые способности модели, а затем размораживают все параметры и обучают модель совместно. Используют несколько задач, включая интерпретацию пользовательской истории, sequential next-item prediction и декодирование айтемов в текстовые описания.<br><br>Дальше — <strong>Reasoning Activation</strong>. Просто взять полную историю и попросить «подумай» не работает: слишком много шума и длинный контекст. Поэтому ризонинг-траектории извлекают хитрее. Берут таргет-айтем и с помощью внешней модели близости айтемов g(·,·) достают top-k (k=10) самых релевантных айтемов из истории пользователя. На этом подмножестве модель способна сгенерировать осмысленное объяснение того, почему пользователь взаимодействовал с таргетным айтемом. Эти объяснения затем используют как SFT-данные: уже на полной истории учат сначала генерировать ризонинг-трейс, а потом — следующий айтем.<br><br>И финальный этап — <strong>Reasoning Enhancement.</strong> Модель сэмплит несколько объяснений, а дальше под каждое считают reward — не в бинарной форме «угадал / не угадал», а на основе степени совпадения семантических токенов предсказанных кандидатов с таргетным айтемом. Для этого используется beam search по продолжениям. В результате ризонинг-траектории, ведущие к более точным предсказаниям, получают больший вес и становятся более вероятными.<br><br>В статье обсуждают, как такую модель можно внедрить при больших RPS. Авторы предлагают схему Think-Ahead: вычислительно тяжёлую часть — генерацию ризонинга и первых шагов декодирования айтем-токенов — считают офлайн и сохраняют для пользователя набор возможных префиксов.<br><br>В онлайне обычный OneRec ограничивается этим множеством и быстро достраивает финальный айтем. За счёт этого снижается стоимость инференса и одновременно в продакшн-систему переносятся знания LLM, зашитые в ризонинг-префиксы.<br><br>В результате модель не только генерирует объяснения и учитывает текстовые ограничения, но и сохраняет качество предсказания следующего айтема, что подтверждают онлайн-эксперименты.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Артём Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/218_480.webp" srcset="../assets/media/thumbs/218_480.webp 480w, ../assets/media/218.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="218" data-image-index="0" /></div></div>
      <div class="actions">
        <span>593 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/218" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/218.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="217" data-search="kvzap: fast, adaptive, and faithful kv cache pruning сегодня посмотрим на совсем свежую статью от nvidia о сжатии kv-кэша. kv-кэш — это сохраненные k- и v-стейты трансформера для последующей авторегрессивной генерации токенов в декодере. в первую очередь проблема сжатия возникает на стадии генерации в llm, однако она актуальна и для ускорения инференса рекомендательных моделей, например, имеющих encoder-decoder-архитектуру. размер kv-кэша линейно зависит от числа слоёв трансформера l, от числа аттеншн-голов h, от длины входной последовательности t и от размерности векторов d. таким образом, он имеет размерность (2, l, h, t, d), где 2 соответствует хранению k- и v-кэшей в одном тензоре. сжатие по l-размерности достигается чередованием обычных mha-слоёв и слоёв со sliding window attention (swa): gpt-oss-120b, gemma3, kimi-linear, и др. для сжатия по размерности h применяют grouped query attention (gqa), в котором одни и те же kv-головы используются в нескольких q-головах: llama3, glm 4.5, qwen3-235b-a22b. вдоль размерности d сжатия добиваются с использованием хранения латентных представлений kv-векторов значительно меньшей размерности — multi-head latent attention (mla): deepseek v2. текущая sota для сжатия вдоль размерности t — kvzip, который: 1. получает входной промпт пользователя; 2. просит модель его повторить, аугментируя промпт следующим образом: «user: &lt;input prompt&gt;. repeat the previous context exactly. assistant: »; 3. для каждой kv-головы для каждого вектора k_i из input prompt запоминают наибольший по длине повторённого промпта вес аттеншна (а в случае gqa максимум берётся и по группе q-голов); 4. фиксированный процент k_i и v_i, соответствующих наименьшим запомненным весам, удаляются; 5. сжатый промпт подаётся модели. во-первых, такая схема скоринга очень дорога. во-вторых, она применима только к стадии cache prefilling — стадия cache decoding сохраняется целиком. последняя проблема особенно актуальна в контексте рассуждающих моделей, которые на стадии декодинга генерируют тысячи токенов. в работе предлагают дистиллировать слегка модифицированные скоры kvzip в легковесный mlp. для каждого слоя трансформера и каждого входного скрытого состояния mlp предсказывает вектор скоров из h (число kv-голов) компонент, после чего откидываются kv-пары, скоры которых не превосходят некоторый порог. таким образом, степень сжатия зависит от информативности промпта. локальный контекст из ближайших 128 токенов, однако, сохраняется полностью. mlp обучается поверх обученной модели на специальном датасете, содержащем целевые скоры kv-пар. поскольку mlp не добавляет значительной вычислительной сложности и применяется к входным токенам поточечно, kvzap можно использовать как во время prefilling’a, так и во время декодинга. сжатие prefilling-стадии также становится дешевле. эвалятся авторы на qwen3-8b, llama-3.1-8b-instruct, и qwen3-32b, kv-кэш удаётся сжать в 2–4 раза при незначительных потерях качества. @recsyschannel разбор подготовил ❣ сергей макеев kvzap: fast, adaptive, and faithful kv cache pruning сегодня посмотрим на совсем свежую статью от nvidia о сжатии kv-кэша. kv-кэш — это сохраненные k- и v-стейты трансформера для последующей авторегрессивной генерации токенов в декодере. в первую очередь проблема сжатия возникает на стадии генерации в llm, однако она актуальна и для ускорения инференса рекомендательных моделей, например, имеющих encoder-decoder-архитектуру. размер kv-кэша линейно зависит от числа слоёв трансформера l, от числа аттеншн-голов h, от длины входной последовательности t и от размерности векторов d. таким образом, он имеет размерность (2, l, h, t, d), где 2 соответствует хранению k- и v-кэшей в одном тензоре. сжатие по l-размерности достигается чередованием обычных mha-слоёв и слоёв со sliding window attention (swa): gpt-oss-120b , gemma3 , kimi-linear , и др. для сжатия по размерности h применяют grouped query attention (gqa), в котором одни и те же kv-головы используются в нескольких q-головах: llama3 , glm 4.5 , qwen3-235b-a22b . вдоль размерности d сжатия добиваются с использованием хранения латентных представлений kv-векторов значительно меньшей размерности — multi-head latent attention (mla): deepseek v2 . текущая sota для сжатия вдоль размерности t — kvzip , который: 1. получает входной промпт пользователя; 2. просит модель его повторить, аугментируя промпт следующим образом: «user: &amp;lt;input prompt&amp;gt; . repeat the previous context exactly. assistant: » ; 3. для каждой kv-головы для каждого вектора k_i из input prompt запоминают наибольший по длине повторённого промпта вес аттеншна (а в случае gqa максимум берётся и по группе q-голов); 4. фиксированный процент k_i и v_i, соответствующих наименьшим запомненным весам, удаляются; 5. сжатый промпт подаётся модели. во-первых, такая схема скоринга очень дорога. во-вторых, она применима только к стадии cache prefilling — стадия cache decoding сохраняется целиком. последняя проблема особенно актуальна в контексте рассуждающих моделей, которые на стадии декодинга генерируют тысячи токенов. в работе предлагают дистиллировать слегка модифицированные скоры kvzip в легковесный mlp. для каждого слоя трансформера и каждого входного скрытого состояния mlp предсказывает вектор скоров из h (число kv-голов) компонент, после чего откидываются kv-пары, скоры которых не превосходят некоторый порог. таким образом, степень сжатия зависит от информативности промпта. локальный контекст из ближайших 128 токенов, однако, сохраняется полностью. mlp обучается поверх обученной модели на специальном датасете, содержащем целевые скоры kv-пар. поскольку mlp не добавляет значительной вычислительной сложности и применяется к входным токенам поточечно, kvzap можно использовать как во время prefilling’a, так и во время декодинга. сжатие prefilling-стадии также становится дешевле. эвалятся авторы на qwen3-8b, llama-3.1-8b-instruct, и qwen3-32b, kv-кэш удаётся сжать в 2–4 раза при незначительных потерях качества. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-29T09:14:01+00:00" href="./posts/217.html">2026-01-29 09:14 UTC</a></div>
      </div>
      <div class="post-body"><strong>KVzap: Fast, Adaptive, and Faithful KV Cache Pruning<br></strong><br>Сегодня посмотрим на совсем свежую <a href="https://arxiv.org/abs/2601.07891v1" rel="nofollow noopener noreferrer">статью</a> от NVIDIA о сжатии KV-кэша. KV-кэш — это сохраненные K- и V-стейты трансформера для последующей авторегрессивной генерации токенов в декодере. В первую очередь проблема сжатия возникает на стадии генерации в LLM, однако она актуальна и для ускорения инференса рекомендательных моделей, например, имеющих encoder-decoder-архитектуру. <br><br>Размер KV-кэша линейно зависит от числа слоёв трансформера L, от числа аттеншн-голов H, от длины входной последовательности T и от размерности векторов D. Таким образом, он имеет размерность (2, L, H, T, D), где 2 соответствует хранению K- и V-кэшей в одном тензоре. Сжатие по L-размерности достигается чередованием обычных MHA-слоёв и слоёв со Sliding Window Attention (SWA): <a href="https://openai.com/index/introducing-gpt-oss/" rel="nofollow noopener noreferrer">GPT-OSS-120B</a>, <a href="https://arxiv.org/abs/2503.19786" rel="nofollow noopener noreferrer">Gemma3</a>, <a href="https://arxiv.org/abs/2510.26692" rel="nofollow noopener noreferrer">Kimi-Linear</a>, и др. Для сжатия по размерности H применяют Grouped Query Attention (GQA), в котором одни и те же KV-головы используются в нескольких Q-головах: <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">Llama3</a>, <a href="https://arxiv.org/abs/2508.06471" rel="nofollow noopener noreferrer">GLM 4.5</a>, <a href="https://arxiv.org/abs/2505.09388" rel="nofollow noopener noreferrer">Qwen3-235B-A22B</a>. Вдоль размерности D сжатия добиваются с использованием хранения латентных представлений KV-векторов значительно меньшей размерности — Multi-head Latent Attention (MLA): <a href="https://arxiv.org/abs/2405.04434" rel="nofollow noopener noreferrer">DeepSeek V2</a>. <br><br>Текущая SOTA для сжатия вдоль размерности T — <a href="https://arxiv.org/abs/2505.23416" rel="nofollow noopener noreferrer">KVzip</a>, который: <br><br>1. получает входной промпт пользователя; <br>2. просит модель его повторить, аугментируя промпт следующим образом: <em>«user:</em> &lt;input prompt&gt;<em>. Repeat the previous context exactly. assistant: »</em>;<br>3. для каждой KV-головы для каждого вектора k_i из input prompt запоминают наибольший по длине повторённого промпта вес аттеншна (а в случае GQA максимум берётся и по группе Q-голов);<br>4. фиксированный процент K_i и v_i, соответствующих наименьшим запомненным весам, удаляются;<br>5. сжатый промпт подаётся модели.<br> <br>Во-первых, такая схема скоринга очень дорога. Во-вторых, она применима только к стадии cache prefilling — стадия cache decoding сохраняется целиком. Последняя проблема особенно актуальна в контексте рассуждающих моделей, которые на стадии декодинга генерируют тысячи токенов. <br><br>В работе предлагают дистиллировать слегка модифицированные скоры KVzip в легковесный MLP. Для каждого слоя трансформера и каждого входного скрытого состояния MLP предсказывает вектор скоров из H (число KV-голов) компонент, после чего откидываются KV-пары, скоры которых не превосходят некоторый порог. Таким образом, степень сжатия зависит от информативности промпта. Локальный контекст из ближайших 128 токенов, однако, сохраняется полностью. MLP обучается поверх обученной модели на специальном датасете, содержащем целевые скоры KV-пар. <br><br>Поскольку MLP не добавляет значительной вычислительной сложности и применяется к входным токенам поточечно, KVzap можно использовать как во время prefilling’a, так и во время декодинга. Сжатие prefilling-стадии также становится дешевле. <br><br>Эвалятся авторы на Qwen3-8B, Llama-3.1-8B-Instruct, и Qwen3-32B, KV-кэш удаётся сжать в 2–4 раза при незначительных потерях качества. <br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/217_480.webp" srcset="../assets/media/thumbs/217_480.webp 480w, ../assets/media/217.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="217" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 084 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/217" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/217.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="210" data-search="orthogonal low rank embedding stabilization сегодня разбираем статью от авторов из netflix о стабилизации обучаемых эмбедов пользователя/документа. в двухбашенной архитектуре с поздним связыванием классическая проблема при дообучении — «разворот» пространств эмбеддингов пользователя/документа при сохранении результирующего dot product. это происходит из-за того, что отдельные координаты эмбедов (например 1-я или i-ная координата вектора документа) не имеют никакого специального смысла, важно лишь их суммарное взаимодействие с соответствующим вектором пользователя. из-за нестабильности приходится пересчитывать эмбеддинги всех айтемов после каждого этапа дообучения модели, что увеличивает затраты на вычисления. также необходимо синхронизировать версии пользовательской и документной частей моделей, что зачастую невозможно. авторы статьи предлагают элегантное решение проблемы, комбинируя две идеи: - эффективное сингулярное разложение матрицы взаимодействий пользователя/документа; - приведение к выбранному референсному пространству с помощью ортогональной задачи прокруста. обозначим таблицу эмбеддингов документов как t (размерностью n * e, где n — количество документов, а e — размерность эмбеддингов), а таблицу эмбеддингов пользователей — как w (размерностью m * e, где m — количество пользователей). тогда их произведение будет иметь смысл матрицы взаимодействий (x=twᵀ). сами документные и пользовательские эмбеддинги могут быть нестабильны при обучении: даже небольшие пертурбации в начальных условиях приводят к существенно разным результатам. при этом сингулярное разложение матрицы взаимодействий остаётся единственным с точностью до знаков сингулярных векторов. однако получить напрямую svd-разложение матрицы x вычислительно сложно: o(mn²). в статье предлагают воспользоваться тем, что матрица x — это произведение двух низкоранговых матриц twᵀ, и сделать qr-разложение каждой из них, что линейно по сложности относительно n и m. а затем сделать svd-разложение уже низкоранговой (e * e) матрицы rₜrwᵀ, svd(rₜrwᵀ)=uᵣsvᵣᵀ. кроме самого сингулярного разложения x потребуются ещё и матрицы перехода в новое пространство для t и w (mₜ и mw соответственно), такие чтоб tmₜ = us¹ᐟ², а wmw = vs¹ᐟ², что сохранит матрицу взаимодействий x: tmₜ(wmw)ᵀ = usv = twᵀ. однако, имея сингулярное разложение rₜrwᵀ, их вычислить несложно: mₜ = rwᵀ vᵣ s⁻¹ᐟ²; mw = rₜᵀ uᵣ s⁻¹ᐟ². второй шаг — перевести полученное стандартизированное представление эмбеддингов к некому референсному пространству. в качестве такого можно выбрать результат произвольной версии модели (например, первый) и зафиксировать его. дальше задача сводится к поиску матрицы, отображающей получившееся на очередном шаге дообучения представление в референсное пространство. хотя такое отображение можно искать среди произвольных матриц, удобно ограничить поиск только среди ортогональных. формально, имея матрицы tₖ (текущее пространство) и t₀ (референсное пространство) требуется найти такую ортогональную матрицу r, что rtₖ ~= t₀. эта задача называется ортогональной задачей прокруста. финально, получив матрицы отображения на первом (mₜ и mw) и втором (r) шагах, мы имеем преобразование, которое стабилизует пространства эмбеддингов документов (mₜr) и пользователей (mwr). так как преобразование ортогональное, то значения матрицы взаимодействий не меняются. при этом размерность матрицы — e * e, что делает её хранение и применение очень лёгкой операцией, которую можно добавить последним слоем нейросети. предложенный в статье способ не зависит от выбранной модели и легко добавляется в любой пайплайн обучения или инференса, что позволяет стабилизировать эмбеды при дообучении. @recsyschannel разбор подготовил ❣ артём ваншулин orthogonal low rank embedding stabilization сегодня разбираем статью от авторов из netflix о стабилизации обучаемых эмбедов пользователя/документа. в двухбашенной архитектуре с поздним связыванием классическая проблема при дообучении — «разворот» пространств эмбеддингов пользователя/документа при сохранении результирующего dot product. это происходит из-за того, что отдельные координаты эмбедов (например 1-я или i-ная координата вектора документа) не имеют никакого специального смысла, важно лишь их суммарное взаимодействие с соответствующим вектором пользователя. из-за нестабильности приходится пересчитывать эмбеддинги всех айтемов после каждого этапа дообучения модели, что увеличивает затраты на вычисления. также необходимо синхронизировать версии пользовательской и документной частей моделей, что зачастую невозможно. авторы статьи предлагают элегантное решение проблемы, комбинируя две идеи: - эффективное сингулярное разложение матрицы взаимодействий пользователя/документа; - приведение к выбранному референсному пространству с помощью ортогональной задачи прокруста. обозначим таблицу эмбеддингов документов как t (размерностью n * e, где n — количество документов, а e — размерность эмбеддингов), а таблицу эмбеддингов пользователей — как w (размерностью m * e, где m — количество пользователей). тогда их произведение будет иметь смысл матрицы взаимодействий (x=twᵀ). сами документные и пользовательские эмбеддинги могут быть нестабильны при обучении: даже небольшие пертурбации в начальных условиях приводят к существенно разным результатам. при этом сингулярное разложение матрицы взаимодействий остаётся единственным с точностью до знаков сингулярных векторов. однако получить напрямую svd-разложение матрицы x вычислительно сложно: o(mn²). в статье предлагают воспользоваться тем, что матрица x — это произведение двух низкоранговых матриц twᵀ, и сделать qr-разложение каждой из них, что линейно по сложности относительно n и m. а затем сделать svd-разложение уже низкоранговой (e * e) матрицы rₜrwᵀ, svd(rₜrwᵀ)=uᵣsvᵣᵀ. кроме самого сингулярного разложения x потребуются ещё и матрицы перехода в новое пространство для t и w (mₜ и mw соответственно), такие чтоб tmₜ = us¹ᐟ², а wmw = vs¹ᐟ², что сохранит матрицу взаимодействий x: tmₜ(wmw)ᵀ = usv = twᵀ. однако, имея сингулярное разложение rₜrwᵀ, их вычислить несложно: mₜ = rwᵀ vᵣ s⁻¹ᐟ²; mw = rₜᵀ uᵣ s⁻¹ᐟ². второй шаг — перевести полученное стандартизированное представление эмбеддингов к некому референсному пространству. в качестве такого можно выбрать результат произвольной версии модели (например, первый) и зафиксировать его. дальше задача сводится к поиску матрицы, отображающей получившееся на очередном шаге дообучения представление в референсное пространство. хотя такое отображение можно искать среди произвольных матриц, удобно ограничить поиск только среди ортогональных. формально, имея матрицы tₖ (текущее пространство) и t₀ (референсное пространство) требуется найти такую ортогональную матрицу r, что rtₖ ~= t₀. эта задача называется ортогональной задачей прокруста. финально, получив матрицы отображения на первом (mₜ и mw) и втором (r) шагах, мы имеем преобразование, которое стабилизует пространства эмбеддингов документов (mₜr) и пользователей (mwr). так как преобразование ортогональное, то значения матрицы взаимодействий не меняются. при этом размерность матрицы — e * e, что делает её хранение и применение очень лёгкой операцией, которую можно добавить последним слоем нейросети. предложенный в статье способ не зависит от выбранной модели и легко добавляется в любой пайплайн обучения или инференса, что позволяет стабилизировать эмбеды при дообучении. @recsyschannel разбор подготовил ❣ артём ваншулин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-22T08:04:13+00:00" href="./posts/210.html">2026-01-22 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Orthogonal Low Rank Embedding Stabilization<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/abs/2508.07574v1" rel="nofollow noopener noreferrer">статью</a> от авторов из Netflix о стабилизации обучаемых эмбедов пользователя/документа. В двухбашенной архитектуре с поздним связыванием классическая проблема при дообучении — «разворот» пространств эмбеддингов пользователя/документа при сохранении результирующего dot product. Это происходит из-за того, что отдельные координаты эмбедов (например 1-я или i-ная координата вектора документа) не имеют никакого специального смысла, важно лишь их суммарное взаимодействие с соответствующим вектором пользователя. <br><br>Из-за нестабильности приходится пересчитывать эмбеддинги <strong>всех</strong> айтемов после каждого этапа дообучения модели, что увеличивает затраты на вычисления. Также необходимо синхронизировать версии пользовательской и документной частей моделей, что зачастую невозможно.<br><br>Авторы статьи предлагают элегантное решение проблемы, комбинируя две идеи: <br>- эффективное сингулярное разложение матрицы взаимодействий пользователя/документа;<br>- приведение к выбранному референсному пространству с помощью ортогональной задачи Прокруста.<br><br>Обозначим таблицу эмбеддингов документов как T (размерностью n * e, где n — количество документов, а e — размерность эмбеддингов), а таблицу эмбеддингов пользователей — как W (размерностью m * e, где m — количество пользователей). Тогда их произведение будет иметь смысл матрицы взаимодействий (X=TWᵀ). Сами документные и пользовательские эмбеддинги могут быть нестабильны при обучении: даже небольшие пертурбации в начальных условиях приводят к существенно разным результатам. При этом сингулярное разложение матрицы взаимодействий остаётся единственным с точностью до знаков сингулярных векторов.<br><br>Однако получить напрямую SVD-разложение матрицы X вычислительно сложно: O(mn²). В статье предлагают воспользоваться тем, что матрица X — это произведение двух низкоранговых матриц TWᵀ, и сделать QR-разложение каждой из них, что линейно по сложности относительно n и m. А затем сделать SVD-разложение уже низкоранговой (e * e) матрицы RₜRwᵀ, SVD(RₜRwᵀ)=UᵣSVᵣᵀ.<br><br>Кроме самого сингулярного разложения X потребуются ещё и матрицы перехода в новое пространство для T и W (Mₜ и Mw соответственно), такие чтоб TMₜ = US¹ᐟ², а WMw = VS¹ᐟ², что сохранит матрицу взаимодействий X: TMₜ(WMw)ᵀ = USV = TWᵀ. Однако, имея сингулярное разложение RₜRwᵀ, их вычислить несложно: Mₜ = Rwᵀ Vᵣ S⁻¹ᐟ²; Mw = Rₜᵀ Uᵣ S⁻¹ᐟ².<br><br>Второй шаг — перевести полученное стандартизированное представление эмбеддингов к некому референсному пространству. В качестве такого можно выбрать результат произвольной версии модели (например, первый) и зафиксировать его. <br><br>Дальше задача сводится к поиску матрицы, отображающей получившееся на очередном шаге дообучения представление в референсное пространство. Хотя такое отображение можно искать среди произвольных матриц, удобно ограничить поиск только среди ортогональных. Формально, имея матрицы Tₖ (текущее пространство) и T₀ (референсное пространство) требуется найти такую ортогональную матрицу R, что RTₖ ~= T₀. Эта задача называется ортогональной задачей Прокруста. <br><br>Финально, получив матрицы отображения на первом (Mₜ и Mw) и втором (R) шагах, мы имеем преобразование, которое стабилизует пространства эмбеддингов документов (MₜR) и пользователей (MwR). Так как преобразование ортогональное, то значения матрицы взаимодействий не меняются. При этом размерность матрицы — e * e, что делает её хранение и применение очень лёгкой операцией, которую можно добавить последним слоем нейросети.<br><br>Предложенный в статье способ не зависит от выбранной модели и легко добавляется в любой пайплайн обучения или инференса, что позволяет стабилизировать эмбеды при дообучении. <br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Артём Ваншулин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/210_480.webp" srcset="../assets/media/thumbs/210_480.webp 480w, ../assets/media/210.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/211_480.webp" srcset="../assets/media/thumbs/211_480.webp 480w, ../assets/media/211.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/212_480.webp" srcset="../assets/media/thumbs/212_480.webp 480w, ../assets/media/212.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/213_480.webp" srcset="../assets/media/thumbs/213_480.webp 480w, ../assets/media/213.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/214_480.webp" srcset="../assets/media/thumbs/214_480.webp 480w, ../assets/media/214.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/215_480.webp" srcset="../assets/media/thumbs/215_480.webp 480w, ../assets/media/215.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/216_480.webp" srcset="../assets/media/thumbs/216_480.webp 480w, ../assets/media/216.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="210" data-image-index="6" /></div></div>
      <div class="actions">
        <span>1 282 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/210" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/210.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="209" data-search="какие статьи 2025 года перечитывают эксперты рекомендательной. часть 2 вместе с авторами канала продолжаем вспоминать самые обсуждаемые статьи о рекомендательных системах за прошедший год. actionpiece: contextually tokenizing action sequences for generative recommendation совместная работа deepmind и авторов sasrec о токенизации в генеративном ретривале. каждое взаимодействие пользователя представляется в виде множества контентных фичей айтема, которые потом токенизируются на основе частоты их совстречаемостей — подобно тому, как делается в bpe. что интересно, мерджиться в один токен могут как фичи одного айтема, так и фичи смежных айтемов. из приятного — есть открытый репозиторий с кодом. correcting the logq correction: revisiting sampled softmax for large-scale retrieval статья от исследователей из яндекса о logq-коррекции отличается своей математичностью и обобщаемостью: её результат можно использовать в любой задаче с любой моделью, лишь бы она обучалась на softmax-лосс над большим каталогом. предложенная корректировка точнее аппроксимирует знаменатель softmax, при этом получается заменой буквально пары строк относительно классической logq-коррекции. рост метрик наблюдается как на закрытых данных, так и на публичных, в чём можно удостовериться, прогнав код из открытого репозитория. scaling recommender transformers to one billion parameters ещё одна статья от яндекса с рецептом масштабирования рекомендательных трансформеров до 1 миллиарда параметров. именно в ней представлен подход argus. его внедрение в яндекс музыку привело к самому большому одномоментному улучшению платформы от нейросетевых подходов: +2,26% к суммарному времени прослушивания и +6,37% к вероятности лайка. pinfm: foundation model for user activity sequences at a billion-scale visual discovery platform foundational-модели в llm — стандарт индустрии: обучать специфичные модели с нуля слишком дорого, поэтому обычно берут универсальную модель и дообучают под задачу. в рекомендациях модели меньше, но для каждой поверхности обучать новые модели с миллиардами эмбеддингов всё равно дорого. поэтому в pinterest предложили единую foundational-рекомендательную модель, которую дообучают под разные поверхности. в статье много практических трюков: комбинация infonce-лоссов под близкие задачи, серьёзные инженерные оптимизации (cross-attention с дедупликацией, int4-квантизация эмбеддингов), добавление компактных контентных эмбеддингов на этапе файнтюна. для cold start предлагают на файнтюне заменять часть айтемов в последовательности на рандомные, а для свежих айтемов использовать агрессивный дропаут. в продакшне это дало рост метрик: сохранения сниппетов +1,2% на главной и +0,72% на странице сниппета, а сохранения свежих айтемов на главной — +5,7%. @recsyschannel статьи отобрали ❣ сергей макеев, руслан кулиев, артём матвеев какие статьи 2025 года перечитывают эксперты рекомендательной. часть 2 вместе с авторами канала продолжаем вспоминать самые обсуждаемые статьи о рекомендательных системах за прошедший год. actionpiece: contextually tokenizing action sequences for generative recommendation совместная работа deepmind и авторов sasrec о токенизации в генеративном ретривале. каждое взаимодействие пользователя представляется в виде множества контентных фичей айтема, которые потом токенизируются на основе частоты их совстречаемостей — подобно тому, как делается в bpe. что интересно, мерджиться в один токен могут как фичи одного айтема, так и фичи смежных айтемов. из приятного — есть открытый репозиторий с кодом. correcting the logq correction: revisiting sampled softmax for large-scale retrieval статья от исследователей из яндекса о logq-коррекции отличается своей математичностью и обобщаемостью: её результат можно использовать в любой задаче с любой моделью, лишь бы она обучалась на softmax-лосс над большим каталогом. предложенная корректировка точнее аппроксимирует знаменатель softmax, при этом получается заменой буквально пары строк относительно классической logq-коррекции. рост метрик наблюдается как на закрытых данных, так и на публичных, в чём можно удостовериться, прогнав код из открытого репозитория. scaling recommender transformers to one billion parameters ещё одна статья от яндекса с рецептом масштабирования рекомендательных трансформеров до 1 миллиарда параметров. именно в ней представлен подход argus . его внедрение в яндекс музыку привело к самому большому одномоментному улучшению платформы от нейросетевых подходов: +2,26% к суммарному времени прослушивания и +6,37% к вероятности лайка. pinfm: foundation model for user activity sequences at a billion-scale visual discovery platform foundational-модели в llm — стандарт индустрии: обучать специфичные модели с нуля слишком дорого, поэтому обычно берут универсальную модель и дообучают под задачу. в рекомендациях модели меньше, но для каждой поверхности обучать новые модели с миллиардами эмбеддингов всё равно дорого. поэтому в pinterest предложили единую foundational-рекомендательную модель, которую дообучают под разные поверхности. в статье много практических трюков: комбинация infonce-лоссов под близкие задачи, серьёзные инженерные оптимизации (cross-attention с дедупликацией, int4-квантизация эмбеддингов), добавление компактных контентных эмбеддингов на этапе файнтюна. для cold start предлагают на файнтюне заменять часть айтемов в последовательности на рандомные, а для свежих айтемов использовать агрессивный дропаут. в продакшне это дало рост метрик: сохранения сниппетов +1,2% на главной и +0,72% на странице сниппета, а сохранения свежих айтемов на главной — +5,7%. @recsyschannel статьи отобрали ❣ сергей макеев, руслан кулиев, артём матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-14T08:16:20+00:00" href="./posts/209.html">2026-01-14 08:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Какие статьи 2025 года перечитывают эксперты Рекомендательной. Часть 2<br></strong><br>Вместе с авторами канала продолжаем вспоминать самые обсуждаемые статьи о рекомендательных системах за прошедший год.<br><br><a href="https://arxiv.org/abs/2502.13581v1" rel="nofollow noopener noreferrer"><strong>ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation</strong></a><br><br>Совместная работа DeepMind и авторов SasRec о токенизации в генеративном ретривале. Каждое взаимодействие пользователя представляется в виде множества контентных фичей айтема, которые потом токенизируются на основе частоты их совстречаемостей — подобно тому, как делается в BPE. Что интересно, мерджиться в один токен могут как фичи одного айтема, так и фичи смежных айтемов. Из приятного — есть открытый репозиторий с кодом.<br><br><a href="https://arxiv.org/abs/2507.09331" rel="nofollow noopener noreferrer"><strong>Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval</strong><br></a><br>Статья от исследователей из Яндекса о LogQ-коррекции отличается своей математичностью и обобщаемостью: её результат можно использовать в любой задаче с любой моделью, лишь бы она обучалась на softmax-лосс над большим каталогом. Предложенная корректировка точнее аппроксимирует знаменатель softmax, при этом получается заменой буквально пары строк относительно классической LogQ-коррекции. Рост метрик наблюдается как на закрытых данных, так и на публичных, в чём можно удостовериться, прогнав код из открытого репозитория.<br><br><a href="https://www.arxiv.org/abs/2507.15994" rel="nofollow noopener noreferrer"><strong>Scaling Recommender Transformers to One Billion Parameters</strong></a><br><br>Ещё одна статья от Яндекса с рецептом масштабирования рекомендательных трансформеров до 1 миллиарда параметров. Именно в ней представлен подход <a href="https://t.me/RecSysChannel/133" rel="nofollow noopener noreferrer">ARGUS</a>. Его внедрение в Яндекс Музыку привело к самому большому одномоментному улучшению платформы от нейросетевых подходов: +2,26% к суммарному времени прослушивания и +6,37% к вероятности лайка.<br><br><a href="https://arxiv.org/abs/2507.12704v3" rel="nofollow noopener noreferrer"><strong>PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform</strong></a><br><br>Foundational-модели в LLM — стандарт индустрии: обучать специфичные модели с нуля слишком дорого, поэтому обычно берут универсальную модель и дообучают под задачу. В рекомендациях модели меньше, но для каждой поверхности обучать новые модели с миллиардами эмбеддингов всё равно дорого. Поэтому в Pinterest предложили единую foundational-рекомендательную модель, которую дообучают под разные поверхности. <br><br>В статье много практических трюков: комбинация InfoNCE-лоссов под близкие задачи, серьёзные инженерные оптимизации (cross-attention с дедупликацией, int4-квантизация эмбеддингов), добавление компактных контентных эмбеддингов на этапе файнтюна. Для cold start предлагают на файнтюне заменять часть айтемов в последовательности на рандомные, а для свежих айтемов использовать агрессивный дропаут. В продакшне это дало рост метрик: сохранения сниппетов +1,2% на главной и +0,72% на странице сниппета, а сохранения свежих айтемов на главной — +5,7%.<br><br>@RecSysChannel<br>Статьи отобрали <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Сергей Макеев, Руслан Кулиев, Артём Матвеев</div>
      <div class="actions">
        <span>1 551 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/209" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/209.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="208" data-search="какие статьи 2025 года перечитывают эксперты рекомендательной. часть 1 прошедший год заметно изменил то, как мы представляли себе рекомендательные системы: границы между кандидатогенерацией, ранжированием и генеративностью начали стираться, а llm всё чаще становятся частью рекомендательных алгоритмов. мы собрали важные статьи, к которым эксперты рекомендательной возвращаются снова и снова. если вам есть что добавить или с чем поспорить — приходите обсуждать в комментарии! onerec technical report и onerec-v2 technical report самая хайповая серия статей этого года. авторы первыми в мире объединили все стадии рекомендательной системы в единую генеративную нейросеть. адаптировали техники, которые давно и активно применяются в других областях: претрейне, grpo rl. модель выкатили на 25% трафика одной из самых больших рекомендательных систем в мире с 400 млн dau. в onerec-v2 авторы уже реализуют описанные в первой части идеи ухода от схемы encoder-decoder и улучшения rl-обучения. onerec-think: in-text reasoning for generative recommendation исследователи одними из первых объединяют генеративные рекомендательные технологии и llm. в статье показаны не только новые способности модели (текстовый интерфейс рекомендаций, ризонинг), но и внедрение в продакшн. аналогичная работа от deepmind вышла чуть раньше, но здесь авторы пошли дальше: добавили ризонинг и усложнили процедуру обучения. meta lattice: model space redesign for cost-effective industry-scale ads recommendations авторы построили фундаментальную модель, сочетающую различные органические и рекламные поверхности meta*. она объединяет ручное признаковое пространство и обработку сырых историй пользователей. архитектура состоит из последовательных блоков трансформерных и interaction-слоёв. в статье — очень подробное описание и впечатляющие результаты внедрения. recgpt technical report и recgpt-v2 technical report в техрепорте от taobao рассказывается о создании их рекомендательной системы — на базе множества llm. recgpt позволяет хорошо учитывать не только коллаборативный сигнал, но и намерения, которыми руководствуются пользователи при выборе товаров, а также объяснять свои рекомендации на основе контекста и пользовательской истории. подход получил развитие в техрепорте recgpt-v2. plum: adapting pre-trained language models for industrial-scale generative recommendations в этой работе авторы из youtube и google deepmind рассматривают возможность переиспользовать предобученные llm для задачи генеративного ретривала. предложили два ключевых улучшения: инициализацию трансформера предобученной текстовой моделью, а также продолженный претрейн с использованием доменных данных (метаданных видео и пользовательских историй просмотров). в результатах показывают, что оба изменения независимо улучшают модель по метрикам генерации кандидатов. статья выделяется тем, что в ней соединяется много современных трендов: recsys+llm, semanticid и генеративная постановка задачи рекомендаций. @recsyschannel лучшие статьи отобрали ❣ николай савушкин, виктор януш, маргарита мишустина ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф какие статьи 2025 года перечитывают эксперты рекомендательной. часть 1 прошедший год заметно изменил то, как мы представляли себе рекомендательные системы: границы между кандидатогенерацией, ранжированием и генеративностью начали стираться, а llm всё чаще становятся частью рекомендательных алгоритмов. мы собрали важные статьи, к которым эксперты рекомендательной возвращаются снова и снова. если вам есть что добавить или с чем поспорить — приходите обсуждать в комментарии! onerec technical report и onerec-v2 technical report самая хайповая серия статей этого года. авторы первыми в мире объединили все стадии рекомендательной системы в единую генеративную нейросеть. адаптировали техники, которые давно и активно применяются в других областях: претрейне, grpo rl. модель выкатили на 25% трафика одной из самых больших рекомендательных систем в мире с 400 млн dau. в onerec-v2 авторы уже реализуют описанные в первой части идеи ухода от схемы encoder-decoder и улучшения rl-обучения. onerec-think: in-text reasoning for generative recommendation исследователи одними из первых объединяют генеративные рекомендательные технологии и llm. в статье показаны не только новые способности модели (текстовый интерфейс рекомендаций, ризонинг), но и внедрение в продакшн. аналогичная работа от deepmind вышла чуть раньше, но здесь авторы пошли дальше: добавили ризонинг и усложнили процедуру обучения. meta lattice: model space redesign for cost-effective industry-scale ads recommendations авторы построили фундаментальную модель, сочетающую различные органические и рекламные поверхности meta*. она объединяет ручное признаковое пространство и обработку сырых историй пользователей. архитектура состоит из последовательных блоков трансформерных и interaction-слоёв. в статье — очень подробное описание и впечатляющие результаты внедрения. recgpt technical report и recgpt-v2 technical report в техрепорте от taobao рассказывается о создании их рекомендательной системы — на базе множества llm. recgpt позволяет хорошо учитывать не только коллаборативный сигнал, но и намерения, которыми руководствуются пользователи при выборе товаров, а также объяснять свои рекомендации на основе контекста и пользовательской истории. подход получил развитие в техрепорте recgpt-v2. plum: adapting pre-trained language models for industrial-scale generative recommendations в этой работе авторы из youtube и google deepmind рассматривают возможность переиспользовать предобученные llm для задачи генеративного ретривала. предложили два ключевых улучшения: инициализацию трансформера предобученной текстовой моделью, а также продолженный претрейн с использованием доменных данных (метаданных видео и пользовательских историй просмотров). в результатах показывают, что оба изменения независимо улучшают модель по метрикам генерации кандидатов. статья выделяется тем, что в ней соединяется много современных трендов: recsys+llm, semanticid и генеративная постановка задачи рекомендаций. @recsyschannel лучшие статьи отобрали ❣ николай савушкин, виктор януш, маргарита мишустина ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-12T09:09:45+00:00" href="./posts/208.html">2026-01-12 09:09 UTC</a></div>
      </div>
      <div class="post-body"><strong>Какие статьи 2025 года перечитывают эксперты Рекомендательной. Часть 1<br></strong><br>Прошедший год заметно изменил то, как мы представляли себе рекомендательные системы: границы между кандидатогенерацией, ранжированием и генеративностью начали стираться, а LLM всё чаще становятся частью рекомендательных алгоритмов. Мы собрали важные статьи, к которым эксперты Рекомендательной возвращаются снова и снова. Если вам есть что добавить или с чем поспорить — приходите обсуждать в комментарии!<br><br><a href="https://arxiv.org/abs/2506.13695" rel="nofollow noopener noreferrer"><strong>OneRec Technical Report</strong></a><strong> и </strong><a href="https://arxiv.org/abs/2508.20900" rel="nofollow noopener noreferrer"><strong>OneRec-V2 Technical Report</strong></a><strong><br></strong><br>Самая хайповая серия статей этого года. Авторы первыми в мире объединили все стадии рекомендательной системы в единую генеративную нейросеть. Адаптировали техники, которые давно и активно применяются в других областях: претрейне, GRPO RL. Модель выкатили на 25% трафика одной из самых больших рекомендательных систем в мире с 400 млн DAU. В OneRec-V2 авторы уже реализуют описанные в первой части идеи ухода от схемы encoder-decoder и улучшения RL-обучения.<br><br><a href="https://arxiv.org/abs/2510.11639" rel="nofollow noopener noreferrer"><strong>OneRec-Think: In-Text Reasoning for Generative Recommendation<br></strong></a><br>Исследователи одними из первых объединяют генеративные рекомендательные технологии и LLM. В статье показаны не только новые способности модели (текстовый интерфейс рекомендаций, ризонинг), но и внедрение в продакшн. Аналогичная работа от Deepmind <a href="https://arxiv.org/abs/2510.07784" rel="nofollow noopener noreferrer">вышла</a> чуть раньше, но здесь авторы пошли дальше: добавили ризонинг и усложнили процедуру обучения.<br><br><a href="https://www.arxiv.org/abs/2512.09200" rel="nofollow noopener noreferrer"><strong>Meta Lattice: Model Space Redesign for Cost-Effective Industry-Scale Ads Recommendations</strong></a> <br><br>Авторы построили фундаментальную модель, сочетающую различные органические и рекламные поверхности Meta*. Она объединяет ручное признаковое пространство и обработку сырых историй пользователей. Архитектура состоит из последовательных блоков трансформерных и interaction-слоёв. В статье — очень подробное описание и впечатляющие результаты внедрения.<br><br><a href="https://arxiv.org/abs/2512.14503" rel="nofollow noopener noreferrer"><strong>RecGPT Technical Report</strong></a><strong> и </strong><a href="https://arxiv.org/abs/2512.14503" rel="nofollow noopener noreferrer"><strong>RecGPT-V2 Technical Report</strong></a><strong><br></strong><br>В техрепорте от Taobao рассказывается о создании их рекомендательной системы — на базе множества LLM. RecGPT позволяет хорошо учитывать не только коллаборативный сигнал, но и намерения, которыми руководствуются пользователи при выборе товаров, а также объяснять свои рекомендации на основе контекста и пользовательской истории. Подход получил развитие в техрепорте RecGPT-V2. <br><br><a href="https://arxiv.org/abs/2510.07784" rel="nofollow noopener noreferrer"><strong>PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations</strong></a><br><br>В этой работе авторы из Youtube и Google DeepMind рассматривают возможность переиспользовать предобученные LLM для задачи генеративного ретривала. Предложили два ключевых улучшения: инициализацию трансформера предобученной текстовой моделью, а также продолженный претрейн с использованием доменных данных (метаданных видео и пользовательских историй просмотров). В результатах показывают, что оба изменения независимо улучшают модель по метрикам генерации кандидатов. Статья выделяется тем, что в ней соединяется много современных трендов: RecSys+LLM, SemanticID и генеративная постановка задачи рекомендаций.<br><br>@RecSysChannel<br><br>Лучшие статьи отобрали <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Николай Савушкин, Виктор Януш, Маргарита Мишустина<br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em></div>
      <div class="actions">
        <span>1 416 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/208" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/208.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="207" data-search="🎉подводим итоги: лучшее за год в рекомендательной у нас в recsys channel есть традиция: каждый год мы вспоминаем популярные посты, которые пользователи читали и лайкали больше всего. так что прямо сейчас предлагаем немного замедлиться и оглянуться назад. будет интересно узнать, совпадает ли наш топ-5 с публикациями, которые запомнились вам. какие рексис-тренды будут развивать в яндексе в 2025 году в начале года в рекомендательных системах было полно многообещающих направлений: от масштабирования и семантических айди до графовых нейросетей и использования диффузионок. о том, на какие из них делали ставки в яндексе, нам рассказала группа исследования перспективных рекомендательных технологий. в новом году ждём новых трендов! исследователи яндекса выложили в опенсорс yambda — датасет на 5 млрд событий пост о yambda — крупнейшем в мире датасете в области рекомендательных систем. рассказали, зачем он нужен, какие у него ключевые особенности и какие методы оценки использовали наши исследователи. а ещё александр плошкин, один из авторов, представил работу на acm recsys ✨такие моменты точно хочется вспомнить в завершение года. transact v2: lifelong user action sequence modeling on pinterest recommendation руслан кулиев разобрал статью pinterest о том, как использовать максимально длинную историю действий в рекомендациях — даже когда у тебя 500 миллионов пользователей, миллиарды пинов и строгие тайминги на инференс. тут всё как в новогодней сказке: испытания непростые, ограничения жёсткие, но хэппи-энд неизбежен, как сельдь под шубой. plum: adapting pre-trained language models for industrial-scale generative recommendations одна из недавних публикаций владимира байкалова также вошла в число популярных. это разбор совместной работы от google deepmind и youtube, которая продолжает тему генеративных рекомендаций, начатую в предыдущей статье авторов — tiger. на этот раз основная идея — использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае google — это gemini). за подробностями приглашаем в разбор. scaling recommender transformers to one billion parameters в завершение подборки — ещё одна важная для нас работа. инженеры из группы исследования перспективных рекомендательных технологий выложили на arxiv статью о подходе argus, а в дальнейшем представят работу на конференции kdd’26. в статье описан опыт масштабирования рекомендательных трансформеров, вдохновлённый нашумевшей работой actions speak louder than words. в новом году ждём развития старых и появления новых рекомендательных трендов. спасибо, что вы с нами. с наступающим! а впереди у нас — подборки лучших статей от авторов канала. @recsyschannel 🎉 подводим итоги: лучшее за год в рекомендательной у нас в recsys channel есть традиция: каждый год мы вспоминаем популярные посты, которые пользователи читали и лайкали больше всего. так что прямо сейчас предлагаем немного замедлиться и оглянуться назад. будет интересно узнать, совпадает ли наш топ-5 с публикациями, которые запомнились вам. какие рексис-тренды будут развивать в яндексе в 2025 году в начале года в рекомендательных системах было полно многообещающих направлений: от масштабирования и семантических айди до графовых нейросетей и использования диффузионок. о том, на какие из них делали ставки в яндексе, нам рассказала группа исследования перспективных рекомендательных технологий. в новом году ждём новых трендов! исследователи яндекса выложили в опенсорс yambda — датасет на 5 млрд событий пост о yambda — крупнейшем в мире датасете в области рекомендательных систем. рассказали, зачем он нужен, какие у него ключевые особенности и какие методы оценки использовали наши исследователи. а ещё александр плошкин, один из авторов, представил работу на acm recsys ✨ такие моменты точно хочется вспомнить в завершение года. transact v2: lifelong user action sequence modeling on pinterest recommendation руслан кулиев разобрал статью pinterest о том, как использовать максимально длинную историю действий в рекомендациях — даже когда у тебя 500 миллионов пользователей, миллиарды пинов и строгие тайминги на инференс. тут всё как в новогодней сказке: испытания непростые, ограничения жёсткие, но хэппи-энд неизбежен, как сельдь под шубой. plum: adapting pre-trained language models for industrial-scale generative recommendations одна из недавних публикаций владимира байкалова также вошла в число популярных. это разбор совместной работы от google deepmind и youtube, которая продолжает тему генеративных рекомендаций, начатую в предыдущей статье авторов — tiger. на этот раз основная идея — использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае google — это gemini). за подробностями приглашаем в разбор. scaling recommender transformers to one billion parameters в завершение подборки — ещё одна важная для нас работа. инженеры из группы исследования перспективных рекомендательных технологий выложили на arxiv статью о подходе argus, а в дальнейшем представят работу на конференции kdd’26. в статье описан опыт масштабирования рекомендательных трансформеров, вдохновлённый нашумевшей работой actions speak louder than words. в новом году ждём развития старых и появления новых рекомендательных трендов. спасибо, что вы с нами. с наступающим! а впереди у нас — подборки лучших статей от авторов канала. @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-30T08:16:32+00:00" href="./posts/207.html">2025-12-30 08:16 UTC</a></div>
      </div>
      <div class="post-body"><tg-emoji emoji-id="5341780233399848004">🎉</tg-emoji><strong>Подводим итоги: лучшее за год в Рекомендательной<br></strong><br>У нас в RecSys Channel есть традиция: каждый год мы вспоминаем популярные посты, которые пользователи читали и лайкали больше всего. Так что прямо сейчас предлагаем немного замедлиться и оглянуться назад. Будет интересно узнать, совпадает ли наш топ-5 с публикациями, которые запомнились вам.<br><br><a href="https://t.me/RecSysChannel/66" rel="nofollow noopener noreferrer"><strong>Какие рексис-тренды будут развивать в Яндексе в 2025 году<br></strong></a><br>В начале года в рекомендательных системах было полно многообещающих направлений: от масштабирования и семантических айди до графовых нейросетей и использования диффузионок. О том, на какие из них делали ставки в Яндексе, нам рассказала группа исследования перспективных рекомендательных технологий. В новом году ждём новых трендов!<br><br><a href="https://t.me/RecSysChannel/103" rel="nofollow noopener noreferrer"><strong>Исследователи Яндекса выложили в опенсорс Yambda — датасет на 5 млрд событий<br></strong></a><br>Пост о Yambda — крупнейшем в мире датасете в области рекомендательных систем. Рассказали, зачем он нужен, какие у него ключевые особенности и какие методы оценки использовали наши исследователи. А ещё Александр Плошкин, один из авторов, <a href="https://www.youtube.com/watch?v=hMYdT9fEZUY" rel="nofollow noopener noreferrer">представил</a> работу на ACM RecSys <tg-emoji emoji-id="5220158626571766442">✨</tg-emoji>Такие моменты точно хочется вспомнить в завершение года.<br> <br><a href="https://t.me/RecSysChannel/122" rel="nofollow noopener noreferrer"><strong>TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation</strong></a><br><br>Руслан Кулиев разобрал статью Pinterest о том, как использовать максимально длинную историю действий в рекомендациях — даже когда у тебя 500 миллионов пользователей, миллиарды пинов и строгие тайминги на инференс. Тут всё как в новогодней сказке: испытания непростые, ограничения жёсткие, но хэппи-энд неизбежен, как сельдь под шубой. <br><br><a href="https://t.me/RecSysChannel/193" rel="nofollow noopener noreferrer"><strong>PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations</strong></a><br><br>Одна из недавних публикаций Владимира Байкалова также вошла в число популярных. Это разбор совместной работы от Google DeepMind и YouTube, которая продолжает тему генеративных рекомендаций, начатую в предыдущей статье авторов — TIGER. На этот раз основная идея — использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае Google — это Gemini). За подробностями приглашаем в разбор.<br><br><a href="https://t.me/RecSysChannel/133" rel="nofollow noopener noreferrer"><strong>Scaling Recommender Transformers to One Billion Parameters</strong></a><br><br>В завершение подборки — ещё одна важная для нас работа. Инженеры из группы исследования перспективных рекомендательных технологий выложили на arXiv статью о подходе ARGUS, а в дальнейшем представят работу на конференции KDD’26. В статье описан опыт масштабирования рекомендательных трансформеров, вдохновлённый нашумевшей работой Actions Speak Louder than Words.<br><br>В новом году ждём развития старых и появления новых рекомендательных трендов. Спасибо, что вы с нами. С наступающим! А впереди у нас — подборки лучших статей от авторов канала.<br><br>@RecSysChannel</div>
      <div class="actions">
        <span>1 666 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/207" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/207.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="206" data-search="gensar: unified generative search and recommendation сегодня разбираем статью от исследователей из renmin university of china и kuaishou technology, представленную на recsys&#x27;25. работа посвящена объединённому моделированию поиска и рекомендаций с использованием генеративного подхода на основе больших языковых моделей. современные коммерческие платформы (e-commerce, видео, музыка) предлагают одновременно и поиск, и рекомендации. совместное моделирование этих задач выглядит перспективно, однако авторы выявили ключевой trade-off: улучшение одной задачи часто приводит к деградации другой. причина кроется в различных информационных требованиях: — поиск фокусируется на семантической релевантности между запросами и айтемами — традиционные варианты поиска часто основаны на предобученных языковых моделях (bge, bert); — рекомендации сильно зависят от коллаборативных сигналов между пользователями и айтемами — id-based-рекомендации дают отличные результаты. gensar — унифицированный генеративный фреймворк для сбалансированного поиска и рекомендаций. для каждого айтема берутся два эмбеддинга: семантический (из текста) и коллаборативный (из user-item-взаимодействий). оба прогоняются через отдельные mlp-энкодеры и приводятся к одной размерности, затем конкатенируются в общий вектор. объединённый вектор квантуется через общие кодбуки: на каждом уровне выбирается ближайший код, его индекс записывается в идентификатор, а сам код вычитается из текущего вектора. накопленная последовательность — это shared prefix, содержащий общую информацию обоих эмбеддингов. далее остаточный вектор делится пополам. одна половина подаётся в семантические кодбуки, другая — в коллаборативные. в итоге: — semantic id (sid) = shared codes + semantic-specific codes; — collaborative id (cid) = shared codes + collaborative-specific codes. лосс состоит из суммы: 1) reconstruction loss: декодеры должны восстановить исходные эмбеддинги по кодам. 2) loss for residual quantization: считается для трёх наборов кодбуков (shared, semantic, collaborative) и включает codebook loss + commitment loss для каждого. выход модели зависит от задачи: - рекомендации → cid (коллаборативный сигнал важнее); - поиск → sid (семантика важнее); модель различает задачи через task-specific-промпты. обучение — joint training на смешанных батчах с балансировкой лоссов между задачами. оффлайн-эксперименты проводились на публичном датасете amazon и коммерческом датасете kuaishou. сравнение с бейзлайнами: sasrec, tiger (рекомендации), dpr, dsi (поиск), jsr и unisar (совместные модели). на amazon gensar показывает +12,9% по recall@10 для рекомендаций и +12,8% для поиска относительно лучшего бейзлайна unisar. на коммерческом датасете kuaishou прирост составляет +10,4% и +11,7% соответственно. ablation study подтверждает важность обоих компонентов: — без cid качество рекомендаций падает на 8,9%; — без sid качество поиска падает на 14,7%; — dual-id подход даёт +12,7% к рекомендациям по сравнению с single-id. @recsyschannel разбор подготовили ❣ михаил сёмин и никита мирошниченко gensar: unified generative search and recommendation сегодня разбираем статью от исследователей из renmin university of china и kuaishou technology, представленную на recsys&amp;#x27;25. работа посвящена объединённому моделированию поиска и рекомендаций с использованием генеративного подхода на основе больших языковых моделей. современные коммерческие платформы (e-commerce, видео, музыка) предлагают одновременно и поиск, и рекомендации. совместное моделирование этих задач выглядит перспективно, однако авторы выявили ключевой trade-off: улучшение одной задачи часто приводит к деградации другой. причина кроется в различных информационных требованиях: — поиск фокусируется на семантической релевантности между запросами и айтемами — традиционные варианты поиска часто основаны на предобученных языковых моделях (bge, bert); — рекомендации сильно зависят от коллаборативных сигналов между пользователями и айтемами — id-based-рекомендации дают отличные результаты. gensar — унифицированный генеративный фреймворк для сбалансированного поиска и рекомендаций. для каждого айтема берутся два эмбеддинга: семантический (из текста) и коллаборативный (из user-item-взаимодействий). оба прогоняются через отдельные mlp-энкодеры и приводятся к одной размерности, затем конкатенируются в общий вектор. объединённый вектор квантуется через общие кодбуки: на каждом уровне выбирается ближайший код, его индекс записывается в идентификатор, а сам код вычитается из текущего вектора. накопленная последовательность — это shared prefix, содержащий общую информацию обоих эмбеддингов. далее остаточный вектор делится пополам. одна половина подаётся в семантические кодбуки , другая — в коллаборативные . в итоге: — semantic id (sid) = shared codes + semantic-specific codes; — collaborative id (cid) = shared codes + collaborative-specific codes. лосс состоит из суммы: 1) reconstruction loss: декодеры должны восстановить исходные эмбеддинги по кодам. 2) loss for residual quantization : считается для трёх наборов кодбуков (shared, semantic, collaborative) и включает codebook loss + commitment loss для каждого. выход модели зависит от задачи: - рекомендации → cid (коллаборативный сигнал важнее); - поиск → sid (семантика важнее); модель различает задачи через task-specific-промпты. обучение — joint training на смешанных батчах с балансировкой лоссов между задачами. оффлайн-эксперименты проводились на публичном датасете amazon и коммерческом датасете kuaishou. сравнение с бейзлайнами: sasrec, tiger (рекомендации), dpr, dsi (поиск), jsr и unisar (совместные модели). на amazon gensar показывает +12,9% по recall@10 для рекомендаций и +12,8% для поиска относительно лучшего бейзлайна unisar. на коммерческом датасете kuaishou прирост составляет +10,4% и +11,7% соответственно. ablation study подтверждает важность обоих компонентов: — без cid качество рекомендаций падает на 8,9%; — без sid качество поиска падает на 14,7%; — dual-id подход даёт +12,7% к рекомендациям по сравнению с single-id. @recsyschannel разбор подготовили ❣ михаил сёмин и никита мирошниченко">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-26T13:43:47+00:00" href="./posts/206.html">2025-12-26 13:43 UTC</a></div>
      </div>
      <div class="post-body"><strong>GenSAR: Unified Generative Search and Recommendation<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/pdf/2504.05730" rel="nofollow noopener noreferrer">статью</a> от исследователей из Renmin University of China и Kuaishou Technology, представленную на RecSys&#x27;25. Работа посвящена объединённому моделированию поиска и рекомендаций с использованием генеративного подхода на основе больших языковых моделей.<br><br>Современные коммерческие платформы (e-commerce, видео, музыка) предлагают одновременно и поиск, и рекомендации. Совместное моделирование этих задач выглядит перспективно, однако авторы выявили ключевой trade-off: улучшение одной задачи часто приводит к деградации другой.<br><br>Причина кроется в различных информационных требованиях:<br><br>— <strong>Поиск</strong> фокусируется на семантической релевантности между запросами и айтемами — традиционные варианты поиска часто основаны на предобученных языковых моделях (BGE, BERT);<br>— <strong>Рекомендации</strong> сильно зависят от коллаборативных сигналов между пользователями и айтемами — ID-based-рекомендации дают отличные результаты.<br><br><strong>GenSAR</strong> — унифицированный генеративный фреймворк для сбалансированного поиска и рекомендаций.<br><br>Для каждого айтема берутся два эмбеддинга: семантический (из текста) и коллаборативный (из user-item-взаимодействий). Оба прогоняются через отдельные MLP-энкодеры и приводятся к одной размерности, затем конкатенируются в общий вектор.<br><br>Объединённый вектор квантуется через общие кодбуки: на каждом уровне выбирается ближайший код, его индекс записывается в идентификатор, а сам код вычитается из текущего вектора. Накопленная последовательность — это shared prefix, содержащий общую информацию обоих эмбеддингов.<br><br>Далее остаточный вектор делится пополам. Одна половина подаётся в <strong>семантические кодбуки</strong>, другая — в <strong>коллаборативные</strong>. В итоге:<br><br>— <strong>Semantic ID (SID)</strong> = shared codes + semantic-specific codes;<br>— <strong>Collaborative ID (CID)</strong> = shared codes + collaborative-specific codes.<br><br>Лосс состоит из суммы:<br>1) <strong>Reconstruction loss:</strong> декодеры должны восстановить исходные эмбеддинги по кодам.<br>2) <strong>Loss for residual quantization</strong>: считается для трёх наборов кодбуков (shared, semantic, collaborative) и включает codebook loss + commitment loss для каждого.<br><br>Выход модели зависит от задачи:<br>- <strong>Рекомендации</strong> → CID (коллаборативный сигнал важнее);<br>- <strong>Поиск</strong> → SID (семантика важнее);<br>Модель различает задачи через task-specific-промпты. Обучение — joint training на смешанных батчах с балансировкой лоссов между задачами.<br><br><strong>Оффлайн-эксперименты</strong> проводились на публичном датасете Amazon и коммерческом датасете Kuaishou. Сравнение с бейзлайнами: SASRec, TIGER (рекомендации), DPR, DSI (поиск), JSR и UniSAR (совместные модели).<br><br>На Amazon GenSAR показывает +12,9% по Recall@10 для рекомендаций и +12,8% для поиска относительно лучшего бейзлайна UniSAR. На коммерческом датасете Kuaishou прирост составляет +10,4% и +11,7% соответственно.<br><br><strong>Ablation study</strong> подтверждает важность обоих компонентов:<br>— Без CID качество рекомендаций падает на 8,9%;<br>— Без SID качество поиска падает на 14,7%;<br>— Dual-ID подход даёт +12,7% к рекомендациям по сравнению с single-ID.<br><br>@RecSysChannel<br>Разбор подготовили <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> </em> Михаил Сёмин и Никита Мирошниченко<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/206_480.webp" srcset="../assets/media/thumbs/206_480.webp 480w, ../assets/media/206.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="206" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 710 просмотров · 45 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/206" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/206.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="205" data-search="longer: scaling up long sequence modeling in industrial recommenders сегодня разбираем статью от bytedance, представленную на recsys&#x27;25. работа посвящена эффективным end-to-end-рекомендациям на gpu с использованием длинных пользовательских последовательностей (до 10 тыс. событий). авторы рассматривают кейсы douyin (китайского tiktok) — как в рекламе, так и в e-commerce. основная проблема длинных последовательностей — квадратичная сложность аттеншна по длине l. авторы предлагают архитектуру longer, решающую эту задачу. 1) token merging. рядом стоящие токены в истории группируются по k штук. группировка выполняется либо простой конкатенацией, либо через лёгкий внутренний трансформер (innertrans). это уменьшает эффективную длину последовательности с l до l/k. для типичных настроек (l=2000, d=32) tokenmerge(k=4) снижает flops аттеншна примерно на 40–50% при минимальной потере качества. авторы аккуратно разбирают tokenmerge и innertrans в ablation study: — без merge (l=2000): flops ≈ 3,73e9; — c merge (k=8, concat, l=250): flops ≈ 3,03e9, δauc +1,58%, δlogloss −3,48%; — добавление innertrans даёт ещё небольшой, но устойчивый буст. таким образом, tokenmerge не только снижает вычислительные затраты, но и даёт буст по метрикам качества, в сравнении с ванильным вариантом. 2) global tokens. на вход подаётся конкатенация глобальных токенов и пользовательской истории. глобальные токены играют роль «якорей» (user profiles, context &amp; cross features). 3) тонкости обучения. dense- и sparse-параметры (огромные embedding-таблицы) находятся на gpu-кластере. обучение в bf16/fp16, часть активаций не хранится, а пересчитывается на backward. на инференсе используется kv cache serving. эксперименты и результаты в офлайне longer решает задачу предсказания conversion rate (cvr) на 5,2 млрд примеров (130 дней данных douyin ads) на кластере 48 × a100. по сравнению с базовым transformer даёт +0,21% auc и −0,39% logloss. онлайн a/b-тесты в douyin ads: — live streaming: adss +1,06%, advv +1,17% — short video: adss +2,10%, advv +2,15% — mall: adss +1,82%, advv +1,41% онлайн a/b-тесты в douyin e-commerce: — live streaming: order/u +7,92%, gmv/u +6654% — short video: order/u +4,61%, gmv/u +5,28% @recsyschannel разбор подготовил ❣ михаил сёмин longer: scaling up long sequence modeling in industrial recommenders сегодня разбираем статью от bytedance, представленную на recsys&amp;#x27;25. работа посвящена эффективным end-to-end-рекомендациям на gpu с использованием длинных пользовательских последовательностей (до 10 тыс. событий). авторы рассматривают кейсы douyin (китайского tiktok) — как в рекламе, так и в e-commerce. основная проблема длинных последовательностей — квадратичная сложность аттеншна по длине l. авторы предлагают архитектуру longer, решающую эту задачу. 1) token merging. рядом стоящие токены в истории группируются по k штук. группировка выполняется либо простой конкатенацией, либо через лёгкий внутренний трансформер (innertrans). это уменьшает эффективную длину последовательности с l до l/k. для типичных настроек (l=2000, d=32) tokenmerge(k=4) снижает flops аттеншна примерно на 40–50% при минимальной потере качества. авторы аккуратно разбирают tokenmerge и innertrans в ablation study: — без merge (l=2000): flops ≈ 3,73e9; — c merge (k=8, concat, l=250): flops ≈ 3,03e9, δauc +1,58%, δlogloss −3,48%; — добавление innertrans даёт ещё небольшой, но устойчивый буст. таким образом, tokenmerge не только снижает вычислительные затраты, но и даёт буст по метрикам качества, в сравнении с ванильным вариантом. 2) global tokens. на вход подаётся конкатенация глобальных токенов и пользовательской истории. глобальные токены играют роль «якорей» (user profiles, context &amp;amp; cross features). 3) тонкости обучения. dense- и sparse-параметры (огромные embedding-таблицы) находятся на gpu-кластере. обучение в bf16/fp16, часть активаций не хранится, а пересчитывается на backward. на инференсе используется kv cache serving. эксперименты и результаты в офлайне longer решает задачу предсказания conversion rate (cvr) на 5,2 млрд примеров (130 дней данных douyin ads) на кластере 48 × a100. по сравнению с базовым transformer даёт +0,21% auc и −0,39% logloss. онлайн a/b-тесты в douyin ads: — live streaming: adss +1,06%, advv +1,17% — short video: adss +2,10%, advv +2,15% — mall: adss +1,82%, advv +1,41% онлайн a/b-тесты в douyin e-commerce: — live streaming: order/u +7,92%, gmv/u +6654% — short video: order/u +4,61%, gmv/u +5,28% @recsyschannel разбор подготовил ❣ михаил сёмин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-18T08:26:32+00:00" href="./posts/205.html">2025-12-18 08:26 UTC</a></div>
      </div>
      <div class="post-body"><strong>LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders <br></strong><br>Сегодня разбираем <a href="https://arxiv.org/abs/2505.04421v1" rel="nofollow noopener noreferrer">статью</a> от ByteDance, представленную на RecSys&#x27;25. Работа посвящена эффективным end-to-end-рекомендациям на GPU с использованием длинных пользовательских последовательностей (до 10 тыс. событий). Авторы рассматривают кейсы Douyin (китайского TikTok) — как в рекламе, так и в e-commerce.<br><br>Основная проблема длинных последовательностей — квадратичная сложность аттеншна по длине L. Авторы предлагают архитектуру LONGER, решающую эту задачу.<br><br><strong>1) Token Merging.</strong> Рядом стоящие токены в истории группируются по K штук. Группировка выполняется либо простой конкатенацией, либо через лёгкий внутренний трансформер (InnerTrans). Это уменьшает эффективную длину последовательности с L до L/K. Для типичных настроек (L=2000, d=32) TokenMerge(K=4) снижает FLOPs аттеншна примерно на 40–50% при минимальной потере качества.<br><br>Авторы аккуратно разбирают TokenMerge и InnerTrans в ablation study:<br>— без Merge (L=2000): FLOPs ≈ 3,73e9;<br>— c Merge (K=8, concat, L=250): FLOPs ≈ 3,03e9, ΔAUC +1,58%, ΔLogLoss −3,48%;<br>— добавление InnerTrans даёт ещё небольшой, но устойчивый буст.<br><br>Таким образом, TokenMerge не только снижает вычислительные затраты, но и даёт буст по метрикам качества, в сравнении с ванильным вариантом.<br><br><strong>2) Global Tokens.</strong> На вход подаётся конкатенация глобальных токенов и пользовательской истории. Глобальные токены играют роль «якорей» (User Profiles, Context &amp; Cross Features).<br><br><strong>3) Тонкости обучения.</strong> Dense- и sparse-параметры (огромные embedding-таблицы) находятся на GPU-кластере. Обучение в BF16/FP16, часть активаций не хранится, а пересчитывается на backward. На инференсе используется KV Cache Serving.<br><br><strong>Эксперименты и результаты<br></strong><br>В офлайне LONGER решает задачу предсказания conversion rate (CVR) на 5,2 млрд примеров (130 дней данных Douyin Ads) на кластере 48 × A100. По сравнению с базовым Transformer даёт +0,21% AUC и −0,39% LogLoss.<br><br>Онлайн A/B-тесты в Douyin Ads:<br>— Live Streaming: ADSS +1,06%, ADVV +1,17%<br>— Short Video: ADSS +2,10%, ADVV +2,15%<br>— Mall: ADSS +1,82%, ADVV +1,41%<br><br>Онлайн A/B-тесты в Douyin E-commerce:<br>— Live Streaming: Order/U +7,92%, GMV/U +6654%<br>— Short Video: Order/U +4,61%, GMV/U +5,28%<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Михаил Сёмин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/205_480.webp" srcset="../assets/media/thumbs/205_480.webp 480w, ../assets/media/205.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="205" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 786 просмотров · 46 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/205" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/205.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="204" data-search="minionerec: an open-source framework for scaling generative recommendation [2/2] завершаем разбор статьи minionerec. в первой части обсуждали sft и семантические id, а теперь посмотрим, что происходит дальше: rl-дообучение, генерация траекторий и насколько авторы смогли воспроизвести индустриальные результаты на открытых данных. rl-дообучение: grpo и генерация траекторий после sft и алайнмента применяется reinforcement learning по аналогии с onerec — используется grpo. модель уже умеет генерировать последовательности семантических токенов, каждая из которых соответствует айтему. генерируются несколько траекторий (beam search или dynamic sampling), затем по каждой считается награда. награда включает два компонента: корректность следующего айтема и ранжирование согласно frozen collaborative модели (sasrec в реализации авторов). чтобы модель генерировала только валидные токены, используется constrained beam search: логиты, не соответствующие существующим айтемам из кодбука, маскируются. то есть стратегия гарантирует, что каждая сгенерированная последовательность соответствует реальному айтему. grpo здесь в «ванильной» версии: есть ограничение на отклонение от начальной политики, чтобы избежать reward hacking — классического случая, когда модель накручивает награду, но начинает генерировать бесполезные последовательности. результаты и масштабирование авторы говорят о законе масштабирования: модели большего размера достигают лучшего качества (меньше лосс). но есть важный момент: все модели обучаются одинаковое количество эпох на одном и том же датасете. нет параметризации по количеству данных, а значит это не полноценный закон масштабирования, а скорее наблюдение: «большая модель лучше маленькой». с другой стороны, до этой работы таких результатов на открытых датасетах не было — и это важное подтверждение работоспособности индустриальных подходов вне kuaishou. в целом, minionerec повторяет ключевые идеи onerec — но делает это на открытых данных, с полностью доступным кодом и понятными экспериментами. авторы аккуратно воспроизводят семантическую токенизацию tiger, sft поверх llm, алайнмент между nlp и рекомендациями и rl-дообучение через grpo. это первая попытка показать, что индустриальные результаты действительно можно повторить за пределами приватных данных. @recsyschannel разбор подготовил ❣ илья мурзин minionerec: an open-source framework for scaling generative recommendation [2/2] завершаем разбор статьи minionerec . в первой части обсуждали sft и семантические id, а теперь посмотрим, что происходит дальше: rl-дообучение, генерация траекторий и насколько авторы смогли воспроизвести индустриальные результаты на открытых данных. rl-дообучение: grpo и генерация траекторий после sft и алайнмента применяется reinforcement learning по аналогии с onerec — используется grpo. модель уже умеет генерировать последовательности семантических токенов, каждая из которых соответствует айтему. генерируются несколько траекторий (beam search или dynamic sampling), затем по каждой считается награда. награда включает два компонента: корректность следующего айтема и ранжирование согласно frozen collaborative модели (sasrec в реализации авторов). чтобы модель генерировала только валидные токены, используется constrained beam search: логиты, не соответствующие существующим айтемам из кодбука, маскируются. то есть стратегия гарантирует, что каждая сгенерированная последовательность соответствует реальному айтему. grpo здесь в «ванильной» версии: есть ограничение на отклонение от начальной политики, чтобы избежать reward hacking — классического случая, когда модель накручивает награду, но начинает генерировать бесполезные последовательности. результаты и масштабирование авторы говорят о законе масштабирования: модели большего размера достигают лучшего качества (меньше лосс). но есть важный момент: все модели обучаются одинаковое количество эпох на одном и том же датасете. нет параметризации по количеству данных, а значит это не полноценный закон масштабирования, а скорее наблюдение: «большая модель лучше маленькой». с другой стороны, до этой работы таких результатов на открытых датасетах не было — и это важное подтверждение работоспособности индустриальных подходов вне kuaishou. в целом, minionerec повторяет ключевые идеи onerec — но делает это на открытых данных, с полностью доступным кодом и понятными экспериментами. авторы аккуратно воспроизводят семантическую токенизацию tiger, sft поверх llm, алайнмент между nlp и рекомендациями и rl-дообучение через grpo. это первая попытка показать, что индустриальные результаты действительно можно повторить за пределами приватных данных. @recsyschannel разбор подготовил ❣ илья мурзин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-12T08:46:14+00:00" href="./posts/204.html">2025-12-12 08:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation [2/2]</strong><br><br>Завершаем разбор <a href="https://arxiv.org/abs/2510.24431" rel="nofollow noopener noreferrer">статьи MiniOneRec</a>. В <a href="https://t.me/RecSysChannel/203" rel="nofollow noopener noreferrer">первой части</a> обсуждали SFT и семантические ID, а теперь посмотрим, что происходит дальше: RL-дообучение, генерация траекторий и насколько авторы смогли воспроизвести индустриальные результаты на открытых данных.<br><br><strong>RL-дообучение: GRPO и генерация траекторий<br></strong><br>После SFT и алайнмента применяется reinforcement learning по аналогии с OneRec — используется GRPO. Модель уже умеет генерировать последовательности семантических токенов, каждая из которых соответствует айтему. Генерируются несколько траекторий (beam search или dynamic sampling), затем по каждой считается награда. Награда включает два компонента: корректность следующего айтема и ранжирование согласно frozen collaborative модели (SASRec в реализации авторов).<br><br>Чтобы модель генерировала только валидные токены, используется constrained beam search: логиты, не соответствующие существующим айтемам из кодбука, маскируются. То есть стратегия гарантирует, что каждая сгенерированная последовательность соответствует реальному айтему.<br><br>GRPO здесь в «ванильной» версии: есть ограничение на отклонение от начальной политики, чтобы избежать reward hacking — классического случая, когда модель накручивает награду, но начинает генерировать бесполезные последовательности.<br><br><strong>Результаты и масштабирование<br></strong><br>Авторы говорят о законе масштабирования: модели большего размера достигают лучшего качества (меньше лосс). Но есть важный момент: все модели обучаются одинаковое количество эпох на одном и том же датасете. Нет параметризации по количеству данных, а значит это не полноценный закон масштабирования, а скорее наблюдение: «большая модель лучше маленькой». С другой стороны, до этой работы таких результатов на открытых датасетах не было — и это важное подтверждение работоспособности индустриальных подходов вне Kuaishou.<br><br>В целом, MiniOneRec повторяет ключевые идеи OneRec — но делает это на открытых данных, с полностью доступным кодом и понятными экспериментами. Авторы аккуратно воспроизводят семантическую токенизацию Tiger, SFT поверх LLM, алайнмент между NLP и рекомендациями и RL-дообучение через GRPO. Это первая попытка показать, что индустриальные результаты действительно можно повторить за пределами приватных данных.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Илья Мурзин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/204_480.webp" srcset="../assets/media/thumbs/204_480.webp 480w, ../assets/media/204.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="204" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 831 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/204" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/204.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="203" data-search="minionerec: an open-source framework for scaling generative recommendation [1/2] сегодня начинаем разбирать неожиданно вышедшую статью minionerec. в ней использованы подходы из нашумевшей серии техрепортов onerec от kuaishou. авторы minionerec — исследователи из университетов китая и сингапура — фактически берут ключевые идеи onerec, переносят их в минимально жизнеспособный фреймворк и подтверждают, что они действительно работают на открытых данных. это выглядит как попытка «повторить onerec», но в академии и без доступа к приватным датасетам. и действительно, llm-подходы в nlp работают слишком хорошо, чтобы не пытаться перенести их в другие домены — в том числе в рекомендации. семантические id и подготовка данных первое препятствие, которое сразу появляется в рекомендациях, — огромный каталог документов. нельзя просто взять llm и обучить её поверх id в десятки или сотни миллионов: embedding/de-embedding-слои и softmax станут непригодными. поэтому minionerec, как и onerec, используют семантические id из работы tiger. суть простая: каждый документ кодируется короткой последовательностью токенов. из исходного текста (название + описание) получают эмбеддинг: текст прогоняется через замороженную qwen3-embedding-4b, затем hidden states последнего слоя усредняются (mean pooling) в один вектор, который и подаётся в трёхуровневую rq-vae-кластеризацию. на каждом уровне отнимается ближайший из 256 центроид (получается semantic_id_0), формируется остаток, который проходит ту же процедуру кластеризации следующего уровня — в итоге документ получает трёхтокенную семантическую подпись. это резко уменьшает словарь: вместо миллионов id становится 3x256 дополнительных к словарю токенов. у tiger и onerec эта идея ключевая, и minionerec полностью повторяет её. авторы также отмечают проблему коллапса кластеров (слишком много документов в одном кластере), поэтому в коде используют не случайную инициализацию, а rq k-means из оригинального onerec. это увеличивает энтропию кластеров и улучшает токенизацию. sft и перенос nlp в рекомендации после токенизации авторы делают sft поверх предобученной llm (берут qwen). в случае с академией это более чем оправдано: экономятся ресурсы, не нужно тренировать архитектуру с нуля и сразу есть сильный старт. истории пользователя подаются в виде последовательностей семантических токенов, а модель учится предсказывать следующий айтем. в этот процесс также привносят новизну вида алайнмента между nlp и рекомендациями. авторы подмешивают в обучение разные форматы примеров, с тем чтобы перенести world knowledge модели на новые токены. получается несколько типов задач: - история на естественном языке — нужно предсказать следующий айтем в виде семантических токенов; - история в виде семантических токенов — нужно предсказать текстовое описание следующего айтема; - просто перевод айтема между двумя представлениями — из текста в семантические токены и наоборот. этот шаг даёт самый большой прирост качества. в аблейшенах видно, что это важнее, чем стартовать со случайных весов. вместе с тем сама идея достаточно проста: смешивать рекомендации с задачами nlp, чтобы модель лучше экстраполировала знания. это похоже на недавнюю работу от google — plum, хотя авторы на неё не ссылаются (возможно результаты получены параллельно). в следующей части обзора расскажем о rl-дообучении, масштабировании и результатах. @recsyschannel разбор подготовил ❣ илья мурзин minionerec: an open-source framework for scaling generative recommendation [1/2] сегодня начинаем разбирать неожиданно вышедшую статью minionerec . в ней использованы подходы из нашумевшей серии техрепортов onerec от kuaishou. авторы minionerec — исследователи из университетов китая и сингапура — фактически берут ключевые идеи onerec, переносят их в минимально жизнеспособный фреймворк и подтверждают, что они действительно работают на открытых данных. это выглядит как попытка «повторить onerec», но в академии и без доступа к приватным датасетам. и действительно, llm-подходы в nlp работают слишком хорошо, чтобы не пытаться перенести их в другие домены — в том числе в рекомендации. семантические id и подготовка данных первое препятствие, которое сразу появляется в рекомендациях, — огромный каталог документов. нельзя просто взять llm и обучить её поверх id в десятки или сотни миллионов: embedding/de-embedding-слои и softmax станут непригодными. поэтому minionerec, как и onerec, используют семантические id из работы tiger . суть простая: каждый документ кодируется короткой последовательностью токенов. из исходного текста (название + описание) получают эмбеддинг: текст прогоняется через замороженную qwen3-embedding-4b, затем hidden states последнего слоя усредняются (mean pooling) в один вектор, который и подаётся в трёхуровневую rq-vae-кластеризацию. на каждом уровне отнимается ближайший из 256 центроид (получается semantic_id_0), формируется остаток, который проходит ту же процедуру кластеризации следующего уровня — в итоге документ получает трёхтокенную семантическую подпись. это резко уменьшает словарь: вместо миллионов id становится 3x256 дополнительных к словарю токенов. у tiger и onerec эта идея ключевая, и minionerec полностью повторяет её. авторы также отмечают проблему коллапса кластеров (слишком много документов в одном кластере), поэтому в коде используют не случайную инициализацию, а rq k-means из оригинального onerec. это увеличивает энтропию кластеров и улучшает токенизацию. sft и перенос nlp в рекомендации после токенизации авторы делают sft поверх предобученной llm (берут qwen). в случае с академией это более чем оправдано: экономятся ресурсы, не нужно тренировать архитектуру с нуля и сразу есть сильный старт. истории пользователя подаются в виде последовательностей семантических токенов, а модель учится предсказывать следующий айтем. в этот процесс также привносят новизну вида алайнмента между nlp и рекомендациями. авторы подмешивают в обучение разные форматы примеров, с тем чтобы перенести world knowledge модели на новые токены. получается несколько типов задач: - история на естественном языке — нужно предсказать следующий айтем в виде семантических токенов; - история в виде семантических токенов — нужно предсказать текстовое описание следующего айтема; - просто перевод айтема между двумя представлениями — из текста в семантические токены и наоборот. этот шаг даёт самый большой прирост качества. в аблейшенах видно, что это важнее, чем стартовать со случайных весов. вместе с тем сама идея достаточно проста: смешивать рекомендации с задачами nlp, чтобы модель лучше экстраполировала знания. это похоже на недавнюю работу от google — plum, хотя авторы на неё не ссылаются (возможно результаты получены параллельно). в следующей части обзора расскажем о rl-дообучении, масштабировании и результатах. @recsyschannel разбор подготовил ❣ илья мурзин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-05T07:34:01+00:00" href="./posts/203.html">2025-12-05 07:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation [1/2]<br></strong><br>Сегодня начинаем разбирать неожиданно вышедшую <a href="https://arxiv.org/abs/2510.24431" rel="nofollow noopener noreferrer">статью MiniOneRec</a>. В ней использованы подходы из нашумевшей серии техрепортов <a href="https://arxiv.org/abs/2506.13695v1" rel="nofollow noopener noreferrer">OneRec</a> от Kuaishou. Авторы MiniOneRec — исследователи из университетов Китая и Сингапура — фактически берут ключевые идеи OneRec, переносят их в минимально жизнеспособный фреймворк и подтверждают, что они действительно работают на открытых данных. Это выглядит как попытка «повторить OneRec», но в академии и без доступа к приватным датасетам. И действительно, LLM-подходы в NLP работают слишком хорошо, чтобы не пытаться перенести их в другие домены — в том числе в рекомендации.<br><br><strong>Семантические ID и подготовка данных<br></strong><br>Первое препятствие, которое сразу появляется в рекомендациях, — огромный каталог документов. Нельзя просто взять LLM и обучить её поверх ID в десятки или сотни миллионов: embedding/de-embedding-слои и softmax станут непригодными. Поэтому MiniOneRec, как и OneRec, используют семантические ID из работы <a href="https://arxiv.org/abs/2305.05065" rel="nofollow noopener noreferrer">TIGER</a>.<br><br>Суть простая: каждый документ кодируется короткой последовательностью токенов. Из исходного текста (название + описание) получают эмбеддинг: текст прогоняется через замороженную Qwen3-Embedding-4B, затем hidden states последнего слоя усредняются (mean pooling) в один вектор, который и подаётся в трёхуровневую RQ-VAE-кластеризацию. На каждом уровне отнимается ближайший из 256 центроид (получается semantic_id_0), формируется остаток, который проходит ту же процедуру кластеризации следующего уровня — в итоге документ получает трёхтокенную семантическую подпись. Это резко уменьшает словарь: вместо миллионов ID становится 3x256 дополнительных к словарю токенов. У Tiger и OneRec эта идея ключевая, и MiniOneRec полностью повторяет её.<br><br>Авторы также отмечают проблему коллапса кластеров (слишком много документов в одном кластере), поэтому в коде используют не случайную инициализацию, а RQ k-means из оригинального OneRec. Это увеличивает энтропию кластеров и улучшает токенизацию.<br><br><strong>SFT и перенос NLP в рекомендации<br></strong><br>После токенизации авторы делают SFT поверх предобученной LLM (берут Qwen). В случае с академией это более чем оправдано: экономятся ресурсы, не нужно тренировать архитектуру с нуля и сразу есть сильный старт. Истории пользователя подаются в виде последовательностей семантических токенов, а модель учится предсказывать следующий айтем.<br><br>В этот процесс также привносят новизну вида алайнмента между NLP и рекомендациями. Авторы подмешивают в обучение разные форматы примеров, с тем чтобы перенести world knowledge модели на новые токены.<br><br>Получается несколько типов задач:<br><br>- история на естественном языке — нужно предсказать следующий айтем в виде семантических токенов;<br><br>- история в виде семантических токенов — нужно предсказать текстовое описание следующего айтема;<br><br>- просто перевод айтема между двумя представлениями — из текста в семантические токены и наоборот.<br><br>Этот шаг даёт самый большой прирост качества. В аблейшенах видно, что это важнее, чем стартовать со случайных весов. Вместе с тем сама идея достаточно проста: смешивать рекомендации с задачами NLP, чтобы модель лучше экстраполировала знания. Это похоже на недавнюю работу от Google — PLUM, хотя авторы на неё не ссылаются (возможно результаты получены параллельно).<br><br>В <a href="https://t.me/RecSysChannel/204" rel="nofollow noopener noreferrer">следующей части</a> обзора расскажем о RL-дообучении, масштабировании и результатах.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Илья Мурзин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/203_480.webp" srcset="../assets/media/thumbs/203_480.webp 480w, ../assets/media/203.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="203" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 032 просмотров · 39 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/203" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/203.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="202" data-search="onetrans: unified feature interaction and sequence modeling with one transformer in industrial recommender сегодня разберём статью о onetrans — нейросетевом ранкере от tiktok. его можно было бы назвать аналогом hstu от meta* или transact от pinterest, но ни на одну из этих работ авторы не ссылаются, упоминают только wukong и rankmixer. исследователи называют свою разработку единой ранжирующей моделью в рамках каскадного рекомендательного стека, которая заменяет финальный ранкер за счёт того, что совмещает sequence-моделирование и взаимодействие признаков (feature interaction). классический подход к финальному ранжированию, ставший стандартом индустрии, обычно предполагает, что историю пользователя обрабатывают отдельно от обработки ручных счётчиков. сначала входную последовательность событий пропускают через sequence modeling block, где вытаскивают и сжимают информацию о пользователе, необходимую для построения рекомендаций. потом сжатое представление попадает в interaction-блок. параллельно набор non-seq-фичей (например, ручные счëтчики) конкатенируют или каким-то другим способом подают в тот же interaction-блок. onetrans одновременно моделирует и последовательные, и non-seq-входы внутри единой модели onetrans. архитектура ранкера — на схеме: последовательности (голубые блоки s на схеме) и non-seq (ns, оранжевые) айтемы токенизируют по отдельности. блоки поведения пользователей разделяют специальными блоками [sep], после чего единую последовательность подают на вход onetrans pyramid stack. внутри этой пирамиды последовательность s итеративно сжимают до тех пор, пока её длина не совпадёт с ns. onetrans block — казуальный трансформер с rmsnorm, mixed causal attention и mixed ffn. под mixed авторы понимают смешанную параметризацию: у s-токенов общие qkv/ffn-матрицы, а каждый ns получает свои токен-специфичные веса. по результатам экспериментов на индустриальных датасетах, onetrans эффективно масштабируется с ростом параметров: систематиически обгоняет сильные бейзлайны и показывает рост на 5,68% per-user gmv в онлайн-a/b-тестах. *компания meta, владеющая instagram, признана экстремистской; её деятельность в россии запрещена. @recsyschannel разбор подготовил ❣ артём матвеев onetrans: unified feature interaction and sequence modeling with one transformer in industrial recommender сегодня разберём статью о onetrans — нейросетевом ранкере от tiktok. его можно было бы назвать аналогом hstu от meta* или transact от pinterest, но ни на одну из этих работ авторы не ссылаются, упоминают только wukong и rankmixer . исследователи называют свою разработку единой ранжирующей моделью в рамках каскадного рекомендательного стека, которая заменяет финальный ранкер за счёт того, что совмещает sequence-моделирование и взаимодействие признаков (feature interaction). классический подход к финальному ранжированию, ставший стандартом индустрии, обычно предполагает, что историю пользователя обрабатывают отдельно от обработки ручных счётчиков. сначала входную последовательность событий пропускают через sequence modeling block, где вытаскивают и сжимают информацию о пользователе, необходимую для построения рекомендаций. потом сжатое представление попадает в interaction-блок. параллельно набор non-seq-фичей (например, ручные счëтчики) конкатенируют или каким-то другим способом подают в тот же interaction-блок. onetrans одновременно моделирует и последовательные, и non-seq-входы внутри единой модели onetrans. архитектура ранкера — на схеме: последовательности (голубые блоки s на схеме) и non-seq (ns, оранжевые) айтемы токенизируют по отдельности. блоки поведения пользователей разделяют специальными блоками [sep], после чего единую последовательность подают на вход onetrans pyramid stack. внутри этой пирамиды последовательность s итеративно сжимают до тех пор, пока её длина не совпадёт с ns. onetrans block — казуальный трансформер с rmsnorm, mixed causal attention и mixed ffn. под mixed авторы понимают смешанную параметризацию: у s-токенов общие qkv/ffn-матрицы, а каждый ns получает свои токен-специфичные веса. по результатам экспериментов на индустриальных датасетах, onetrans эффективно масштабируется с ростом параметров: систематиически обгоняет сильные бейзлайны и показывает рост на 5,68% per-user gmv в онлайн-a/b-тестах. *компания meta, владеющая instagram, признана экстремистской; её деятельность в россии запрещена. @recsyschannel разбор подготовил ❣ артём матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-26T09:12:01+00:00" href="./posts/202.html">2025-11-26 09:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2510.26104" rel="nofollow noopener noreferrer">статью</a> о OneTrans — нейросетевом ранкере от TikTok. Его можно было бы назвать аналогом <a href="https://arxiv.org/abs/2402.17152" rel="nofollow noopener noreferrer">HSTU</a> от Meta* или <a href="https://arxiv.org/abs/2306.00248" rel="nofollow noopener noreferrer">TransAct</a> от Pinterest, но ни на одну из этих работ авторы не ссылаются, упоминают только <a href="https://t.me/RecSysChannel/80" rel="nofollow noopener noreferrer">Wukong</a> и <a href="https://arxiv.org/abs/2507.15551" rel="nofollow noopener noreferrer">RankMixer</a>.<br><br>Исследователи называют свою разработку единой ранжирующей моделью в рамках каскадного рекомендательного стека, которая заменяет финальный ранкер за счёт того, что совмещает sequence-моделирование и взаимодействие признаков (feature interaction).<br><br>Классический подход к финальному ранжированию, ставший стандартом индустрии, обычно предполагает, что историю пользователя обрабатывают отдельно от обработки ручных счётчиков. Сначала входную последовательность событий пропускают через Sequence Modeling Block, где вытаскивают и сжимают информацию о пользователе, необходимую для построения рекомендаций. Потом сжатое представление попадает в Interaction-блок. Параллельно набор Non-Seq-фичей (например, ручные счëтчики) конкатенируют или каким-то другим способом подают в тот же Interaction-блок.<br><br>OneTrans одновременно моделирует и последовательные, и Non-Seq-входы внутри единой модели OneTrans. Архитектура ранкера — на схеме: последовательности (голубые блоки S на схеме) и non-seq (NS, оранжевые) айтемы токенизируют по отдельности. Блоки поведения пользователей разделяют специальными блоками [SEP], после чего единую последовательность подают на вход OneTrans Pyramid Stack. Внутри этой пирамиды последовательность S итеративно сжимают до тех пор, пока её длина не совпадёт с NS.  <br><br>OneTrans Block — казуальный трансформер с RMSNorm, Mixed Causal Attention и Mixed FFN. Под Mixed авторы понимают смешанную параметризацию: у S-токенов общие QKV/FFN-матрицы, а каждый NS получает свои токен-специфичные веса.<br><br>По результатам экспериментов на индустриальных датасетах, OneTrans эффективно масштабируется с ростом параметров: систематиически обгоняет сильные бейзлайны и показывает рост на 5,68% per-user GMV в онлайн-A/B-тестах.<br><br><em>*Компания Meta, владеющая Instagram, признана экстремистской; её деятельность в России запрещена.</em><br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Артём Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/202_480.webp" srcset="../assets/media/thumbs/202_480.webp 480w, ../assets/media/202.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="202" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 945 просмотров · 39 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/202" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/202.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="201" data-search="balancing fine-tuning and rag: a hybrid strategy for dynamic llm recommendation updates сегодня разберём статью от компании google deepmind, главный фокус которой в последнее время — llm в рекомендациях. у рекомендательных моделей есть ряд преимуществ относительно более традиционных рексистем: богатое понимание мира, ризонинг, способность объяснять, почему был порекомендован тот или иной объект, и многое другое. но это не отменяет слабые места, например, проблему динамики в интересах пользователей и корпусе айтемов. именно этот аспект авторы разбирают в статье. эксперименты проводятся в youtube shorts. авторы выясняют: нужно ли вообще обновлять рекомендательную llm в таком домене, или со своим знанием мира она и так справится. отвечают интересным экспериментом: кластеризуют тематики шортсов и по логам пользователей собирают тройки (c1, c2, c_next) кластеров, с которыми кто-то последовательно провзаимодействовал. делают так отдельно для нескольких месяцев, после чего для всех пар (c1, c2) собирают топ-5 переходов в c_next для каждого месяца i: {c_next_1, …, c_next_5}_i. далее для пар (c1, c2) считают iou множеств переходов за соседние месяцы (i vs. i+1) и получают низкое значение 0,17, что подчеркивает высокую изменчивость паттернов пользователей во времени. отсюда возникает необходимость постоянного обновления рекомендательной llm. в статье сравниваются два метода: fine-tuning и rag. первый обновляет веса модели через дообучение на новом трафике. второй, грубо говоря, усиливает промпт недостающей информацией о пользователе и домене, при этом никак не влияет на саму модель. fine-tuning. модель дообучается предсказывать следующий кластер, с которым провзаимодействовало большинство пользователей: (c_1, c_2, …, c_n) → c_{n+1}. описания кластеров поступают в llm в словесной форме. из минусов метода — сложность, возможность переобучения и высокие вычислительные затраты. из-за последнего дообучение происходит лишь ежемесячно. rag. точно так же представляет историю в виде последних взаимодействий с кластерами (обновленные интересы пользователя), но ещё и добавляет в промпт наиболее популярное продолжение для этой последовательности взаимодействий (обновленные реалии домена). поскольку множество всевозможных историй вида (c_1, c_2, …, c_k) невелико и конечно, инференс производится несколько раз в неделю, а предпосчитанные кандидаты для каждой истории достаются в реальном времени лукапом. в офлайн-эксперименте проверяют, нужен ли rag и стоит ли пересчитывать кандидатов раз в несколько дней. оказывается, что на оба вопроса ответ положительный. в a/b-тесте отчитываются о приростах satisfied user outcomes, satisfaction rate и об уменьшении dissatisfaction rate и negative interaction. @recsyschannel разбор подготовил ❣ сергей макеев balancing fine-tuning and rag: a hybrid strategy for dynamic llm recommendation updates сегодня разберём статью от компании google deepmind, главный фокус которой в последнее время — llm в рекомендациях. у рекомендательных моделей есть ряд преимуществ относительно более традиционных рексистем: богатое понимание мира, ризонинг, способность объяснять, почему был порекомендован тот или иной объект, и многое другое. но это не отменяет слабые места, например, проблему динамики в интересах пользователей и корпусе айтемов. именно этот аспект авторы разбирают в статье. эксперименты проводятся в youtube shorts. авторы выясняют: нужно ли вообще обновлять рекомендательную llm в таком домене, или со своим знанием мира она и так справится. отвечают интересным экспериментом: кластеризуют тематики шортсов и по логам пользователей собирают тройки (c1, c2, c_next) кластеров, с которыми кто-то последовательно провзаимодействовал. делают так отдельно для нескольких месяцев, после чего для всех пар (c1, c2) собирают топ-5 переходов в c_next для каждого месяца i: {c_next_1, …, c_next_5}_i. далее для пар (c1, c2) считают iou множеств переходов за соседние месяцы (i vs. i+1) и получают низкое значение 0,17, что подчеркивает высокую изменчивость паттернов пользователей во времени. отсюда возникает необходимость постоянного обновления рекомендательной llm. в статье сравниваются два метода: fine-tuning и rag. первый обновляет веса модели через дообучение на новом трафике. второй, грубо говоря, усиливает промпт недостающей информацией о пользователе и домене, при этом никак не влияет на саму модель. fine-tuning. модель дообучается предсказывать следующий кластер, с которым провзаимодействовало большинство пользователей: (c_1, c_2, …, c_n) → c_{n+1}. описания кластеров поступают в llm в словесной форме. из минусов метода — сложность, возможность переобучения и высокие вычислительные затраты. из-за последнего дообучение происходит лишь ежемесячно. rag. точно так же представляет историю в виде последних взаимодействий с кластерами (обновленные интересы пользователя), но ещё и добавляет в промпт наиболее популярное продолжение для этой последовательности взаимодействий (обновленные реалии домена). поскольку множество всевозможных историй вида (c_1, c_2, …, c_k) невелико и конечно, инференс производится несколько раз в неделю, а предпосчитанные кандидаты для каждой истории достаются в реальном времени лукапом. в офлайн-эксперименте проверяют, нужен ли rag и стоит ли пересчитывать кандидатов раз в несколько дней. оказывается, что на оба вопроса ответ положительный. в a/b-тесте отчитываются о приростах satisfied user outcomes, satisfaction rate и об уменьшении dissatisfaction rate и negative interaction. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-20T08:23:01+00:00" href="./posts/201.html">2025-11-20 08:23 UTC</a></div>
      </div>
      <div class="post-body"><strong>Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2510.20260" rel="nofollow noopener noreferrer">статью</a> от компании Google DeepMind, главный фокус которой в последнее время — LLM в рекомендациях. У рекомендательных моделей есть ряд преимуществ относительно более традиционных рексистем: богатое понимание мира, ризонинг, способность объяснять, почему был порекомендован тот или иной объект, и многое другое. Но это не отменяет слабые места, например, проблему динамики в интересах пользователей и корпусе айтемов. Именно этот аспект авторы разбирают в статье. <br><br>Эксперименты проводятся в YouTube Shorts. Авторы выясняют: нужно ли вообще обновлять рекомендательную LLM в таком домене, или со своим знанием мира она и так справится. Отвечают интересным экспериментом: кластеризуют тематики шортсов и по логам пользователей собирают тройки (c1, c2, c_next) кластеров, с которыми кто-то последовательно провзаимодействовал. Делают так отдельно для нескольких месяцев, после чего для всех пар (c1, c2) собирают топ-5 переходов в c_next для каждого месяца i: {c_next_1, …, c_next_5}_i. Далее для пар (c1, c2) считают IoU множеств переходов за соседние месяцы (i vs. i+1) и получают низкое значение 0,17, что подчеркивает высокую изменчивость паттернов пользователей во времени. Отсюда возникает необходимость постоянного обновления рекомендательной LLM.<br><br>В статье сравниваются два метода: fine-tuning и RAG. Первый обновляет веса модели через дообучение на новом трафике. Второй, грубо говоря, усиливает промпт недостающей информацией о пользователе и домене, при этом никак не влияет на саму модель. <br><br><strong>Fine-tuning.</strong> Модель дообучается предсказывать следующий кластер, с которым провзаимодействовало большинство пользователей: (c_1, c_2, …, c_n) → c_{n+1}. Описания кластеров поступают в LLM в словесной форме. Из минусов метода — сложность, возможность переобучения и высокие вычислительные затраты. Из-за последнего дообучение происходит лишь ежемесячно. <br><br><strong>RAG. </strong>Точно так же представляет историю в виде последних взаимодействий с кластерами (обновленные интересы пользователя), но ещё и добавляет в промпт наиболее популярное продолжение для этой последовательности взаимодействий (обновленные реалии домена). Поскольку множество всевозможных историй вида (c_1, c_2, …, c_k) невелико и конечно, инференс производится несколько раз в неделю, а предпосчитанные кандидаты для каждой истории достаются в реальном времени лукапом. <br><br>В офлайн-эксперименте проверяют, нужен ли RAG и стоит ли пересчитывать кандидатов раз в несколько дней. Оказывается, что на оба вопроса ответ положительный. В A/B-тесте отчитываются о приростах Satisfied User Outcomes, Satisfaction Rate и об уменьшении Dissatisfaction Rate и Negative Interaction. <br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/201_480.webp" srcset="../assets/media/thumbs/201_480.webp 480w, ../assets/media/201.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="201" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 927 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/201" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/201.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="198" data-search="cikm’25 в разгаре: интересные статьи с третьего дня конференции по наблюдению наших инженеров, в этом году хорошие доклады на cikm распределены крайне неравномерно: в одни тайм-слоты интересного мало, зато в другие несколько любопытных работ представляют параллельно. о том, что запомнилось 12 ноября, рассказал разработчик службы рекомендательных технологий яндекса иван артемьев. первая половина дня была более спокойной, зато вторая — очень насыщенной, так что пришлось делиться на группы и бегать между комнатами. в первой половине была одна запоминающаяся статья — das: dual-aligned semantic ids empowered industrial recommender system. авторы прямо во время обучения семантических id замешивают коллаборативный сигнал. в дополнении раскладывали на семантики не только айтемы, но и пользователей — и применяли пользовательские id в рекомендательной системе. во второй половине дня было три классных работы. ⚫️mpformer: adaptive framework for industrial multi-task personalized sequential retriever в статье учат кандидатогенератор, который умеет предсказывать кандидатов для разных таргетов (лайки, клики и прочее) и при этом персонализировано распределяет бюджет на них. ⚫️tbgrecall: a generative retrieval model for e-commerce recommendation scenarios taming ultra-long behavior sequence in session-wise generative recommendation ⚫️taming ultra-long behavior sequence in session-wise generative recommendation в этих двух работах обучают кандидатогенератор для задачи генерации сессий. при этом в последней — добавляют очень большую историю (до 100 000 айтемов) в сжатом виде, чтобы учитывать долгосрочные интересы пользователей. @recsyschannel cikm’25 в разгаре: интересные статьи с третьего дня конференции по наблюдению наших инженеров, в этом году хорошие доклады на cikm распределены крайне неравномерно: в одни тайм-слоты интересного мало, зато в другие несколько любопытных работ представляют параллельно. о том, что запомнилось 12 ноября, рассказал разработчик службы рекомендательных технологий яндекса иван артемьев. первая половина дня была более спокойной, зато вторая — очень насыщенной, так что пришлось делиться на группы и бегать между комнатами. в первой половине была одна запоминающаяся статья — das: dual-aligned semantic ids empowered industrial recommender system . авторы прямо во время обучения семантических id замешивают коллаборативный сигнал. в дополнении раскладывали на семантики не только айтемы, но и пользователей — и применяли пользовательские id в рекомендательной системе. во второй половине дня было три классных работы. ⚫️ mpformer: adaptive framework for industrial multi-task personalized sequential retriever в статье учат кандидатогенератор, который умеет предсказывать кандидатов для разных таргетов (лайки, клики и прочее) и при этом персонализировано распределяет бюджет на них. ⚫️ tbgrecall: a generative retrieval model for e-commerce recommendation scenarios taming ultra-long behavior sequence in session-wise generative recommendation ⚫️ taming ultra-long behavior sequence in session-wise generative recommendation в этих двух работах обучают кандидатогенератор для задачи генерации сессий. при этом в последней — добавляют очень большую историю (до 100 000 айтемов) в сжатом виде, чтобы учитывать долгосрочные интересы пользователей. @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-13T10:25:03+00:00" href="./posts/198.html">2025-11-13 10:25 UTC</a></div>
      </div>
      <div class="post-body"><strong>CIKM’25 в разгаре: интересные статьи с третьего дня конференции</strong><br><br>По наблюдению наших инженеров, в этом году хорошие доклады на CIKM распределены крайне неравномерно: в одни тайм-слоты интересного мало, зато в другие несколько любопытных работ представляют параллельно. О том, что запомнилось 12 ноября, рассказал разработчик службы рекомендательных технологий Яндекса Иван Артемьев.<br><br><blockquote>Первая половина дня была более спокойной, зато вторая — очень насыщенной, так что пришлось делиться на группы и бегать между комнатами.<br><br>В первой половине была одна запоминающаяся статья — <a href="https://arxiv.org/abs/2508.10584" rel="nofollow noopener noreferrer"><strong>DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System</strong></a>. Авторы прямо во время обучения семантических ID замешивают коллаборативный сигнал. В дополнении раскладывали на семантики не только айтемы, но и пользователей — и применяли пользовательские ID в рекомендательной системе.<br><br>Во второй половине дня было три классных работы.<br><br><tg-emoji emoji-id="5339424272039308836">⚫️</tg-emoji><a href="https://arxiv.org/abs/2508.20400v1" rel="nofollow noopener noreferrer"><strong>MPFormer: Adaptive Framework for Industrial Multi-Task Personalized Sequential Retriever</strong></a><br><br>В статье учат кандидатогенератор, который умеет предсказывать кандидатов для разных таргетов (лайки, клики и прочее) и при этом персонализировано распределяет бюджет на них.<br><br><tg-emoji emoji-id="5339424272039308836">⚫️</tg-emoji><a href="https://arxiv.org/abs/2508.11977" rel="nofollow noopener noreferrer"><strong>TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios Taming Ultra-Long Behavior Sequence in Session-wise Generative Recommendation</strong></a><br><br><tg-emoji emoji-id="5339424272039308836">⚫️</tg-emoji><a href="https://dl.acm.org/doi/10.1145/3746252.3761564" rel="nofollow noopener noreferrer"><strong>Taming Ultra-Long Behavior Sequence in Session-wise Generative Recommendation</strong></a><br><br>В этих двух работах обучают кандидатогенератор для задачи генерации сессий. При этом в последней — добавляют очень большую историю (до 100 000 айтемов) в сжатом виде, чтобы учитывать долгосрочные интересы пользователей. </blockquote><br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/198_480.webp" srcset="../assets/media/thumbs/198_480.webp 480w, ../assets/media/198.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="198" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/199_480.webp" srcset="../assets/media/thumbs/199_480.webp 480w, ../assets/media/199.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="198" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/200_480.webp" srcset="../assets/media/thumbs/200_480.webp 480w, ../assets/media/200.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="198" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 764 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/198" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/198.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="194" data-search="cikm’25: начинаем репортаж с конференции в сеуле в эти дни в южной корее проходит международная конференция cikm 2025, на которую отправилась часть команды рекомендательных технологий яндекса. cikm менее известна широкой аудитории, чем, например, recsys, но тоже регулярно собирает интересные работы в области информационного поиска, анализа данных и рекомендательных систем. так, в программе этого года заявлены доклады от pinterest (transact v2, pinrec), kuaishou (qarm, pantheon) и meituan (ega-v1). с нетерпением ждём подробностей от наших инженеров. кроме туториалов и воркшопов, будет analyticup 2025 — конкурсный трек с задачами по анализу данных. в этом году его проводят alibaba international и finvolution. впечатлениями от первого дня конференции поделился николай савушкин, руководитель службы рекомендательных технологий: отличное начало — сильные доклады от pinterest и живое общение с участниками. в конце дня были интересные выступления от ebay и google. от ebay докладывала русскоязычная исследовательница, пообщались после её презентации о ресёрче в компании. основная программа стартует завтра. продолжим держать вас в курсе! а пока несём немного атмосферных фото из сеула. @recsyschannel cikm’25: начинаем репортаж с конференции в сеуле в эти дни в южной корее проходит международная конференция cikm 2025, на которую отправилась часть команды рекомендательных технологий яндекса. cikm менее известна широкой аудитории, чем, например, recsys, но тоже регулярно собирает интересные работы в области информационного поиска, анализа данных и рекомендательных систем. так, в программе этого года заявлены доклады от pinterest ( transact v2 , pinrec ), kuaishou ( qarm , pantheon ) и meituan ( ega-v1 ). с нетерпением ждём подробностей от наших инженеров. кроме туториалов и воркшопов, будет analyticup 2025 — конкурсный трек с задачами по анализу данных. в этом году его проводят alibaba international и finvolution. впечатлениями от первого дня конференции поделился николай савушкин, руководитель службы рекомендательных технологий: отличное начало — сильные доклады от pinterest и живое общение с участниками. в конце дня были интересные выступления от ebay и google. от ebay докладывала русскоязычная исследовательница, пообщались после её презентации о ресёрче в компании. основная программа стартует завтра. продолжим держать вас в курсе! а пока несём немного атмосферных фото из сеула. @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-10T13:30:39+00:00" href="./posts/194.html">2025-11-10 13:30 UTC</a></div>
      </div>
      <div class="post-body"><strong>CIKM’25: начинаем репортаж с конференции в Сеуле<br></strong><br>В эти дни в Южной Корее проходит международная конференция CIKM 2025, на которую отправилась часть команды рекомендательных технологий Яндекса.<br><br><a href="https://cikm2025.org/" rel="nofollow noopener noreferrer">CIKM</a> менее известна широкой аудитории, чем, например, RecSys, но тоже регулярно собирает интересные работы в области информационного поиска, анализа данных и рекомендательных систем.<br><br>Так, в программе этого года заявлены доклады от Pinterest (<a href="https://arxiv.org/abs/2506.02267" rel="nofollow noopener noreferrer">TransAct V2</a>, <a href="https://arxiv.org/html/2504.10507v1" rel="nofollow noopener noreferrer">PinRec</a>), Kuaishou (<a href="https://arxiv.org/abs/2411.11739" rel="nofollow noopener noreferrer">QARM</a>, <a href="https://arxiv.org/abs/2505.13894" rel="nofollow noopener noreferrer">Pantheon</a>) и Meituan (<a href="https://arxiv.org/abs/2505.19755" rel="nofollow noopener noreferrer">EGA-V1</a>). С нетерпением ждём подробностей от наших инженеров.<br><br>Кроме туториалов и воркшопов, будет AnalytiCup 2025 — конкурсный трек с задачами по анализу данных. В этом году его проводят Alibaba International и FinVolution.<br><br>Впечатлениями от первого дня конференции поделился Николай Савушкин, руководитель службы рекомендательных технологий:<br><br><blockquote>Отличное начало — сильные доклады от Pinterest и живое общение с участниками. В конце дня были интересные выступления от eBay и Google. От eBay докладывала русскоязычная исследовательница, пообщались после её презентации о ресёрче в компании. Основная программа стартует завтра.</blockquote><br><br>Продолжим держать вас в курсе! А пока несём немного атмосферных фото из Сеула.<br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/194_480.webp" srcset="../assets/media/thumbs/194_480.webp 480w, ../assets/media/194.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="194" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/195_480.webp" srcset="../assets/media/thumbs/195_480.webp 480w, ../assets/media/195.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="194" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/196_480.webp" srcset="../assets/media/thumbs/196_480.webp 480w, ../assets/media/196.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="194" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/197_480.webp" srcset="../assets/media/thumbs/197_480.webp 480w, ../assets/media/197.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="194" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 542 просмотров · 42 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/194" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/194.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="193" data-search="plum: adapting pre-trained language models for industrial-scale generative recommendations сегодня разбираем совместную статью google deepmind и youtube. об этой работе было известно заранее — на конференции recsys авторы проекта, включая ed chi и lichan hong, упоминали, что готовится статья о генеративных рекомендациях. через пару недель после конференции она действительно вышла. исследование продолжает трек генеративных рекомендаций, заданный предыдущей работой авторов tiger. на этот раз основная идея — использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае google — это gemini). простая llm из коробки не подходит: модель не знает ни о корпусе айтемов, ни о пользовательских поведенческих сценариях, что приводит к плохим результатам. чтобы исправить это, команда предлагает фреймворк plum, включающий три стадии: item tokenization, continued pre-training и task-specific fine-tuning. кратко разберём каждую из них. 1) item tokenization. за основу взята работа tiger. в ней семантические идентификаторы (sids) формировались через rq-vae поверх текстового описания товара (эксперименты были на открытых датасетах amazon). в plum к этому подходу добавляют коллаборативный сигнал и мультимодальные контентные представления. используются уже готовые аудио-, видео- и текстовые эмбеддинги youtube, которые конкатенируются и проходят через энкодер rq-vae. новые предложенные компоненты: — multi-resolution codebooks: число идентификаторов в кодбуках уменьшается от слоя к слою, чтобы верхние уровни разделяли крупные семантические категории, а нижние — более гранулярные признаки. — progressive masking: модель обучается восстанавливать не полный набор sids, а его префикс. ключевая вещь в архитектуре — дополнительный contrastive learning на rq-vae, который вводит коллаборативный сигнал прямо в процесс токенизации. берутся пары айтемов, встречавшихся рядом в пользовательской истории как позитивные пары, обучается с помощью infonce по батчу. так коллаборативный сигнал тоже участвует в формировании кодбуков без отдельной стадии дообучения как, например, в onerec. в итоге sids начинают отражать не только контентную информацию об айтемах, но и коллаборативные пользовательские связи между ними. 2) continued pre-training (cpt). здесь языковая модель дообучается с увеличенным словарём, в который, помимо изначальных токенов, встроены токены айтемов. модель обучается на смешанной задаче (supervised + self-supervised). цель этой стадии — заставить llm встроить в общее семантическое пространство представления токенов и sids. 3) task-specific fine-tuning. это полноценное обучение на задачу генеративного ретривала: модель предсказывает релевантные айтемы в пользовательских историях (обучение на next token prediction). в целом идея plum строится на прямой аналогии между словами в языковых моделях и айтемами в recsys: если в nlp слова токенизируются для работы с огромным словарём, то в рекомендациях можно аналогично токенизировать айтемы. эксперименты и результаты основная модель — mixture-of-experts с ~900 млн активных параметров (всего 4,2 млрд). в онлайн-a/b-тестах plum показывает рост ключевых метрик: ctr и вовлечённости пользователей, особенно в коротких видео (youtube shorts). аблейшены подтверждают, что важны все предложенные компоненты. в работе показывают законы масштабирования для предложенного фреймворка: при увеличении размера моделей при разном фиксированном вычислительном бюджете ошибки на обучении и валидации снижаются, но самые большие модели (около 3 млрд активных параметров, 20 млрд всего) пока упираются в ограничения вычислительных ресурсов. исследователям не хватило времени, данных и мощностей, чтобы хорошо обучить модели такого размера, однако инженеры считают, что при дальнейшем масштабировании качество может вырасти ещё больше. финальная plum-модель дообучается ежедневно на ~0,25 млрд примеров, тогда как предыдущие lem (large embedding models) подходы требовали многомиллиардных датасетов. @recsyschannel разбор подготовил ❣ владимир байкалов plum: adapting pre-trained language models for industrial-scale generative recommendations сегодня разбираем совместную статью google deepmind и youtube. об этой работе было известно заранее — на конференции recsys авторы проекта, включая ed chi и lichan hong , упоминали, что готовится статья о генеративных рекомендациях. через пару недель после конференции она действительно вышла. исследование продолжает трек генеративных рекомендаций, заданный предыдущей работой авторов tiger . на этот раз основная идея — использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае google — это gemini). простая llm из коробки не подходит: модель не знает ни о корпусе айтемов, ни о пользовательских поведенческих сценариях, что приводит к плохим результатам. чтобы исправить это, команда предлагает фреймворк plum, включающий три стадии: item tokenization, continued pre-training и task-specific fine-tuning. кратко разберём каждую из них. 1) item tokenization. за основу взята работа tiger . в ней семантические идентификаторы (sids) формировались через rq-vae поверх текстового описания товара (эксперименты были на открытых датасетах amazon). в plum к этому подходу добавляют коллаборативный сигнал и мультимодальные контентные представления. используются уже готовые аудио-, видео- и текстовые эмбеддинги youtube, которые конкатенируются и проходят через энкодер rq-vae. новые предложенные компоненты: — multi-resolution codebooks : число идентификаторов в кодбуках уменьшается от слоя к слою, чтобы верхние уровни разделяли крупные семантические категории, а нижние — более гранулярные признаки. — progressive masking: модель обучается восстанавливать не полный набор sids, а его префикс. ключевая вещь в архитектуре — дополнительный contrastive learning на rq-vae, который вводит коллаборативный сигнал прямо в процесс токенизации. берутся пары айтемов, встречавшихся рядом в пользовательской истории как позитивные пары, обучается с помощью infonce по батчу. так коллаборативный сигнал тоже участвует в формировании кодбуков без отдельной стадии дообучения как, например, в onerec. в итоге sids начинают отражать не только контентную информацию об айтемах, но и коллаборативные пользовательские связи между ними. 2) continued pre-training (cpt). здесь языковая модель дообучается с увеличенным словарём, в который, помимо изначальных токенов, встроены токены айтемов. модель обучается на смешанной задаче (supervised + self-supervised). цель этой стадии — заставить llm встроить в общее семантическое пространство представления токенов и sids. 3) task-specific fine-tuning. это полноценное обучение на задачу генеративного ретривала: модель предсказывает релевантные айтемы в пользовательских историях (обучение на next token prediction). в целом идея plum строится на прямой аналогии между словами в языковых моделях и айтемами в recsys: если в nlp слова токенизируются для работы с огромным словарём, то в рекомендациях можно аналогично токенизировать айтемы. эксперименты и результаты основная модель — mixture-of-experts с ~900 млн активных параметров (всего 4,2 млрд). в онлайн-a/b-тестах plum показывает рост ключевых метрик: ctr и вовлечённости пользователей, особенно в коротких видео (youtube shorts). аблейшены подтверждают, что важны все предложенные компоненты. в работе показывают законы масштабирования для предложенного фреймворка: при увеличении размера моделей при разном фиксированном вычислительном бюджете ошибки на обучении и валидации снижаются, но самые большие модели (около 3 млрд активных параметров, 20 млрд всего) пока упираются в ограничения вычислительных ресурсов. исследователям не хватило времени, данных и мощностей, чтобы хорошо обучить модели такого размера, однако инженеры считают, что при дальнейшем масштабировании качество может вырасти ещё больше. финальная plum-модель дообучается ежедневно на ~0,25 млрд примеров, тогда как предыдущие lem (large embedding models) подходы требовали многомиллиардных датасетов. @recsyschannel разбор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-07T11:51:48+00:00" href="./posts/193.html">2025-11-07 11:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations<br></strong><br>Сегодня разбираем совместную <a href="https://arxiv.org/abs/2510.07784" rel="nofollow noopener noreferrer">статью</a> Google DeepMind и YouTube. Об этой работе было известно заранее — на конференции RecSys авторы проекта, включая <a href="https://scholar.google.com/citations?user=VuWl-KUAAAAJ&amp;hl=en" rel="nofollow noopener noreferrer">Ed Chi</a> и <a href="https://scholar.google.com/citations?user=20Y73oIAAAAJ&amp;hl=en" rel="nofollow noopener noreferrer">Lichan Hong</a>, упоминали, что готовится статья о генеративных рекомендациях. Через пару недель после конференции она действительно вышла. <br><br>Исследование продолжает трек генеративных рекомендаций, заданный предыдущей работой авторов <a href="https://arxiv.org/pdf/2305.05065" rel="nofollow noopener noreferrer">TIGER</a>. На этот раз основная идея — использование предобученных больших языковых моделей в рекомендательных пайплайнах (в случае Google — это Gemini). Простая LLM из коробки не подходит: модель не знает ни о корпусе айтемов, ни о пользовательских поведенческих сценариях, что приводит к плохим результатам. Чтобы исправить это, команда предлагает фреймворк PLUM, включающий три стадии: item tokenization, continued pre-training и task-specific fine-tuning. Кратко разберём каждую из них.<br><br><strong>1) Item tokenization.</strong> За основу взята работа <a href="https://arxiv.org/pdf/2305.05065" rel="nofollow noopener noreferrer">TIGER</a>. В ней семантические идентификаторы (SIDs) формировались через RQ-VAE поверх текстового описания товара (эксперименты были на открытых датасетах Amazon). В PLUM к этому подходу добавляют коллаборативный сигнал и мультимодальные контентные представления. Используются уже готовые аудио-, видео- и текстовые эмбеддинги YouTube, которые конкатенируются и проходят через энкодер RQ-VAE.<br><br>Новые предложенные компоненты:<br><br>— <strong>Multi-Resolution Codebooks</strong>: число идентификаторов в кодбуках уменьшается от слоя к слою, чтобы верхние уровни разделяли крупные семантические категории, а нижние — более гранулярные признаки.<br>— <strong>Progressive Masking:</strong> модель обучается восстанавливать не полный набор SIDs, а его префикс.<br><br>Ключевая вещь в архитектуре — дополнительный contrastive learning на RQ-VAE, который вводит коллаборативный сигнал прямо в процесс токенизации. Берутся пары айтемов, встречавшихся рядом в пользовательской истории как позитивные пары, обучается с помощью InfoNCE по батчу. Так коллаборативный сигнал тоже участвует в формировании кодбуков без отдельной стадии дообучения как, например, в OneRec. В итоге SIDs начинают отражать не только контентную информацию об айтемах, но и коллаборативные пользовательские связи между ними.<br><br><strong>2) Continued Pre-Training (CPT).</strong> Здесь языковая модель дообучается с увеличенным словарём, в который, помимо изначальных токенов, встроены токены айтемов. Модель обучается на смешанной задаче (supervised + self-supervised). Цель этой стадии — заставить LLM встроить в общее семантическое пространство представления токенов и SIDs.<br><br><strong>3) Task-Specific Fine-Tuning.</strong> Это полноценное обучение на задачу генеративного ретривала: модель предсказывает релевантные айтемы в пользовательских историях (обучение на next token prediction).<br><br>В целом идея PLUM строится на прямой аналогии между словами в языковых моделях и айтемами в RecSys: если в NLP слова токенизируются для работы с огромным словарём, то в рекомендациях можно аналогично токенизировать айтемы.<br><br><strong>Эксперименты и результаты</strong><br><strong><br></strong>Основная модель — Mixture-of-Experts с ~900 млн активных параметров (всего 4,2 млрд). <br><br>В онлайн-A/B-тестах PLUM показывает рост ключевых метрик: CTR и вовлечённости пользователей, особенно в коротких видео (YouTube Shorts). Аблейшены подтверждают, что важны все предложенные компоненты.<br><br>В работе показывают законы масштабирования для предложенного фреймворка: при увеличении размера моделей при разном фиксированном вычислительном бюджете ошибки на обучении и валидации снижаются, но самые большие модели (около 3 млрд активных параметров, 20 млрд всего) пока упираются в ограничения вычислительных ресурсов. Исследователям не хватило времени, данных и мощностей, чтобы хорошо обучить модели такого размера, однако инженеры считают, что при дальнейшем масштабировании качество может вырасти ещё больше.<br><br>Финальная PLUM-модель дообучается ежедневно на ~0,25 млрд примеров, тогда как предыдущие LEM (Large Embedding Models) подходы требовали многомиллиардных датасетов.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/193_480.webp" srcset="../assets/media/thumbs/193_480.webp 480w, ../assets/media/193.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="193" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 364 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/193" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/193.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="192" data-search="tbgrecall: a generative retrieval model for e-commerce recommendation scenarios разбираем работу alibaba, архитектурно напоминающую argus, используемый в рекламе яндекса. модель tbg recall, описанная в статье, генерирует кандидатов для главной страницы taobao, крупнейшего e-commerce-сервиса компании. во многих работах для рекомендаций применяются генеративные и последовательные модели, но они предполагают, что история пользователя — это строгая последовательность событий. в e-commerce всё иначе: пользователь делает запрос и получает «пачку» товаров, потом ещё одну — внутри таких пачек никакой упорядоченности нет, поэтому обычные sequence-based-подходы здесь работают не совсем корректно. в качестве решения авторы вводят предсказание следующей сессии, где сессия понимается как один запрос пользователя. модель учится предсказывать, какие товары пользователь увидит в следующей выдаче.также в работе используют incremental training, чтобы регулярно обновлять модель на свежих данных без перерасхода gpu. архитектура как уже сказали, в основе tbgrecall — next session prediction: история пользователя кодируется в вектор и сравнивается с векторами кандидатов через ann-индекс, как в классических двухбашенных моделях. слово «генеративная» в названии относится не к инференсу, а к способу обучения — авторегрессионному. в начале каждой сессии стоит контекстный токен — обобщённое описание запроса. при инференсе он формируется из текущего контекста пользователя и напрямую влияет на итоговый вектор, с которым рекомендательная система делает запрос в индекс. по нашим наблюдениям, контекстные токены дают почти двукратный прирост качества — особенно в сервисах вроде поиска и рекламы, где контекст крайне важен. кодирование и обучение каждое событие описывается набором признаков: item id, action, sideinfo (id продавца или категория), context и timestamp. вход модели — сумма этих векторов. сначала они проходят через tower-модули, а затем через hstu-блоки. для контекстных и айтемных токенов используются отдельные tower-модули — небольшие проекции, без которых качество падает (что совпадает с нашим опытом в argus). основная схема обучения — session-wise autoregressive approach с маской внимания, которая не позволяет айтемам внутри одной сессии «видеть» друг друга. также применяется session-wise rope (sw-rope) — позиционные эмбеддинги, нумерующие сессии. мы пока не видели стабильного выигрыша от подобных схем, но идея любопытная. лосс состоит из трёх частей: 1. lnce — воспроизводит логирующую политику, учит отличать реальные айтемы в сессии от случайных негативов. 2. lclick — отличает кликнутые айтемы от показанных. 3. lpay — отличает купленные от всех прочих. все три компоненты считаются по разным продуктовым сценариям и взвешиваются по числу сессий в них. отдельного претрейна или fine-tune-фазы, как в argus, нет — всё обучение проходит за один этап. инференс и результаты в проде модель работает не в реальном времени: кандидаты пересчитываются асинхронно и обновляются с небольшой задержкой. авторы считают, что контекст пользователя меняется нечасто, поэтому такая схема не вредит качеству. на закрытом датасете (около 2 трлн записей) tbgrecall превзошёл собственный dual-tower baseline компании. в a/b-тестах модель показала +0,5% по числу транзакций и +2% по обороту. новый кандидат-генератор теперь отвечает за 24% показов на поверхности guess you like — одной из ключевых страниц taobao. в целом, tbgrecall — это шаг от классической двухбашенной архитектуры к генеративному обучению. контекстные токены дают сильный прирост, moe и sw-rope работают стабильно, а near-line-инференс показывает себя лучше, чем ожидалось. @recsyschannel разбор подготовил ❣ николай савушкин tbgrecall: a generative retrieval model for e-commerce recommendation scenarios разбираем работу alibaba, архитектурно напоминающую argus , используемый в рекламе яндекса. модель tbg recall, описанная в статье, генерирует кандидатов для главной страницы taobao, крупнейшего e-commerce-сервиса компании. во многих работах для рекомендаций применяются генеративные и последовательные модели, но они предполагают, что история пользователя — это строгая последовательность событий. в e-commerce всё иначе: пользователь делает запрос и получает «пачку» товаров, потом ещё одну — внутри таких пачек никакой упорядоченности нет, поэтому обычные sequence-based-подходы здесь работают не совсем корректно. в качестве решения авторы вводят предсказание следующей сессии, где сессия понимается как один запрос пользователя. модель учится предсказывать, какие товары пользователь увидит в следующей выдаче.также в работе используют incremental training, чтобы регулярно обновлять модель на свежих данных без перерасхода gpu. архитектура как уже сказали, в основе tbgrecall — next session prediction: история пользователя кодируется в вектор и сравнивается с векторами кандидатов через ann-индекс, как в классических двухбашенных моделях. слово «генеративная» в названии относится не к инференсу, а к способу обучения — авторегрессионному. в начале каждой сессии стоит контекстный токен — обобщённое описание запроса. при инференсе он формируется из текущего контекста пользователя и напрямую влияет на итоговый вектор, с которым рекомендательная система делает запрос в индекс. по нашим наблюдениям, контекстные токены дают почти двукратный прирост качества — особенно в сервисах вроде поиска и рекламы, где контекст крайне важен. кодирование и обучение каждое событие описывается набором признаков: item id, action, sideinfo (id продавца или категория), context и timestamp. вход модели — сумма этих векторов. сначала они проходят через tower-модули, а затем через hstu-блоки. для контекстных и айтемных токенов используются отдельные tower-модули — небольшие проекции, без которых качество падает (что совпадает с нашим опытом в argus). основная схема обучения — session-wise autoregressive approach с маской внимания, которая не позволяет айтемам внутри одной сессии «видеть» друг друга. также применяется session-wise rope (sw-rope) — позиционные эмбеддинги, нумерующие сессии. мы пока не видели стабильного выигрыша от подобных схем, но идея любопытная. лосс состоит из трёх частей: 1. lnce — воспроизводит логирующую политику, учит отличать реальные айтемы в сессии от случайных негативов. 2. lclick — отличает кликнутые айтемы от показанных. 3. lpay — отличает купленные от всех прочих. все три компоненты считаются по разным продуктовым сценариям и взвешиваются по числу сессий в них. отдельного претрейна или fine-tune-фазы, как в argus, нет — всё обучение проходит за один этап. инференс и результаты в проде модель работает не в реальном времени: кандидаты пересчитываются асинхронно и обновляются с небольшой задержкой. авторы считают, что контекст пользователя меняется нечасто, поэтому такая схема не вредит качеству. на закрытом датасете (около 2 трлн записей) tbgrecall превзошёл собственный dual-tower baseline компании. в a/b-тестах модель показала +0,5% по числу транзакций и +2% по обороту. новый кандидат-генератор теперь отвечает за 24% показов на поверхности guess you like — одной из ключевых страниц taobao. в целом, tbgrecall — это шаг от классической двухбашенной архитектуры к генеративному обучению. контекстные токены дают сильный прирост, moe и sw-rope работают стабильно, а near-line-инференс показывает себя лучше, чем ожидалось. @recsyschannel разбор подготовил ❣ николай савушкин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-30T09:17:01+00:00" href="./posts/192.html">2025-10-30 09:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios<br></strong><br>Разбираем <a href="https://arxiv.org/pdf/2508.11977" rel="nofollow noopener noreferrer">работу</a> Alibaba, архитектурно напоминающую <a href="https://arxiv.org/pdf/2302.07589" rel="nofollow noopener noreferrer">ARGUS</a>, используемый в Рекламе Яндекса. Модель TBG Recall, описанная в статье, генерирует кандидатов для главной страницы Taobao, крупнейшего e-commerce-сервиса компании.<br><br>Во многих работах для рекомендаций применяются генеративные и последовательные модели, но они предполагают, что история пользователя — это строгая последовательность событий. В e-commerce всё иначе: пользователь делает запрос и получает «пачку» товаров, потом ещё одну — внутри таких пачек никакой упорядоченности нет, поэтому обычные sequence-based-подходы здесь работают не совсем корректно.<br><br>В качестве решения авторы вводят предсказание следующей сессии, где сессия понимается как один запрос пользователя. Модель учится предсказывать, какие товары пользователь увидит в следующей выдаче.Также в работе используют incremental training, чтобы регулярно обновлять модель на свежих данных без перерасхода GPU.<br><br><strong>Архитектура<br></strong><br>Как уже сказали, в основе TBGRecall — next session prediction: история пользователя кодируется в вектор и сравнивается с векторами кандидатов через ANN-индекс, как в классических двухбашенных моделях. Слово «генеративная» в названии относится не к инференсу, а к способу обучения — авторегрессионному.<br><br>В начале каждой сессии стоит контекстный токен — обобщённое описание запроса. При инференсе он формируется из текущего контекста пользователя и напрямую влияет на итоговый вектор, с которым рекомендательная система делает запрос в индекс. По нашим наблюдениям, контекстные токены дают почти двукратный прирост качества — особенно в сервисах вроде Поиска и Рекламы, где контекст крайне важен.<br><br><strong>Кодирование и обучение</strong><br><br>Каждое событие описывается набором признаков: Item ID, Action, SideInfo (ID продавца или категория), Context и Timestamp. Вход модели — сумма этих векторов. Сначала они проходят через tower-модули, а затем через HSTU-блоки. Для контекстных и айтемных токенов используются отдельные tower-модули — небольшие проекции, без которых качество падает (что совпадает с нашим опытом в ARGUS).<br><br>Основная схема обучения — session-wise autoregressive approach с маской внимания, которая не позволяет айтемам внутри одной сессии «видеть» друг друга. Также применяется session-wise ROPE (sw-ROPE) — позиционные эмбеддинги, нумерующие сессии. Мы пока не видели стабильного выигрыша от подобных схем, но идея любопытная.<br><br>Лосс состоит из трёх частей:<br>1. Lnce — воспроизводит логирующую политику, учит отличать реальные айтемы в сессии от случайных негативов.<br>2. Lclick — отличает кликнутые айтемы от показанных.<br>3. Lpay — отличает купленные от всех прочих.<br><br>Все три компоненты считаются по разным продуктовым сценариям и взвешиваются по числу сессий в них. Отдельного претрейна или fine-tune-фазы, как в ARGUS, нет — всё обучение проходит за один этап.<br><br><strong>Инференс и результаты<br></strong><br>В проде модель работает не в реальном времени: кандидаты пересчитываются асинхронно и обновляются с небольшой задержкой. Авторы считают, что контекст пользователя меняется нечасто, поэтому такая схема не вредит качеству.<br><br>На закрытом датасете (около 2 трлн записей) TBGRecall превзошёл собственный dual-tower baseline компании. В A/B-тестах модель показала +0,5% по числу транзакций и +2% по обороту. Новый кандидат-генератор теперь отвечает за 24% показов на поверхности Guess You Like — одной из ключевых страниц Taobao.<br><br>В целом, TBGRecall — это шаг от классической двухбашенной архитектуры к генеративному обучению. Контекстные токены дают сильный прирост, MoE и SW-ROPE работают стабильно, а near-line-инференс показывает себя лучше, чем ожидалось.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Николай Савушкин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/192_480.webp" srcset="../assets/media/thumbs/192_480.webp 480w, ../assets/media/192.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="192" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 888 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/192" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/192.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="191" data-search="onepiece: bringing context engineering and reasoning to industrial cascade ranking system [2/2] продолжаем разбор техрепорта от shopee. так чем же интересен reasoning? авторы берут hidden state из последнего блока backbone, подают на вход в декодер с блочно-каузальным attention. по словам авторов, блоки позволяют учитывать больше информации о каждом токене. блоки в итоге учатся на разные таски: — retrieval: binary-cross-entropy loss (будет клик или не будет, добавят товар в корзину или нет, купят ли) и bidirectional contrastive learning (симметричный user to item и item to user). — ranking: вместо bcl используют set contrastive learning на успешных случаях, чтобы расширить границы положительных и отрицательных исходов. для тренировки моделей авторы воспроизводят ежедневное онлайн-дообучение, которое ждёт систему в проде. данные упорядочены между собой по дням, но внутри них семплы пошаффлены. результат за каждый день сохраняется и оценивается по итогам следующего. период данных для обучения — месяц. сделав вход модели более информативным, а также добавив многошаговый reasoning, авторы улучшили результаты работы модели. внедрение нового фреймворка в основной сценарий персонализированного поиска помогло добиться +2% gmv/uu и +2,90% дохода от рекламы. @recsyschannel разбор подготовил ❣ виктор януш onepiece: bringing context engineering and reasoning to industrial cascade ranking system [2/2] продолжаем разбор техрепорта от shopee. так чем же интересен reasoning? авторы берут hidden state из последнего блока backbone, подают на вход в декодер с блочно-каузальным attention. по словам авторов, блоки позволяют учитывать больше информации о каждом токене. блоки в итоге учатся на разные таски: — retrieval: binary-cross-entropy loss (будет клик или не будет, добавят товар в корзину или нет, купят ли) и bidirectional contrastive learning (симметричный user to item и item to user). — ranking: вместо bcl используют set contrastive learning на успешных случаях, чтобы расширить границы положительных и отрицательных исходов. для тренировки моделей авторы воспроизводят ежедневное онлайн-дообучение, которое ждёт систему в проде. данные упорядочены между собой по дням, но внутри них семплы пошаффлены. результат за каждый день сохраняется и оценивается по итогам следующего. период данных для обучения — месяц. сделав вход модели более информативным, а также добавив многошаговый reasoning, авторы улучшили результаты работы модели. внедрение нового фреймворка в основной сценарий персонализированного поиска помогло добиться +2% gmv/uu и +2,90% дохода от рекламы. @recsyschannel разбор подготовил ❣ виктор януш">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-23T08:05:54+00:00" href="./posts/191.html">2025-10-23 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System [2/2]</strong><br><br>Продолжаем разбор <a href="https://arxiv.org/abs/2509.18091" rel="nofollow noopener noreferrer">техрепорта</a> от Shopee. Так чем же интересен reasoning? <br><br>Авторы берут hidden state из последнего блока backbone, подают на вход в декодер с блочно-каузальным attention. По словам авторов, блоки позволяют учитывать больше информации о каждом токене. <br><br>Блоки в итоге учатся на разные таски: <br>— Retrieval: binary-cross-entropy loss (будет клик или не будет, добавят товар в корзину или нет, купят ли) и bidirectional contrastive learning (симметричный User to Item и Item to User).<br>— Ranking: вместо BCL используют set contrastive learning на успешных случаях, чтобы расширить границы положительных и отрицательных исходов. <br><br>Для тренировки моделей авторы воспроизводят ежедневное онлайн-дообучение, которое ждёт систему в проде. Данные упорядочены между собой по дням, но внутри них семплы пошаффлены. Результат за каждый день сохраняется и оценивается по итогам следующего. Период данных для обучения — месяц.<br><br>Сделав вход модели более информативным, а также добавив многошаговый reasoning, авторы улучшили результаты работы модели. Внедрение нового фреймворка в основной сценарий персонализированного поиска помогло добиться +2% GMV/UU и +2,90% дохода от рекламы.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Виктор Януш</div>
      <div class="actions">
        <span>1 850 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/191" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/191.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="190" data-search="onepiece: bringing context engineering and reasoning to industrial cascade ranking system [1/2] сегодня разберём очередной техрепорт от shopee — маркетплейса, популярного в южной америке и азии. авторы представляют новый фреймворк onepiece, где адаптируют llm и к retrieval, и к ранжированию. идеи, на которых основан подход, простые, но интересные: — structured context engineering: обогатить историю взаимодействия с пользователями. — block-wise latent reasoning. авторы в некотором роде придумали, как прикрутить к рекомендательным системам reasoning от llm. — progressive multi-task training: прогрессивно усложнять обучающие задачи для учёта фидбека. по названию материала можно было бы подумать, что речь пойдёт про одностадийную модель, но нет. как заведено в современных рекомендательных системах, стадии две: retrieval и ranking. в основе модели — энкодер с трансформером. размеры в статье не приводят, но по косвенным признакам, модель не очень большая. подробности можно рассмотреть на схеме. начнём с retrieval. вход стандартный: история взаимодействий, описание контекста через пользовательские фичи. из интересного — preference anchors, которые помогают собрать топы товаров по количеству покупок, добавлению в корзину или кликов. можно сказать, что это аналог rag (от llm) для рекомендашек. для стадии ранжирования — то же самое, плюс множество кандидатов, как в подходе с target-aware-трансформером. представление входов довольно стандартное. товары описываются набором id: название, магазин, категория. запросы представлены мешком слов. токены получаются с помощью mlp над конкатенацией эмбеддингов. если не использовать маскирование, получится полный attention между всеми кандидатами. чтобы сэкономить compute и избежать артефактов в зависимостях, авторы выбрали промежуточный вариант: делят кандидатов на рандомные группы и подают на вход по одной. backbone тоже стандартный и не стоит отдельного внимания. а вот reasoning интересный. почему? расскажем в следующем посте! @recsyschannel разбор подготовил ❣ виктор януш onepiece: bringing context engineering and reasoning to industrial cascade ranking system [1/2] сегодня разберём очередной техрепорт от shopee — маркетплейса, популярного в южной америке и азии. авторы представляют новый фреймворк onepiece, где адаптируют llm и к retrieval, и к ранжированию. идеи, на которых основан подход, простые, но интересные: — structured context engineering: обогатить историю взаимодействия с пользователями. — block-wise latent reasoning. авторы в некотором роде придумали, как прикрутить к рекомендательным системам reasoning от llm. — progressive multi-task training: прогрессивно усложнять обучающие задачи для учёта фидбека. по названию материала можно было бы подумать, что речь пойдёт про одностадийную модель, но нет. как заведено в современных рекомендательных системах, стадии две: retrieval и ranking. в основе модели — энкодер с трансформером. размеры в статье не приводят, но по косвенным признакам, модель не очень большая. подробности можно рассмотреть на схеме. начнём с retrieval. вход стандартный: история взаимодействий, описание контекста через пользовательские фичи. из интересного — preference anchors, которые помогают собрать топы товаров по количеству покупок, добавлению в корзину или кликов. можно сказать, что это аналог rag (от llm) для рекомендашек. для стадии ранжирования — то же самое, плюс множество кандидатов, как в подходе с target-aware-трансформером. представление входов довольно стандартное. товары описываются набором id: название, магазин, категория. запросы представлены мешком слов. токены получаются с помощью mlp над конкатенацией эмбеддингов. если не использовать маскирование, получится полный attention между всеми кандидатами. чтобы сэкономить compute и избежать артефактов в зависимостях, авторы выбрали промежуточный вариант: делят кандидатов на рандомные группы и подают на вход по одной. backbone тоже стандартный и не стоит отдельного внимания. а вот reasoning интересный. почему? расскажем в следующем посте! @recsyschannel разбор подготовил ❣ виктор януш">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-21T08:33:07+00:00" href="./posts/190.html">2025-10-21 08:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System [1/2]</strong><br><br>Сегодня разберём очередной <a href="https://arxiv.org/abs/2509.18091" rel="nofollow noopener noreferrer">техрепорт</a> от Shopee — маркетплейса, популярного в Южной Америке и Азии. Авторы представляют новый фреймворк OnePiece, где адаптируют LLM и к retrieval, и к ранжированию. <br><br>Идеи, на которых основан подход, простые, но интересные: <br><br>— Structured context engineering: обогатить историю взаимодействия с пользователями.<br>— Block-wise latent reasoning. Авторы в некотором роде придумали, как прикрутить к рекомендательным системам reasoning от LLM.<br>— Progressive multi-task training: прогрессивно усложнять обучающие задачи для учёта фидбека.<br> <br>По названию материала можно было бы подумать, что речь пойдёт про одностадийную модель, но нет. Как заведено в современных рекомендательных системах, стадии две: retrieval и ranking. <br><br>В основе модели — энкодер с трансформером. Размеры в статье не приводят, но по косвенным признакам, модель не очень большая. <br><br>Подробности можно рассмотреть на схеме. Начнём с retrieval. Вход стандартный: история взаимодействий, описание контекста через пользовательские фичи. Из интересного — preference anchors, которые помогают собрать топы товаров по количеству покупок, добавлению в корзину или кликов. Можно сказать, что это аналог RAG (от LLM) для рекомендашек. <br><br>Для стадии ранжирования — то же самое, плюс множество кандидатов, как в подходе с target-aware-трансформером.<br><br>Представление входов довольно стандартное. Товары описываются набором ID: название, магазин, категория. Запросы представлены мешком слов. Токены получаются с помощью MLP над конкатенацией эмбеддингов. <br><br>Если не использовать маскирование, получится полный attention между всеми кандидатами. Чтобы сэкономить compute и избежать артефактов в зависимостях, авторы выбрали промежуточный вариант: делят кандидатов на рандомные группы и подают на вход по одной.<br><br>Backbone тоже стандартный и не стоит отдельного внимания. А вот reasoning интересный. Почему? Расскажем в следующем посте!<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Виктор Януш<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/190_480.webp" srcset="../assets/media/thumbs/190_480.webp 480w, ../assets/media/190.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="190" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 836 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/190" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/190.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="181" data-search="kuaishou: обзор ключевых статей и техрепортов собрали в карточках краткие описания семи больших работ kuaishou, включая те, на основе которых вырос onerec и его продолжения. материал поможет быстро сложить в голове картину того, как компания шаг за шагом пришла к созданию первых генеративных рекомендательных систем в индустрии. ссылки на работы, упомянутые в посте: — onerec: unifying retrieve and rank with generative recommender and iterative preference alignment — onerec technical report — onerec-v2 technical report — qarm: quantitative alignment multi-modal recommendation at kuaishou — twin v2: scaling ultra-long user behavior sequence modeling for enhanced ctr prediction at kuaishou — pantheon: personalized multi-objective ensemble sort via iterative pareto policy optimization — oneloc: geo-aware generative recommender systems for local life service @recsyschannel обзор подготовил ❣ владимир байкалов kuaishou: обзор ключевых статей и техрепортов собрали в карточках краткие описания семи больших работ kuaishou, включая те, на основе которых вырос onerec и его продолжения. материал поможет быстро сложить в голове картину того, как компания шаг за шагом пришла к созданию первых генеративных рекомендательных систем в индустрии. ссылки на работы, упомянутые в посте: — onerec: unifying retrieve and rank with generative recommender and iterative preference alignment — onerec technical report — onerec-v2 technical report — qarm: quantitative alignment multi-modal recommendation at kuaishou — twin v2: scaling ultra-long user behavior sequence modeling for enhanced ctr prediction at kuaishou — pantheon: personalized multi-objective ensemble sort via iterative pareto policy optimization — oneloc: geo-aware generative recommender systems for local life service @recsyschannel обзор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-14T09:31:56+00:00" href="./posts/181.html">2025-10-14 09:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>Kuaishou: обзор ключевых статей и техрепортов</strong><br><strong><br></strong>Собрали в карточках краткие описания семи больших работ Kuaishou, включая те, на основе которых вырос OneRec и его продолжения.<br><br>Материал поможет быстро сложить в голове картину того, как компания шаг за шагом пришла к созданию первых генеративных рекомендательных систем в индустрии.<br><br>Ссылки на работы, упомянутые в посте:<br><br>— <a href="https://arxiv.org/abs/2502.18965?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment</a><br>— <a href="https://arxiv.org/abs/2506.13695?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">OneRec Technical Report</a><br>— <a href="https://arxiv.org/abs/2508.20900?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">OneRec-V2 Technical Report</a><br>— <a href="https://arxiv.org/abs/2411.11739?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou</a><br>— <a href="https://arxiv.org/abs/2407.16357?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou</a><br>— <a href="https://arxiv.org/abs/2505.13894?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">Pantheon: Personalized Multi-objective Ensemble Sort via Iterative Pareto Policy Optimization</a><br>— <a href="https://arxiv.org/abs/2508.14646?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service</a><br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/181_480.webp" srcset="../assets/media/thumbs/181_480.webp 480w, ../assets/media/181.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/182_480.webp" srcset="../assets/media/thumbs/182_480.webp 480w, ../assets/media/182.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/183_480.webp" srcset="../assets/media/thumbs/183_480.webp 480w, ../assets/media/183.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/184_480.webp" srcset="../assets/media/thumbs/184_480.webp 480w, ../assets/media/184.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/185_480.webp" srcset="../assets/media/thumbs/185_480.webp 480w, ../assets/media/185.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/186_480.webp" srcset="../assets/media/thumbs/186_480.webp 480w, ../assets/media/186.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/187_480.webp" srcset="../assets/media/thumbs/187_480.webp 480w, ../assets/media/187.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/188_480.webp" srcset="../assets/media/thumbs/188_480.webp 480w, ../assets/media/188.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/189_480.webp" srcset="../assets/media/thumbs/189_480.webp 480w, ../assets/media/189.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="181" data-image-index="8" /></div></div>
      <div class="actions">
        <span>1 868 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/181" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/181.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="180" data-search="pinrec: outcome-conditioned, multi-token generative retrieval for industry-scale recommendation systems сегодня разбираем статью от pinterest о модели pinrec. в индустрии существуют два основных подхода к transformer-based retrieval. первый — классическая двухбашенная схема: трансформер анализирует пользовательскую историю и сжимает её в эмбеддинг, с которым затем обращаемся к hnsw-индексу, построенному на айтемных эмбеддингах. второй подход — появившиеся относительно недавно генеративные модели, в которых трансформер порождает непосредственно список айтемов, релевантных для данного юзера, например генерируя их в виде последовательностей семантических айдишников. в модели pinrec авторы совмещают обе парадигмы и получают нечто среднее: с одной стороны, трансформер генерирует последовательность, однако это последовательность не айтемных идентификаторов, а сырых эмбеддингов, с каждым из которых затем ходим в hnsw-индекс для поиска ближайших айтемных эмбеддингов. в базовой версии модель представляет собой трансформер с каузальной маской, авторегрессивно предсказывающий каждый следующий айтем, с которым провзаимодействовал пользователь, при условии его предыдущей истории. учится модель на sampled softmax loss с logq-коррекцией и mixed-negative sampling (используются in-batch и random-негативы). на инференсе предсказываем по истории пользователя эмбеддинг следующего айтема, дописываем его в сыром виде в конец истории и повторяем такой процесс несколько раз — в результате генерируем последовательность эмбеддингов и от каждого из них набираем ближайшие айтемы в hnsw-индексе. помимо указанного совмещения парадигм ключевые новшества статьи — это две идеи, навешиваемые поверх базовой версии модели. во-первых, авторы предлагают способ, при помощи которого можно обуславливать генерацию на желаемые действия пользователя — не прибегая к sft и rl. при предсказании следующего айтема будем давать модели информацию о действии, совершенном юзером с этим айтемом. а именно, будем конкатить выход из трансформера, сжимающий предыдущую историю, с эмбеддингом действия и уже из этого конката предсказывать следующий айтем. такая схема позволяет на инференсе подставить эмбеддинг нужного нам действия и предсказать наиболее вероятные айтемы при условии этого действия. во-вторых, авторы замечают, что взаимодействия пользователя с айтемами в сервисе совершенно не обязательно имеют строго последовательную логику и жёсткую очередность. а задача next item prediction как раз предполагает, что для каждой предыстории есть ровно один верный предикт — тот айтем, с которым пользователь провзаимодействовал следующим. в статье предлагается изменить постановку задачи: важно угадать не в точности следующий айтем, а хотя бы один из будущих айтемов в некотором временном окне. для этого вводится такая модификация лосса — обычный sampled softmax loss посчитаем по всем айтемам в этом окне и затем возьмём минимум из полученных значений. тогда и на инференсе можно предсказывать не по одному эмбеддингу за раз, а сразу целыми окнами. в статье утверждается, что это повышает и качество, и разнообразие кандидатов, а за счёт генерации целыми окнами значительно ускоряет инференс. авторы репортят, что описываемая ими модель внедрена как один из кандидатогенераторов в pinterest на весь их внушительный industrial scale. при этом получены значимые приросты онлайн-метрик: на поверхности homefeed +0,28% fulfilled sessions, +0,55% timespent и +3,33% кликов. помимо архитектурных идей статья содержит интересные детали сервинга модели в продакшене (используется triton inference server с python-бэкендом). также авторы сравнивают в своём сетапе трансформер с архитектурой hstu (не в пользу последней), проводят эксперименты со скейлингом трансформера вплоть до миллиарда параметров и репортят, что добавление в модель таблицы id-based-эмбеддингов с 10 миллиардами параметров докидывает +14% к recall@10 поверх только контентных эмбеддингов. @recsyschannel разбор подготовил ❣ сергей лямаев pinrec: outcome-conditioned, multi-token generative retrieval for industry-scale recommendation systems сегодня разбираем статью от pinterest о модели pinrec. в индустрии существуют два основных подхода к transformer-based retrieval. первый — классическая двухбашенная схема: трансформер анализирует пользовательскую историю и сжимает её в эмбеддинг, с которым затем обращаемся к hnsw-индексу, построенному на айтемных эмбеддингах. второй подход — появившиеся относительно недавно генеративные модели, в которых трансформер порождает непосредственно список айтемов, релевантных для данного юзера, например генерируя их в виде последовательностей семантических айдишников. в модели pinrec авторы совмещают обе парадигмы и получают нечто среднее: с одной стороны, трансформер генерирует последовательность, однако это последовательность не айтемных идентификаторов, а сырых эмбеддингов, с каждым из которых затем ходим в hnsw-индекс для поиска ближайших айтемных эмбеддингов. в базовой версии модель представляет собой трансформер с каузальной маской, авторегрессивно предсказывающий каждый следующий айтем, с которым провзаимодействовал пользователь, при условии его предыдущей истории. учится модель на sampled softmax loss с logq-коррекцией и mixed-negative sampling (используются in-batch и random-негативы). на инференсе предсказываем по истории пользователя эмбеддинг следующего айтема, дописываем его в сыром виде в конец истории и повторяем такой процесс несколько раз — в результате генерируем последовательность эмбеддингов и от каждого из них набираем ближайшие айтемы в hnsw-индексе. помимо указанного совмещения парадигм ключевые новшества статьи — это две идеи, навешиваемые поверх базовой версии модели. во-первых, авторы предлагают способ, при помощи которого можно обуславливать генерацию на желаемые действия пользователя — не прибегая к sft и rl. при предсказании следующего айтема будем давать модели информацию о действии, совершенном юзером с этим айтемом. а именно, будем конкатить выход из трансформера, сжимающий предыдущую историю, с эмбеддингом действия и уже из этого конката предсказывать следующий айтем. такая схема позволяет на инференсе подставить эмбеддинг нужного нам действия и предсказать наиболее вероятные айтемы при условии этого действия. во-вторых, авторы замечают, что взаимодействия пользователя с айтемами в сервисе совершенно не обязательно имеют строго последовательную логику и жёсткую очередность. а задача next item prediction как раз предполагает, что для каждой предыстории есть ровно один верный предикт — тот айтем, с которым пользователь провзаимодействовал следующим. в статье предлагается изменить постановку задачи: важно угадать не в точности следующий айтем, а хотя бы один из будущих айтемов в некотором временном окне. для этого вводится такая модификация лосса — обычный sampled softmax loss посчитаем по всем айтемам в этом окне и затем возьмём минимум из полученных значений. тогда и на инференсе можно предсказывать не по одному эмбеддингу за раз, а сразу целыми окнами. в статье утверждается, что это повышает и качество, и разнообразие кандидатов, а за счёт генерации целыми окнами значительно ускоряет инференс. авторы репортят, что описываемая ими модель внедрена как один из кандидатогенераторов в pinterest на весь их внушительный industrial scale. при этом получены значимые приросты онлайн-метрик: на поверхности homefeed +0,28% fulfilled sessions, +0,55% timespent и +3,33% кликов. помимо архитектурных идей статья содержит интересные детали сервинга модели в продакшене (используется triton inference server с python-бэкендом). также авторы сравнивают в своём сетапе трансформер с архитектурой hstu (не в пользу последней), проводят эксперименты со скейлингом трансформера вплоть до миллиарда параметров и репортят, что добавление в модель таблицы id-based-эмбеддингов с 10 миллиардами параметров докидывает +14% к recall@10 поверх только контентных эмбеддингов. @recsyschannel разбор подготовил ❣ сергей лямаев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-09T08:33:00+00:00" href="./posts/180.html">2025-10-09 08:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems</strong><br><strong><br></strong>Сегодня разбираем <a href="https://arxiv.org/abs/2504.10507v1" rel="nofollow noopener noreferrer">статью</a> от Pinterest о модели PinRec. В индустрии существуют два основных подхода к transformer-based retrieval. Первый — классическая двухбашенная схема: трансформер анализирует пользовательскую историю и сжимает её в эмбеддинг, с которым затем обращаемся к HNSW-индексу, построенному на айтемных эмбеддингах. Второй подход — появившиеся относительно недавно генеративные модели, в которых трансформер порождает непосредственно список айтемов, релевантных для данного юзера, например генерируя их в виде последовательностей семантических айдишников. <br><br>В модели PinRec авторы совмещают обе парадигмы и получают нечто среднее: с одной стороны, трансформер генерирует последовательность, однако это последовательность не айтемных идентификаторов, а сырых эмбеддингов, с каждым из которых затем ходим в HNSW-индекс для поиска ближайших айтемных эмбеддингов. <br><br>В базовой версии модель представляет собой трансформер с каузальной маской, авторегрессивно предсказывающий каждый следующий айтем, с которым провзаимодействовал пользователь, при условии его предыдущей истории. Учится модель на sampled softmax loss с logQ-коррекцией и mixed-negative sampling (используются in-batch и random-негативы). На инференсе предсказываем по истории пользователя эмбеддинг следующего айтема, дописываем его в сыром виде в конец истории и повторяем такой процесс несколько раз — в результате генерируем последовательность эмбеддингов и от каждого из них набираем ближайшие айтемы в HNSW-индексе.<br><br>Помимо указанного совмещения парадигм ключевые новшества статьи — это две идеи, навешиваемые поверх базовой версии модели.<br><br>Во-первых, авторы предлагают способ, при помощи которого можно обуславливать генерацию на желаемые действия пользователя — не прибегая к SFT и RL. При предсказании следующего айтема будем давать модели информацию о действии, совершенном юзером с этим айтемом. А именно, будем конкатить выход из трансформера, сжимающий предыдущую историю, с эмбеддингом действия и уже из этого конката предсказывать следующий айтем. Такая схема позволяет на инференсе подставить эмбеддинг нужного нам действия и предсказать наиболее вероятные айтемы при условии этого действия.  <br><br>Во-вторых, авторы замечают, что взаимодействия пользователя с айтемами в сервисе совершенно не обязательно имеют строго последовательную логику и жёсткую очередность. А задача next item prediction как раз предполагает, что для каждой предыстории есть ровно один верный предикт — тот айтем, с которым пользователь провзаимодействовал следующим.<br><br>В статье предлагается изменить постановку задачи: важно угадать не в точности следующий айтем, а хотя бы один из будущих айтемов в некотором временном окне. Для этого вводится такая модификация лосса — обычный sampled softmax loss посчитаем по всем айтемам в этом окне и затем возьмём минимум из полученных значений. Тогда и на инференсе можно предсказывать не по одному эмбеддингу за раз, а сразу целыми окнами. В статье утверждается, что это повышает и качество, и разнообразие кандидатов, а за счёт генерации целыми окнами значительно ускоряет инференс.<br><br>Авторы репортят, что описываемая ими модель внедрена как один из кандидатогенераторов в Pinterest на весь их внушительный industrial scale. При этом получены значимые приросты онлайн-метрик: на поверхности homefeed +0,28% fulfilled sessions, +0,55% timespent и +3,33% кликов.<br><br>Помимо архитектурных идей статья содержит интересные детали сервинга модели в продакшене (используется Triton Inference Server с Python-бэкендом). Также авторы сравнивают в своём сетапе трансформер с архитектурой HSTU (не в пользу последней), проводят эксперименты со скейлингом трансформера вплоть до миллиарда параметров и репортят, что добавление в модель таблицы id-based-эмбеддингов с 10 миллиардами параметров докидывает +14% к recall@10 поверх только контентных эмбеддингов.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Сергей Лямаев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/180_480.webp" srcset="../assets/media/thumbs/180_480.webp 480w, ../assets/media/180.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="180" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 079 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/180" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/180.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="175" data-search="подборка статей с recsys 2025 делимся ещё несколькими работами, которые показались любопытными инженерам яндекса. в сегодняшней подборке: диффузионки, которые генерируют целые плейлисты, борьба с cold start, обучение семантических id на все задачи сразу и презентация с иллюстрациями из мультика «холодное сердце». prompt-to-slate: diffusion models for prompt-conditioned slate generation авторы представили dmsg — диффузионную модель для генерации целых наборов контента (плейлисты, корзины товаров) по текстовому запросу. ключевая идея: вместо ранжирования отдельных элементов сеть учится порождать весь слейт целиком. каждый объект каталога кодируется вектором-эмбеддингом. слейт фиксированной длины представляют как конкатенацию этих векторов. текстовый промпт кодируется трансформером и подаётся в diffusion transformer через cross-attention. диффузионная часть пошагово «разшумляет» случайный вектор в латент слейта. готовые латенты проецируются в ближайшие объекты каталога с фильтрацией дублей. такой подход даёт согласованность набора, стохастичность и разнообразие (несколько валидных слейтов для одного промпта). в экспериментах на музыкальных плейлистах и e-commerce-бандлах модель показала до +17% по ndcg и +6,8% взаимодействий в онлайне. not all impressions are created equal: psychology-informed retention optimization for short-form video recommendation хорошая идея для рексистем с плотным пользовательским сигналом. в таргет ставится ретеншн (вернётся ли пользователь в сервис завтра), а в текущей сессии выделяются пиковый и последний документы — психологически именно они запоминаются и влияют на решение вернуться. для поиска пика используют как положительные, так и отрицательные взаимодействия в сессии. semantic ids for joint generative search and recommendation довольно простая, но, скорее всего, рабочая мысль — учить семантические id документов сразу на все задачи. по сути то же, что и обучение многоголовых сетей, только применительно не к эмбедам, а к семантической токенизации документов. let it go? not quite: addressing item cold start in sequential recommendations with content-based initialization авторы сначала учат эмбеддинги документов только на контенте, а затем доучивают на id, контролируя, чтобы норма изменения эмбеддинга оставалась малой. говорят, это хорошо работает на «холодных» документах, и при этом на «горячих» качество почти не проседает. а ещё в презентации статьи были шикарные иллюстрации с героями из мультика «холодное сердце». @recsyschannel статьи выбрали ❣ александр шуваев и андрей мищенко подборка статей с recsys 2025 делимся ещё несколькими работами, которые показались любопытными инженерам яндекса. в сегодняшней подборке: диффузионки, которые генерируют целые плейлисты, борьба с cold start, обучение семантических id на все задачи сразу и презентация с иллюстрациями из мультика «холодное сердце». prompt-to-slate: diffusion models for prompt-conditioned slate generation авторы представили dmsg — диффузионную модель для генерации целых наборов контента (плейлисты, корзины товаров) по текстовому запросу. ключевая идея: вместо ранжирования отдельных элементов сеть учится порождать весь слейт целиком. каждый объект каталога кодируется вектором-эмбеддингом. слейт фиксированной длины представляют как конкатенацию этих векторов. текстовый промпт кодируется трансформером и подаётся в diffusion transformer через cross-attention. диффузионная часть пошагово «разшумляет» случайный вектор в латент слейта. готовые латенты проецируются в ближайшие объекты каталога с фильтрацией дублей. такой подход даёт согласованность набора, стохастичность и разнообразие (несколько валидных слейтов для одного промпта). в экспериментах на музыкальных плейлистах и e-commerce-бандлах модель показала до +17% по ndcg и +6,8% взаимодействий в онлайне. not all impressions are created equal: psychology-informed retention optimization for short-form video recommendation хорошая идея для рексистем с плотным пользовательским сигналом. в таргет ставится ретеншн (вернётся ли пользователь в сервис завтра), а в текущей сессии выделяются пиковый и последний документы — психологически именно они запоминаются и влияют на решение вернуться. для поиска пика используют как положительные, так и отрицательные взаимодействия в сессии. semantic ids for joint generative search and recommendation довольно простая, но, скорее всего, рабочая мысль — учить семантические id документов сразу на все задачи. по сути то же, что и обучение многоголовых сетей, только применительно не к эмбедам, а к семантической токенизации документов. let it go? not quite: addressing item cold start in sequential recommendations with content-based initialization авторы сначала учат эмбеддинги документов только на контенте, а затем доучивают на id, контролируя, чтобы норма изменения эмбеддинга оставалась малой. говорят, это хорошо работает на «холодных» документах, и при этом на «горячих» качество почти не проседает. а ещё в презентации статьи были шикарные иллюстрации с героями из мультика «холодное сердце». @recsyschannel статьи выбрали ❣ александр шуваев и андрей мищенко">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-02T09:04:46+00:00" href="./posts/175.html">2025-10-02 09:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Подборка статей с RecSys 2025</strong><br><strong><br></strong>Делимся ещё несколькими работами, которые показались любопытными инженерам Яндекса. В сегодняшней подборке: диффузионки, которые генерируют целые плейлисты, борьба с cold start, обучение семантических ID на все задачи сразу и презентация с иллюстрациями из мультика «Холодное сердце».<br><br><a href="https://arxiv.org/html/2408.06883v2" rel="nofollow noopener noreferrer"><strong>Prompt-to-Slate: Diffusion Models for Prompt-Conditioned Slate Generation</strong></a><br><strong><br></strong>Авторы представили DMSG — диффузионную модель для генерации целых наборов контента (плейлисты, корзины товаров) по текстовому запросу. Ключевая идея: вместо ранжирования отдельных элементов сеть учится порождать весь слейт целиком.<br><br>Каждый объект каталога кодируется вектором-эмбеддингом. Слейт фиксированной длины представляют как конкатенацию этих векторов. Текстовый промпт кодируется трансформером и подаётся в Diffusion Transformer через cross-attention. Диффузионная часть пошагово «разшумляет» случайный вектор в латент слейта. Готовые латенты проецируются в ближайшие объекты каталога с фильтрацией дублей.<br><br>Такой подход даёт согласованность набора, стохастичность и разнообразие (несколько валидных слейтов для одного промпта). В экспериментах на музыкальных плейлистах и e-commerce-бандлах модель показала до +17% по NDCG и +6,8% взаимодействий в онлайне.<br><br><a href="https://dl.acm.org/doi/10.1145/3705328.3748122" rel="nofollow noopener noreferrer"><strong>Not All Impressions Are Created Equal: Psychology-Informed Retention Optimization for Short-Form Video Recommendation</strong></a><br><br>Хорошая идея для рексистем с плотным пользовательским сигналом. В таргет ставится  ретеншн (вернётся ли пользователь в сервис завтра), а в текущей сессии выделяются пиковый и последний документы — психологически именно они запоминаются и влияют на решение вернуться. Для поиска пика используют как положительные, так и отрицательные взаимодействия в сессии. <br><br><a href="https://arxiv.org/abs/2508.10478" rel="nofollow noopener noreferrer"><strong>Semantic IDs for Joint Generative Search and Recommendation</strong></a><br><br>Довольно простая, но, скорее всего, рабочая мысль — учить семантические ID документов сразу на все задачи. По сути то же, что и обучение многоголовых сетей, только применительно не к эмбедам, а к семантической токенизации документов.<br><br><a href="https://arxiv.org/abs/2507.19473" rel="nofollow noopener noreferrer"><strong>Let it Go? Not Quite: Addressing Item Cold Start in Sequential Recommendations with Content-Based Initialization</strong></a><br><br>Авторы сначала учат эмбеддинги документов только на контенте, а затем доучивают на ID, контролируя, чтобы норма изменения эмбеддинга оставалась малой. Говорят, это хорошо работает на «холодных» документах, и при этом на «горячих» качество почти не проседает. А ещё в презентации статьи были шикарные иллюстрации с героями из мультика «Холодное сердце». <br><br>@RecSysChannel<br>Статьи выбрали <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Александр Шуваев и Андрей Мищенко<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/175_480.webp" srcset="../assets/media/thumbs/175_480.webp 480w, ../assets/media/175.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="175" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/176_480.webp" srcset="../assets/media/thumbs/176_480.webp 480w, ../assets/media/176.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="175" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/177_480.webp" srcset="../assets/media/thumbs/177_480.webp 480w, ../assets/media/177.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="175" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/178_480.webp" srcset="../assets/media/thumbs/178_480.webp 480w, ../assets/media/178.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="175" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/179_480.webp" srcset="../assets/media/thumbs/179_480.webp 480w, ../assets/media/179.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="175" data-image-index="4" /></div></div>
      <div class="actions">
        <span>1 563 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/175" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/175.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="171" data-search="новые впечатления с recsys 2025 продолжаем смотреть на конференцию recsys глазами инженеров яндекса. сегодня подсветим три интересные работы и вдохновляющий keynote от xavier amatriain. scaling generative recommendations with context parallelism on hierarchical sequential transducers авторы рассказали, как наращивают длину пользовательской истории при обучении hstu-моделей. оказалось, что использование истории длиной более 10 к событий всё ещё даёт прирост продуктовых метрик. работа исключительно инженерная, но полезная для масштабирования используемых длин в истории. исследователи используют подход context parallelism: шардинг q/k/v по длине последовательностей в батче на p частей. при вычислении аттеншна ключи и значения нужно агрегировать со всех частей. вместо стандартной схемы all-gather предлагают использовать all-to-all, чтобы пересылать только нужные блоки. как итог, память под активации и kv-кэш на каждом gpu снизилась в ~1/p, а поддерживаемая длина истории выросла с 3 k до 16 k токенов. rankgraph: unified heterogeneous graph learning for cross-domain recommendation на конференции прозвучало несколько докладов о графовых нейросетях (gnn), но особенно выделился этот. в онлайновых a/b-тестах решение показало рост продуктовых метрик: +0,92% к кликам и +2,82% к конверсиям. для построения графа используются все доступные поверхности — лента, видео, рекламные объявления. формируется единый гетерогенный граф, включающий пользователей и айтемы из разных доменов. модель основана на rgcn (relational graph convolutional network) и обучается на contrastive-лоссы (triplet loss и infonce). для каждого отношения (типа ребра) агрегируются сообщения от соседей этого типа; затем результаты объединяются через «mixer» и обновляют представление узла. ключевой момент — сохранение самопетель (self-loop), чтобы прежнее представление узла также учитывалось при обновлении. модель используется как для кандидат-генерации (user-to-item и item-to-item), так и как источник эмбеддингов пользователей и объектов, которые затем передаются в другие доменные модели в качестве фичей. scaling retrieval for web-scale recommenders: lessons from inverted indexes to embedding search linkedin поделились историей эволюции своей retrieval-системы. начинали, как многие, с инвертированных индексов: решение быстрое и объяснимое, но требует ручного тюнинга (query expansion, переписывание запросов). сверху добавили ml — learning to retrieve, графовые методы, атрибутные связи. это помогало, но ограничения оставались: много ручной работы, оффлайн-билд индекса раз в неделю, больно интегрировать эмбеддинги. следующий шаг — embedding-based retrieval на ann внутри старой системы. но с такой архитектурой тяжело экспериментировать: квантование портило качество, cpu не тянуло, итерации шли медленно. решение — построить с нуля gpu retrieval-систему. теперь это огромные sparse/dense матрицы в gpu-памяти без «костыльного» ann. knn считается честно и быстро, а терм-поиск и эмбеддинги можно гибко комбинировать. внедрили множество оптимизаций: кастомные cuda-кернелы, bfloat16, батчинг, шардирование по регионам. результаты: –75% инфраструктурных затрат и +30% к скорости экспериментов. в продакшене на кейсе job-matching это дало +4,6% к числу поданных заявок и +5,8% к budget utilization в промовакансиях. главный инсайт: inverted index хороши для классического поиска, но в современных ml-рексис они быстро достигают потолка. gpu-based ebr (embedding-based retrieval) даёт гибкость и multi-objective-оптимизацию уже на этапе retrieval, а значит — приносит больше пользы для бизнеса. напоследок — впечатление от выступления xavier amatriain, посвящённого комплексному подходу к развитию рекомендательных систем: главный тезис: нельзя сильно вырасти, если сосредотачиваться на улучшении одной метрики. развитие должно охватывать сразу несколько уровней — пользовательский опыт, в том числе с применением генеративного ai, алгоритмический стек рекомендаций и сам продукт. @recsyschannel заметки собрали ❣ александр шуваев, влад тыцкий, артём ваншулин новые впечатления с recsys 2025 продолжаем смотреть на конференцию recsys глазами инженеров яндекса. сегодня подсветим три интересные работы и вдохновляющий keynote от xavier amatriain. scaling generative recommendations with context parallelism on hierarchical sequential transducers авторы рассказали, как наращивают длину пользовательской истории при обучении hstu-моделей. оказалось, что использование истории длиной более 10 к событий всё ещё даёт прирост продуктовых метрик. работа исключительно инженерная, но полезная для масштабирования используемых длин в истории. исследователи используют подход context parallelism: шардинг q/k/v по длине последовательностей в батче на p частей. при вычислении аттеншна ключи и значения нужно агрегировать со всех частей. вместо стандартной схемы all-gather предлагают использовать all-to-all, чтобы пересылать только нужные блоки. как итог, память под активации и kv-кэш на каждом gpu снизилась в ~1/p, а поддерживаемая длина истории выросла с 3 k до 16 k токенов. rankgraph: unified heterogeneous graph learning for cross-domain recommendation на конференции прозвучало несколько докладов о графовых нейросетях (gnn), но особенно выделился этот. в онлайновых a/b-тестах решение показало рост продуктовых метрик: +0,92% к кликам и +2,82% к конверсиям. для построения графа используются все доступные поверхности — лента, видео, рекламные объявления. формируется единый гетерогенный граф, включающий пользователей и айтемы из разных доменов. модель основана на rgcn (relational graph convolutional network) и обучается на contrastive-лоссы (triplet loss и infonce). для каждого отношения (типа ребра) агрегируются сообщения от соседей этого типа; затем результаты объединяются через «mixer» и обновляют представление узла. ключевой момент — сохранение самопетель (self-loop), чтобы прежнее представление узла также учитывалось при обновлении. модель используется как для кандидат-генерации (user-to-item и item-to-item), так и как источник эмбеддингов пользователей и объектов, которые затем передаются в другие доменные модели в качестве фичей. scaling retrieval for web-scale recommenders: lessons from inverted indexes to embedding search linkedin поделились историей эволюции своей retrieval-системы. начинали, как многие, с инвертированных индексов: решение быстрое и объяснимое, но требует ручного тюнинга (query expansion, переписывание запросов). сверху добавили ml — learning to retrieve, графовые методы, атрибутные связи. это помогало, но ограничения оставались: много ручной работы, оффлайн-билд индекса раз в неделю, больно интегрировать эмбеддинги. следующий шаг — embedding-based retrieval на ann внутри старой системы. но с такой архитектурой тяжело экспериментировать: квантование портило качество, cpu не тянуло, итерации шли медленно. решение — построить с нуля gpu retrieval-систему. теперь это огромные sparse/dense матрицы в gpu-памяти без «костыльного» ann. knn считается честно и быстро, а терм-поиск и эмбеддинги можно гибко комбинировать. внедрили множество оптимизаций: кастомные cuda-кернелы, bfloat16, батчинг, шардирование по регионам. результаты: –75% инфраструктурных затрат и +30% к скорости экспериментов. в продакшене на кейсе job-matching это дало +4,6% к числу поданных заявок и +5,8% к budget utilization в промовакансиях. главный инсайт: inverted index хороши для классического поиска, но в современных ml-рексис они быстро достигают потолка. gpu-based ebr (embedding-based retrieval) даёт гибкость и multi-objective-оптимизацию уже на этапе retrieval, а значит — приносит больше пользы для бизнеса. напоследок — впечатление от выступления xavier amatriain , посвящённого комплексному подходу к развитию рекомендательных систем: главный тезис: нельзя сильно вырасти, если сосредотачиваться на улучшении одной метрики. развитие должно охватывать сразу несколько уровней — пользовательский опыт, в том числе с применением генеративного ai, алгоритмический стек рекомендаций и сам продукт. @recsyschannel заметки собрали ❣ александр шуваев, влад тыцкий, артём ваншулин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-26T12:05:33+00:00" href="./posts/171.html">2025-09-26 12:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Новые впечатления с RecSys 2025<br></strong><br>Продолжаем смотреть на конференцию RecSys глазами инженеров Яндекса. Сегодня подсветим три интересные работы и вдохновляющий keynote от Xavier Amatriain.<br><br><a href="https://arxiv.org/html/2508.04711v1" rel="nofollow noopener noreferrer"><strong>Scaling Generative Recommendations with Context Parallelism<br>on Hierarchical Sequential Transducers<br></strong></a><br>Авторы рассказали, как наращивают длину пользовательской истории при обучении HSTU-моделей. Оказалось, что использование истории длиной более 10 К событий всё ещё даёт прирост продуктовых метрик. Работа исключительно инженерная, но полезная для масштабирования используемых длин в истории.<br><br>Исследователи используют подход context parallelism: шардинг q/k/v по длине последовательностей в батче на P частей. При вычислении аттеншна ключи и значения нужно агрегировать со всех частей. Вместо стандартной схемы all-gather предлагают использовать all-to-all, чтобы пересылать только нужные блоки. Как итог, память под активации и KV-кэш на каждом GPU снизилась в ~1/P, а поддерживаемая длина истории выросла с 3 K до 16 K токенов.<br><br><a href="https://arxiv.org/abs/2509.02942" rel="nofollow noopener noreferrer"><strong>RankGraph: Unified Heterogeneous Graph Learning for<br>Cross-Domain Recommendation<br></strong></a><br>На конференции прозвучало несколько докладов о графовых нейросетях (GNN), но особенно выделился этот. В онлайновых A/B-тестах решение показало рост продуктовых метрик: +0,92% к кликам и +2,82% к конверсиям.<br><br>Для построения графа используются все доступные поверхности — лента, видео, рекламные объявления. Формируется единый гетерогенный граф, включающий пользователей и айтемы из разных доменов. Модель основана на RGCN (Relational Graph Convolutional Network) и обучается на contrastive-лоссы (triplet loss и InfoNCE). Для каждого отношения (типа ребра) агрегируются сообщения от соседей этого типа; затем результаты объединяются через «mixer» и обновляют представление узла.<br><br>Ключевой момент — сохранение самопетель (self-loop), чтобы прежнее представление узла также учитывалось при обновлении. Модель используется как для кандидат-генерации (user-to-item и item-to-item), так и как источник эмбеддингов пользователей и объектов, которые затем передаются в другие доменные модели в качестве фичей.<br><br><a href="https://dl.acm.org/doi/10.1145/3705328.3748116" rel="nofollow noopener noreferrer"><strong>Scaling Retrieval for Web-Scale Recommenders: Lessons from Inverted Indexes to Embedding Search<br></strong></a><br>LinkedIn поделились историей эволюции своей retrieval-системы. Начинали, как многие, с инвертированных индексов: решение быстрое и объяснимое, но требует ручного тюнинга (query expansion, переписывание запросов). Сверху добавили ML — learning to retrieve, графовые методы, атрибутные связи. Это помогало, но ограничения оставались: много ручной работы, оффлайн-билд индекса раз в неделю, больно интегрировать эмбеддинги.<br><br>Следующий шаг — embedding-based retrieval на ANN внутри старой системы. Но с такой архитектурой тяжело экспериментировать: квантование портило качество, CPU не тянуло, итерации шли медленно.<br><br>Решение — построить с нуля GPU retrieval-систему. Теперь это огромные sparse/dense матрицы в GPU-памяти без «костыльного» ANN. KNN считается честно и быстро, а терм-поиск и эмбеддинги можно гибко комбинировать. Внедрили множество оптимизаций: кастомные CUDA-кернелы, bfloat16, батчинг, шардирование по регионам.<br><br>Результаты: –75% инфраструктурных затрат и +30% к скорости экспериментов. В продакшене на кейсе job-matching это дало +4,6% к числу поданных заявок и +5,8% к budget utilization в промовакансиях.<br><br>Главный инсайт: inverted index хороши для классического поиска, но в современных ML-рексис они быстро достигают потолка. GPU-based EBR (Embedding-Based Retrieval) даёт гибкость и multi-objective-оптимизацию уже на этапе retrieval, а значит — приносит больше пользы для бизнеса.<br><br>Напоследок — впечатление от <a href="https://amatria.in/recsys25.html" rel="nofollow noopener noreferrer">выступления Xavier Amatriain</a>, посвящённого комплексному подходу к развитию рекомендательных систем:<br><br><em>Главный тезис: нельзя сильно вырасти, если сосредотачиваться на улучшении одной метрики. Развитие должно охватывать сразу несколько уровней — пользовательский опыт, в том числе с применением генеративного AI, алгоритмический стек рекомендаций и сам продукт.</em><br><br>@RecSysChannel<br>Заметки собрали <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Александр Шуваев, Влад Тыцкий, Артём Ваншулин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/171_480.webp" srcset="../assets/media/thumbs/171_480.webp 480w, ../assets/media/171.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="171" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/172_480.webp" srcset="../assets/media/thumbs/172_480.webp 480w, ../assets/media/172.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="171" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/173_480.webp" srcset="../assets/media/thumbs/173_480.webp 480w, ../assets/media/173.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="171" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/174_480.webp" srcset="../assets/media/thumbs/174_480.webp 480w, ../assets/media/174.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="171" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 610 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/171" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/171.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="162" data-search="продолжаем делиться работами с recsys 2025 второй день конференции запомнился нам не только выступлением александра плошкина с oral&#x27;ом о датасете yambda, но и интересными статьями. некоторые из них собрали в этом посте. longer: scaling up long sequence modeling in industrial recommenders авторы из bytedance обучают модель в неавторегрессивном режиме на 10 000 событий, используя 10 000 gpu. поскольку исследователи не связаны авторегрессивной схемой обучения (hstu, argus), они используют глобальные токены с эмбеддингом пользователя, счётчиками и т. п. также применяется target-aware-подход: эмбеддинг целевого товара подаётся как глобальный токен. в первом слое задействован cross-attention: в запросах (query) — глобальные токены и последние события, в ключах (key) — вся последовательность. таким образом, последовательность сжимается до числа query-токенов на выходе слоя cross-attention. далее идут стандартные слои self-attention с каузальной маской. каузальная маска нужна, чтобы на инференсе переиспользовать kv-кэш. enhancing embedding representation stability in recommendation systems with semantic id исследователи рассказали, как применяют семантический id для повышения стабильности рекламных моделей. в рекламе крайне неравномерное распределение айтемов в датасете, к тому же они быстро меняются (примерно половина корпуса обновляется за шесть дней). поэтому модели с обычными или случайными id со временем деградируют. как решение предложен семантический id, который создаётся на основе контента объявления (текста и картинок). в продакшене он генерируется из шести уровней иерархии (codebooks), из которых составляется префикс разной длины. это позволяет похожим по смыслу объявлениям «обмениваться знаниями» и улучшает офлайн-метрики для новых айтемов и для хвоста распределения. наибольший выигрыш виден в моделях, анализирующих историю взаимодействий пользователя. чтобы оценить влияние на стабильность, замеряют изменение скора модели при замене id на его точную копию. в онлайне показано, что использование семантического id снижает изменение скора на 43%. итог: рост целевой метрики на 0,15%. generalized user representations for large-scale recommendations and downstream tasks интересный постер от spotify. авторы дообучают модели с дневным и даже более коротким интервалом. для аудио и коллаборативных эмбеддингов используются одинаковые по размерности векторы — всего 80. при этом исследователи отмечают, что без стабилизации выходных эмбедов (как для аудио, так и для коллаборативных) система вообще не работала. отдельно видно, что старых пользователей специально не обрабатывают: модель всё ещё пытается восстанавливать очень давний онбординг, хотя это иногда даёт негативный эффект. вероятно, основной акцент сделан на работу с холодными пользователями. любопытно, что для обучения используется автоэнкодер, причём его тренируют ежедневно всего на одном дне данных. для аудиоэмбедов применяется трансформер-энкодер с выборкой из истории, чтобы оставить только наиболее релевантные треки. @recsyschannel работами поделились ❣ александр шуваев, пётр зайдель, даниил бурлаков продолжаем делиться работами с recsys 2025 второй день конференции запомнился нам не только выступлением александра плошкина с oral&amp;#x27;ом о датасете yambda , но и интересными статьями. некоторые из них собрали в этом посте. longer: scaling up long sequence modeling in industrial recommenders авторы из bytedance обучают модель в неавторегрессивном режиме на 10 000 событий, используя 10 000 gpu. поскольку исследователи не связаны авторегрессивной схемой обучения (hstu, argus), они используют глобальные токены с эмбеддингом пользователя, счётчиками и т. п. также применяется target-aware-подход: эмбеддинг целевого товара подаётся как глобальный токен. в первом слое задействован cross-attention: в запросах (query) — глобальные токены и последние события, в ключах (key) — вся последовательность. таким образом, последовательность сжимается до числа query-токенов на выходе слоя cross-attention. далее идут стандартные слои self-attention с каузальной маской. каузальная маска нужна, чтобы на инференсе переиспользовать kv-кэш. enhancing embedding representation stability in recommendation systems with semantic id исследователи рассказали, как применяют семантический id для повышения стабильности рекламных моделей. в рекламе крайне неравномерное распределение айтемов в датасете, к тому же они быстро меняются (примерно половина корпуса обновляется за шесть дней). поэтому модели с обычными или случайными id со временем деградируют. как решение предложен семантический id, который создаётся на основе контента объявления (текста и картинок). в продакшене он генерируется из шести уровней иерархии (codebooks), из которых составляется префикс разной длины. это позволяет похожим по смыслу объявлениям «обмениваться знаниями» и улучшает офлайн-метрики для новых айтемов и для хвоста распределения. наибольший выигрыш виден в моделях, анализирующих историю взаимодействий пользователя. чтобы оценить влияние на стабильность, замеряют изменение скора модели при замене id на его точную копию. в онлайне показано, что использование семантического id снижает изменение скора на 43%. итог: рост целевой метрики на 0,15%. generalized user representations for large-scale recommendations and downstream tasks интересный постер от spotify. авторы дообучают модели с дневным и даже более коротким интервалом. для аудио и коллаборативных эмбеддингов используются одинаковые по размерности векторы — всего 80. при этом исследователи отмечают, что без стабилизации выходных эмбедов (как для аудио, так и для коллаборативных) система вообще не работала. отдельно видно, что старых пользователей специально не обрабатывают: модель всё ещё пытается восстанавливать очень давний онбординг, хотя это иногда даёт негативный эффект. вероятно, основной акцент сделан на работу с холодными пользователями. любопытно, что для обучения используется автоэнкодер, причём его тренируют ежедневно всего на одном дне данных. для аудиоэмбедов применяется трансформер-энкодер с выборкой из истории, чтобы оставить только наиболее релевантные треки. @recsyschannel работами поделились ❣ александр шуваев, пётр зайдель, даниил бурлаков">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-25T16:47:57+00:00" href="./posts/162.html">2025-09-25 16:47 UTC</a></div>
      </div>
      <div class="post-body"><strong>Продолжаем делиться работами с RecSys 2025<br></strong><br>Второй день конференции запомнился нам не только выступлением Александра Плошкина с oral&#x27;ом о датасете <a href="https://t.me/RecSysChannel/103" rel="nofollow noopener noreferrer">Yambda</a>, но и интересными статьями. Некоторые из них собрали в этом посте.<br><br><a href="https://arxiv.org/abs/2505.04421" rel="nofollow noopener noreferrer"><strong>LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders</strong></a><br><br>Авторы из ByteDance обучают модель в неавторегрессивном режиме на 10 000 событий, используя 10 000 GPU. Поскольку исследователи не связаны авторегрессивной схемой обучения (HSTU, Argus), они используют глобальные токены с эмбеддингом пользователя, счётчиками и т. п. Также применяется target-aware-подход: эмбеддинг целевого товара подаётся как глобальный токен.<br><br>В первом слое задействован cross-attention: в запросах (query) — глобальные токены и последние события, в ключах (key) — вся последовательность. Таким образом, последовательность сжимается до числа query-токенов на выходе слоя cross-attention. Далее идут стандартные слои self-attention с каузальной маской. Каузальная маска нужна, чтобы на инференсе переиспользовать KV-кэш.<br><br><a href="https://arxiv.org/abs/2504.02137" rel="nofollow noopener noreferrer"><strong>Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID</strong></a><strong><br></strong><br>Исследователи рассказали, как применяют семантический ID для повышения стабильности рекламных моделей. В рекламе крайне неравномерное распределение айтемов в датасете, к тому же они быстро меняются (примерно половина корпуса обновляется за шесть дней). Поэтому модели с обычными или случайными ID со временем деградируют.<br><br>Как решение предложен семантический ID, который создаётся на основе контента объявления (текста и картинок). В продакшене он генерируется из шести уровней иерархии (codebooks), из которых составляется префикс разной длины. Это позволяет похожим по смыслу объявлениям «обмениваться знаниями» и улучшает офлайн-метрики для новых айтемов и для хвоста распределения. Наибольший выигрыш виден в моделях, анализирующих историю взаимодействий пользователя.<br><br>Чтобы оценить влияние на стабильность, замеряют изменение скора модели при замене ID на его точную копию. В онлайне показано, что использование семантического ID снижает изменение скора на 43%. Итог: рост целевой метрики на 0,15%.<br><br><a href="https://dl.acm.org/doi/10.1145/3705328.3748132" rel="nofollow noopener noreferrer"><strong>Generalized User Representations for Large-Scale Recommendations and Downstream Tasks</strong></a><br><br>Интересный постер от Spotify. Авторы дообучают модели с дневным и даже более коротким интервалом. Для аудио и коллаборативных эмбеддингов используются одинаковые по размерности векторы — всего 80. При этом исследователи отмечают, что без стабилизации выходных эмбедов (как для аудио, так и для коллаборативных) система вообще не работала.<br><br>Отдельно видно, что старых пользователей специально не обрабатывают: модель всё ещё пытается восстанавливать очень давний онбординг, хотя это иногда даёт негативный эффект. Вероятно, основной акцент сделан на работу с холодными пользователями.<br><br>Любопытно, что для обучения используется автоэнкодер, причём его тренируют ежедневно всего на одном дне данных. Для аудиоэмбедов применяется трансформер-энкодер с выборкой из истории, чтобы оставить только наиболее релевантные треки.<br><br>@RecSysChannel<br>Работами поделились <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Александр Шуваев, Пётр Зайдель, Даниил Бурлаков<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/162_480.webp" srcset="../assets/media/thumbs/162_480.webp 480w, ../assets/media/162.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/163_480.webp" srcset="../assets/media/thumbs/163_480.webp 480w, ../assets/media/163.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/164_480.webp" srcset="../assets/media/thumbs/164_480.webp 480w, ../assets/media/164.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/165_480.webp" srcset="../assets/media/thumbs/165_480.webp 480w, ../assets/media/165.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/166_480.webp" srcset="../assets/media/thumbs/166_480.webp 480w, ../assets/media/166.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/167_480.webp" srcset="../assets/media/thumbs/167_480.webp 480w, ../assets/media/167.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/168_480.webp" srcset="../assets/media/thumbs/168_480.webp 480w, ../assets/media/168.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/169_480.webp" srcset="../assets/media/thumbs/169_480.webp 480w, ../assets/media/169.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/170_480.webp" srcset="../assets/media/thumbs/170_480.webp 480w, ../assets/media/170.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="162" data-image-index="8" /></div></div>
      <div class="actions">
        <span>1 203 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/162" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/162.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="160" data-search="что обсуждают на recsys 2025 прямо сейчас в праге проходит 19-я международная конференция о рекомендательных системах. по традиции, делимся с вами самым интересным. вот как прошли воркшопы, на которых побывали наши коллеги. practical bandits: an industry perspective этот доклад мы услышали на воркшопе consequences’25. сначала спикеры разобрали различия между off-policy- и on-policy-стратегиями и подробно рассказали, что такое importance weighting, inverse propensity scoring (ips) и для чего они используются. а потом перешли к сбору данных: — показали методы сбора: ε-greedy, softmax и гибридный подход. — ввели effective sample size — оценку того, сколько данных нужно собрать. — уточнили, какие данные необходимо логировать: контекст, все возможные и выбранные действия, награду и распределение вероятностей. после этого перешли к тому, что делать, если некоторые действия блокируются (например, из-за бизнес-логики) и как выявлять смещение с помощью control variates. отдельно отметили проблему symbiosis bias — явление, когда разные политики начинают зависеть друг от друга из-за обучения на всех данных что есть. а завершили всё обсуждением большой кардинальности множества действий и решениям проблем, которые из-за этого возникают. gen ai for e-commerce докладов было много. несколько спикеров поделились опытом того, как используют llm в e-com: генерируют фичи для классического ml, пишут заголовки для e-mail-рассылок, создают поисковые саджесты, размечают данные для active learning, собирают системы из нескольких агентов, чтобы генерировать тексты, привлекающие пользователей. доклады интересные, где-то перекликаются с тем, что мы пробуем делать в яндекс go. но ни в одном из выступлений не услышал, как применение llm бустит метрики, связанные с деньгами — в лучшем случае менялись прокси-метрики. как я понял (и уточнил на стендах), самое популярное решение — не хостить llm самим, а ходить в api готовых ии и платить за токены. было весело, когда у докладчика, который рассказывал про llm для active learning, спросили, сколько они потратили на openai api — в выступлении упоминалось 1+ млн запросов. немного удивило, что существенная часть докладчиков не тестировала свои решения в a/b, только планирует сделать это в будущем. на конференции в этом году — не протолкнуться. кому-то даже пришлось обедать на лестнице. кто знает, может, именно эти воркшопы коллеги обсуждают за трапезой 👀 @recsyschannel суммаризировали для вас воркшопы ❣ михаил сёмин и алексей ельчанинов сгенерировал фото ❣ андрей мищенко что обсуждают на recsys 2025 прямо сейчас в праге проходит 19-я международная конференция о рекомендательных системах. по традиции, делимся с вами самым интересным. вот как прошли воркшопы, на которых побывали наши коллеги. practical bandits: an industry perspective этот доклад мы услышали на воркшопе consequences’25. сначала спикеры разобрали различия между off-policy- и on-policy-стратегиями и подробно рассказали, что такое importance weighting, inverse propensity scoring (ips) и для чего они используются. а потом перешли к сбору данных: — показали методы сбора: ε-greedy, softmax и гибридный подход. — ввели effective sample size — оценку того, сколько данных нужно собрать. — уточнили, какие данные необходимо логировать: контекст, все возможные и выбранные действия, награду и распределение вероятностей. после этого перешли к тому, что делать, если некоторые действия блокируются (например, из-за бизнес-логики) и как выявлять смещение с помощью control variates. отдельно отметили проблему symbiosis bias — явление, когда разные политики начинают зависеть друг от друга из-за обучения на всех данных что есть. а завершили всё обсуждением большой кардинальности множества действий и решениям проблем, которые из-за этого возникают. gen ai for e-commerce докладов было много. несколько спикеров поделились опытом того, как используют llm в e-com: генерируют фичи для классического ml, пишут заголовки для e-mail-рассылок, создают поисковые саджесты, размечают данные для active learning, собирают системы из нескольких агентов, чтобы генерировать тексты, привлекающие пользователей. доклады интересные, где-то перекликаются с тем, что мы пробуем делать в яндекс go. но ни в одном из выступлений не услышал, как применение llm бустит метрики, связанные с деньгами — в лучшем случае менялись прокси-метрики. как я понял (и уточнил на стендах), самое популярное решение — не хостить llm самим, а ходить в api готовых ии и платить за токены. было весело, когда у докладчика, который рассказывал про llm для active learning, спросили, сколько они потратили на openai api — в выступлении упоминалось 1+ млн запросов. немного удивило, что существенная часть докладчиков не тестировала свои решения в a/b, только планирует сделать это в будущем. на конференции в этом году — не протолкнуться. кому-то даже пришлось обедать на лестнице. кто знает, может, именно эти воркшопы коллеги обсуждают за трапезой 👀 @recsyschannel суммаризировали для вас воркшопы ❣ михаил сёмин и алексей ельчанинов сгенерировал фото ❣ андрей мищенко">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-24T15:15:01+00:00" href="./posts/160.html">2025-09-24 15:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что обсуждают на RecSys 2025</strong><br><br>Прямо сейчас в Праге проходит 19-я международная конференция о рекомендательных системах. По традиции, делимся с вами самым интересным. Вот как прошли воркшопы, на которых побывали наши коллеги.<br><br><strong>Practical Bandits: An Industry Perspective</strong><br><em>Этот доклад мы услышали на воркшопе CONSEQUENCES’25. Сначала спикеры разобрали различия между off-policy- и on-policy-стратегиями и подробно рассказали, что такое importance weighting, Inverse Propensity Scoring (IPS) и для чего они используются. А потом перешли к сбору данных:<br><br>— Показали методы сбора: ε-greedy, softmax и гибридный подход. <br>— Ввели effective sample size — оценку того, сколько данных нужно собрать.<br>— Уточнили, какие данные необходимо логировать: контекст, все возможные и выбранные действия, награду и распределение вероятностей.<br><br>После этого перешли к тому, что делать, если некоторые действия блокируются (например, из-за бизнес-логики) и как выявлять смещение с помощью control variates.<br><br>Отдельно отметили проблему symbiosis bias — явление, когда разные политики начинают зависеть друг от друга из-за обучения на всех данных что есть. А завершили всё обсуждением большой кардинальности множества действий и решениям проблем, которые из-за этого возникают.<br></em><br><strong>Gen AI for E-commerce</strong><br><em>Докладов было много. Несколько спикеров поделились опытом того, как используют LLM в E-com: генерируют фичи для классического ML, пишут заголовки для e-mail-рассылок, создают поисковые саджесты, размечают данные для active learning, собирают системы из нескольких агентов, чтобы генерировать тексты, привлекающие пользователей. <br><br>Доклады интересные, где-то перекликаются с тем, что мы пробуем делать в Яндекс Go. Но ни в одном из выступлений не услышал, как применение LLM бустит метрики, связанные с деньгами — в лучшем случае менялись прокси-метрики. <br><br>Как я понял (и уточнил на стендах), самое популярное решение — не хостить LLM самим, а ходить в API готовых ИИ и платить за токены. Было весело, когда у докладчика, который рассказывал про LLM для active learning, спросили, сколько они потратили на OpenAI API — в выступлении упоминалось 1+ млн запросов. <br><br>Немного удивило, что существенная часть докладчиков не тестировала свои решения в A/B, только планирует сделать это в будущем.<br></em><br>На конференции в этом году — не протолкнуться. Кому-то даже пришлось обедать на лестнице. Кто знает, может, именно эти воркшопы коллеги обсуждают за трапезой 👀<br><br>@RecSysChannel<br>Суммаризировали для вас воркшопы <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Михаил Сёмин и Алексей Ельчанинов<br>Сгенерировал фото <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Андрей Мищенко<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/160_480.webp" srcset="../assets/media/thumbs/160_480.webp 480w, ../assets/media/160.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="160" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/161_480.webp" srcset="../assets/media/thumbs/161_480.webp 480w, ../assets/media/161.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="160" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 357 просмотров · 32 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/160" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/160.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="148" data-search="recsys 2025: интересные статьи первого дня вчера в праге стартовала конференция recsys 2025. первый день был посвящён в основном воркшопам. в промежутках можно было посмотреть постеры и пообщаться с авторами. именно этим занимались инженеры яндекса, которые уже разобрали несколько интересных работ. in-context learning for addressing user cold-start in sequential movie recommenders авторы из amazon используют sequential models (модели, основанные на цепочке событий пользователя) для задачи рекомендации видео, так как такие модели дают лучшее качество. в своих подходах указывают recformer, sasrec, gru4rec, tiger, liger. однако подобные модели чувствительны к проблеме холодного старта. когда у пользователя ещё нет никакой истории, что ему показать? по данным авторов, таких пользователей — большинство: 47%, а еще 46% — имеют длину истории до пяти событий. в качестве решения исследователи предлагают добавить к реальной истории пользователя выдуманную llm (imaginary interactions). её получают с помощью специально подготовленного промпта. причём утверждают, что не так страшно, если модель сгаллюцинирует и вернёт несуществующие фильмы, так как это не финальная последовательность. затем происходит объединение выдуманной истории с реальной. в работе используют два подхода: — early fusion — просто объединяют выдуманную историю с реальной (последняя — реальная), формируя одну длинную последовательность; — late fusion — генерируют k последовательностей независимо, каждую продолжают реальной, а потом делают avg pooling над эмбедами. в экспериментах авторы репортят два датасета: публичный the movielens 1m и проприетарный the amazon proprietary. early fusion лучше себя показал на публичном датасете, причём бустит он именно «холодных» пользователей, тогда как на более «горячих» его влияние пропадает. а вот на проприетарном датасете лучше сработала late fusion. это объясняют тем, что подход добавляет разнообразия выдаче. следующие шаги: — из k произвольных фильмов заданного жанра предложить llm выбрать подходящий; — добавить rag; — собирать информацию для «холодных» пользователей путём опроса. denserec: revisiting dense content embeddings for sequential transformer-based recommendation основные идеи: — sasrec хорошо работает, но плохо справляется с cold items. надо поправить эту проблему (другие подходы, например, semantic ids требуют сильного изменения всего пайплайна). — предлагается использовать контентные фичи. но замена «в лоб» просаживает качество. — предлагается выучить модель, которая будет работать поверх всё той же embedding table по id в части случаев, но также научиться переводить в это пространство контентные фичи. — формально при обучении подбрасывают монетку для каждой позиции в последовательности айтемов, с вероятностью p берут эмбед из таблицы эмбеддингов, с вероятностью (1-p) берут конктентные фичи и с помощью простой модели (в данном случае — линейной проекции) переводят контентные эмбеды в пространство обычных. — при инференсе для знакомых id всегда используют таблицу эмбедов, для новых — конкретные фичи и линейный слой проекции. — в экспериментах на датасете amazon авторы показывают значимое улучшение метрик, причём основной прирост — не на «холодных» документах. авторы объясняют это тем, что подход обучения с использованием контентных фичей не только улучшает их представление (new items as target), но и улучшает качество самой последовательности (new items in the sequence). @recsyschannel статьи заметил ❣ артём ваншулин recsys 2025: интересные статьи первого дня вчера в праге стартовала конференция recsys 2025. первый день был посвящён в основном воркшопам. в промежутках можно было посмотреть постеры и пообщаться с авторами. именно этим занимались инженеры яндекса, которые уже разобрали несколько интересных работ. in-context learning for addressing user cold-start in sequential movie recommenders авторы из amazon используют sequential models (модели, основанные на цепочке событий пользователя) для задачи рекомендации видео, так как такие модели дают лучшее качество. в своих подходах указывают recformer, sasrec, gru4rec, tiger, liger. однако подобные модели чувствительны к проблеме холодного старта. когда у пользователя ещё нет никакой истории, что ему показать? по данным авторов, таких пользователей — большинство: 47%, а еще 46% — имеют длину истории до пяти событий. в качестве решения исследователи предлагают добавить к реальной истории пользователя выдуманную llm (imaginary interactions). её получают с помощью специально подготовленного промпта. причём утверждают, что не так страшно, если модель сгаллюцинирует и вернёт несуществующие фильмы, так как это не финальная последовательность. затем происходит объединение выдуманной истории с реальной. в работе используют два подхода: — early fusion — просто объединяют выдуманную историю с реальной (последняя — реальная), формируя одну длинную последовательность; — late fusion — генерируют k последовательностей независимо, каждую продолжают реальной, а потом делают avg pooling над эмбедами. в экспериментах авторы репортят два датасета: публичный the movielens 1m и проприетарный the amazon proprietary. early fusion лучше себя показал на публичном датасете, причём бустит он именно «холодных» пользователей, тогда как на более «горячих» его влияние пропадает. а вот на проприетарном датасете лучше сработала late fusion. это объясняют тем, что подход добавляет разнообразия выдаче. следующие шаги: — из k произвольных фильмов заданного жанра предложить llm выбрать подходящий; — добавить rag; — собирать информацию для «холодных» пользователей путём опроса. denserec: revisiting dense content embeddings for sequential transformer-based recommendation основные идеи: — sasrec хорошо работает, но плохо справляется с cold items. надо поправить эту проблему (другие подходы, например, semantic ids требуют сильного изменения всего пайплайна). — предлагается использовать контентные фичи. но замена «в лоб» просаживает качество. — предлагается выучить модель, которая будет работать поверх всё той же embedding table по id в части случаев, но также научиться переводить в это пространство контентные фичи. — формально при обучении подбрасывают монетку для каждой позиции в последовательности айтемов, с вероятностью p берут эмбед из таблицы эмбеддингов, с вероятностью (1-p) берут конктентные фичи и с помощью простой модели (в данном случае — линейной проекции) переводят контентные эмбеды в пространство обычных. — при инференсе для знакомых id всегда используют таблицу эмбедов, для новых — конкретные фичи и линейный слой проекции. — в экспериментах на датасете amazon авторы показывают значимое улучшение метрик, причём основной прирост — не на «холодных» документах. авторы объясняют это тем, что подход обучения с использованием контентных фичей не только улучшает их представление (new items as target), но и улучшает качество самой последовательности (new items in the sequence). @recsyschannel статьи заметил ❣ артём ваншулин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-23T09:01:01+00:00" href="./posts/148.html">2025-09-23 09:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>RecSys 2025: интересные статьи первого дня<br></strong><br>Вчера в Праге стартовала конференция RecSys 2025. Первый день был посвящён в основном воркшопам. В промежутках можно было посмотреть постеры и пообщаться с авторами. Именно этим занимались инженеры Яндекса, которые уже разобрали несколько интересных работ.<br><br><a href="https://dl.acm.org/doi/10.1145/3705328.3748109" rel="nofollow noopener noreferrer"><strong>In-Context Learning for Addressing User Cold-Start in Sequential Movie Recommenders</strong></a><br><br>Авторы из Amazon используют sequential models (модели, основанные на цепочке событий пользователя) для задачи рекомендации видео, так как такие модели дают лучшее качество. В своих подходах указывают Recformer, SASRec, GRU4Rec, Tiger, Liger. Однако подобные модели чувствительны к проблеме холодного старта. Когда у пользователя ещё нет никакой истории, что ему показать? По данным авторов, таких пользователей — большинство: 47%, а еще 46% — имеют длину истории до пяти событий.<br><br>В качестве решения исследователи предлагают добавить к реальной истории пользователя выдуманную LLM (imaginary interactions). Её получают с помощью специально подготовленного промпта. Причём утверждают, что не так страшно, если модель сгаллюцинирует и вернёт несуществующие фильмы, так как это не финальная последовательность. Затем происходит объединение выдуманной истории с реальной. В работе используют два подхода:<br><br>— early fusion — просто объединяют выдуманную историю с реальной (последняя — реальная), формируя одну длинную последовательность;<br>— late fusion — генерируют k последовательностей независимо, каждую продолжают реальной, а потом делают avg pooling над эмбедами.<br><br>В экспериментах авторы репортят два датасета: публичный the MovieLens 1M и проприетарный the Amazon Proprietary. Early fusion лучше себя показал на публичном датасете, причём бустит он именно «холодных» пользователей, тогда как на более «горячих» его влияние пропадает. А вот на проприетарном датасете лучше сработала late fusion. Это объясняют тем, что подход добавляет разнообразия выдаче.<br><br>Следующие шаги:<br>— из k произвольных фильмов заданного жанра предложить LLM выбрать подходящий;<br>— добавить RAG;<br>— собирать информацию для «холодных» пользователей путём опроса.<br><br><a href="https://arxiv.org/html/2508.18442v1" rel="nofollow noopener noreferrer"><strong>DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation</strong></a><br><br>Основные идеи:<br>— SASRec хорошо работает, но плохо справляется с cold items. Надо поправить эту проблему (другие подходы, например, semantic IDs требуют сильного изменения всего пайплайна).<br>— Предлагается использовать контентные фичи. Но замена «в лоб» просаживает качество.<br>— Предлагается выучить модель, которая будет работать поверх всё той же embedding table по ID в части случаев, но также научиться переводить в это пространство контентные фичи.<br>— Формально при обучении подбрасывают монетку для каждой позиции в последовательности айтемов, с вероятностью p берут эмбед из таблицы эмбеддингов, с вероятностью (1-p) берут конктентные фичи и с помощью простой модели (в данном случае — линейной проекции) переводят контентные эмбеды в пространство обычных.<br>— При инференсе для знакомых ID всегда используют таблицу эмбедов, для новых — конкретные фичи и линейный слой проекции.<br>— В экспериментах на датасете Amazon авторы показывают значимое улучшение метрик, причём основной прирост — не на «холодных» документах. Авторы объясняют это тем, что подход обучения с использованием контентных фичей не только улучшает их представление (new items as target), но и улучшает качество самой последовательности (new items in the sequence).<br><br>@RecSysChannel<br>Статьи заметил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Артём Ваншулин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/148_480.webp" srcset="../assets/media/thumbs/148_480.webp 480w, ../assets/media/148.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/149_480.webp" srcset="../assets/media/thumbs/149_480.webp 480w, ../assets/media/149.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/150_480.webp" srcset="../assets/media/thumbs/150_480.webp 480w, ../assets/media/150.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/151_480.webp" srcset="../assets/media/thumbs/151_480.webp 480w, ../assets/media/151.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/152_480.webp" srcset="../assets/media/thumbs/152_480.webp 480w, ../assets/media/152.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/153_480.webp" srcset="../assets/media/thumbs/153_480.webp 480w, ../assets/media/153.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/154_480.webp" srcset="../assets/media/thumbs/154_480.webp 480w, ../assets/media/154.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/155_480.webp" srcset="../assets/media/thumbs/155_480.webp 480w, ../assets/media/155.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/156_480.webp" srcset="../assets/media/thumbs/156_480.webp 480w, ../assets/media/156.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/157_480.webp" srcset="../assets/media/thumbs/157_480.webp 480w, ../assets/media/157.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="148" data-image-index="9" /></div></div>
      <div class="actions">
        <span>1 360 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/148" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/148.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="147" data-search="large foundation model for ads recommendation сегодня разбираем свежую статью tencent с интригующим названием, содержащим слова large и foundation. обращает на себя внимание и список авторов: он очень длинный, что обычно указывает на масштабный внутренний проект, важный для компании. в работе предлагают инкорпорировать большую вычислительно дорогую foundation-модель в более компактные ctr-модели ранжирования. но авторов не устраивает простое подключение выходов в качестве эмбеддингов или скалярных признаков. инженеры хотят использовать знания большой модели более умным способом, сохраняя эффективность в проде. авторы пишут, что обычно большие foundation-модели используют только user-представления, игнорируя другие важные сигналы. предлагается перенести в downstream-модель все три вида: user-, item- и user-item-представления. напрямую работать с сырыми кросс-представлениями невозможно: они жёстко привязаны к конкретным парам user–item, и для каждой такой пары пришлось бы вычислять большую модель в онлайне. именно этого авторы стараются избежать, предлагая обновлять и хранить агрегированные user- и item-векторы асинхронно. интересная находка: лучшие результаты даёт не использование последнего слоя модели, а извлечение представлений из предпоследнего, хотя замеры противоречивые — на графиках виден шум. архитектура triple tower для обучения используется так называемый triple tower design: — user-башня, — item-башня, — mix-tower для их взаимодействия. при этом архитектура разделена на две ветви (dual-branch design): одна обучается на органическом контенте (просмотры, лайки, комментарии), другая — на рекламных сэмплах (клики, конверсии). user- и item-вектора остаются общими, а cross-вектор извлекается только из рекламной ветви, так как он ближе к целевым downstream-задачам. авторы описывают три способа интеграции foundation-модели в downstream ctr-модель: добавление представлений в качестве новых фичей, подключение блока обработки внутри архитектуры, использование всей большой модели для генерации кандидатов. простое добавление эмбеддингов в downstream-модель работает плохо: пробовали и линейные проекции, и alignment-лоссы, но улучшений не добились. вместо этого применяют другой приём: каждую входную фичу комбинируют с представлением из foundation-модели с помощью покомпонентного умножения и нелинейности. таким образом, user-item-вектор встраивается в модель уже на уровне входных признаков. эксперименты и результаты валидацию делали только на внутренних данных tencent: больших датасетах с рекламными и органическими действиями, онлайн-a/b-тестах. авторы пишут что систему внедрили уже в десяти с лишним продуктах экосистемы и получили рост gmv на 2,45% по всей платформе. больше о внедрении фундаментальных моделей применительно к экосистеме яндекса можно узнать в канале руководителя службы рекомендательных технологий николая савушкина — @light_from_black_box. @recsyschannel разбор подготовил ❣ николай савушкин large foundation model for ads recommendation сегодня разбираем свежую статью tencent с интригующим названием, содержащим слова large и foundation. обращает на себя внимание и список авторов: он очень длинный, что обычно указывает на масштабный внутренний проект, важный для компании. в работе предлагают инкорпорировать большую вычислительно дорогую foundation-модель в более компактные ctr-модели ранжирования. но авторов не устраивает простое подключение выходов в качестве эмбеддингов или скалярных признаков. инженеры хотят использовать знания большой модели более умным способом, сохраняя эффективность в проде. авторы пишут, что обычно большие foundation-модели используют только user-представления, игнорируя другие важные сигналы. предлагается перенести в downstream-модель все три вида: user-, item- и user-item-представления. напрямую работать с сырыми кросс-представлениями невозможно: они жёстко привязаны к конкретным парам user–item, и для каждой такой пары пришлось бы вычислять большую модель в онлайне. именно этого авторы стараются избежать, предлагая обновлять и хранить агрегированные user- и item-векторы асинхронно. интересная находка: лучшие результаты даёт не использование последнего слоя модели, а извлечение представлений из предпоследнего, хотя замеры противоречивые — на графиках виден шум. архитектура triple tower для обучения используется так называемый triple tower design: — user-башня, — item-башня, — mix-tower для их взаимодействия. при этом архитектура разделена на две ветви (dual-branch design): одна обучается на органическом контенте (просмотры, лайки, комментарии), другая — на рекламных сэмплах (клики, конверсии). user- и item-вектора остаются общими, а cross-вектор извлекается только из рекламной ветви, так как он ближе к целевым downstream-задачам. авторы описывают три способа интеграции foundation-модели в downstream ctr-модель: добавление представлений в качестве новых фичей, подключение блока обработки внутри архитектуры, использование всей большой модели для генерации кандидатов. простое добавление эмбеддингов в downstream-модель работает плохо: пробовали и линейные проекции, и alignment-лоссы, но улучшений не добились. вместо этого применяют другой приём: каждую входную фичу комбинируют с представлением из foundation-модели с помощью покомпонентного умножения и нелинейности. таким образом, user-item-вектор встраивается в модель уже на уровне входных признаков. эксперименты и результаты валидацию делали только на внутренних данных tencent: больших датасетах с рекламными и органическими действиями, онлайн-a/b-тестах. авторы пишут что систему внедрили уже в десяти с лишним продуктах экосистемы и получили рост gmv на 2,45% по всей платформе. больше о внедрении фундаментальных моделей применительно к экосистеме яндекса можно узнать в канале руководителя службы рекомендательных технологий николая савушкина — @light_from_black_box . @recsyschannel разбор подготовил ❣ николай савушкин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-17T12:12:20+00:00" href="./posts/147.html">2025-09-17 12:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Large Foundation Model for Ads Recommendation</strong><br><strong> <br></strong>Сегодня разбираем свежую <a href="https://arxiv.org/html/2508.14948v1" rel="nofollow noopener noreferrer">статью</a> Tencent с интригующим названием, содержащим слова large и foundation. Обращает на себя внимание и список авторов: он очень длинный, что обычно указывает на масштабный внутренний проект, важный для компании.<br><br>В работе предлагают инкорпорировать большую вычислительно дорогую foundation-модель в более компактные CTR-модели ранжирования. Но авторов не устраивает простое подключение выходов в качестве эмбеддингов или скалярных признаков. Инженеры хотят использовать знания большой модели более умным способом, сохраняя эффективность в проде.<br><br>Авторы пишут, что обычно большие foundation-модели используют только user-представления, игнорируя другие важные сигналы. Предлагается перенести в downstream-модель все три вида: user-, item- и user-item-представления.<br><br>Напрямую работать с сырыми кросс-представлениями невозможно: они жёстко привязаны к конкретным парам user–item, и для каждой такой пары пришлось бы вычислять большую модель в онлайне. Именно этого авторы стараются избежать, предлагая обновлять и хранить агрегированные user- и item-векторы асинхронно.<br><br>Интересная находка: лучшие результаты даёт не использование последнего слоя модели, а извлечение представлений из предпоследнего, хотя замеры противоречивые — на графиках виден шум.<br><br><strong>Архитектура Triple Tower</strong><br><br>Для обучения используется так называемый triple tower design:<br>— user-башня,<br>— item-башня,<br>— mix-tower для их взаимодействия.<br><br>При этом архитектура разделена на две ветви (dual-branch design): одна обучается на органическом контенте (просмотры, лайки, комментарии), другая — на рекламных сэмплах (клики, конверсии). User- и item-вектора остаются общими, а cross-вектор извлекается только из рекламной ветви, так как он ближе к целевым downstream-задачам.<br><br>Авторы описывают три способа интеграции foundation-модели в downstream CTR-модель: добавление представлений в качестве новых фичей, подключение блока обработки внутри архитектуры, использование всей большой модели для генерации кандидатов.<br><br>Простое добавление эмбеддингов в downstream-модель работает плохо: пробовали и линейные проекции, и alignment-лоссы, но улучшений не добились. Вместо этого применяют другой приём: каждую входную фичу комбинируют с представлением из foundation-модели с помощью покомпонентного умножения и нелинейности. Таким образом, user-item-вектор встраивается в модель уже на уровне входных признаков.<br><br><strong>Эксперименты и результаты<br></strong><br>Валидацию делали только на внутренних данных Tencent: больших датасетах с рекламными и органическими действиями, онлайн-A/B-тестах. Авторы пишут что систему внедрили уже в десяти с лишним продуктах экосистемы и получили рост GMV на 2,45% по всей платформе.<br><br><em>Больше о внедрении фундаментальных моделей применительно к экосистеме Яндекса можно узнать в канале руководителя службы рекомендательных технологий Николая Савушкина — </em><em>@light_from_black_box</em><em>.</em><br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Николай Савушкин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/147_480.webp" srcset="../assets/media/thumbs/147_480.webp 480w, ../assets/media/147.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="147" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 906 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/147" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/147.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="146" data-search="recgpt technical report, 2/2 в первой части разбора рассказали об идее и результатах recgpt. теперь — детали реализации. как мы уже упомянули, система состоит из четырёх ключевых компонентов. user interest mining главная трудность оказалась в том, что у пользователей слишком длинные истории — в среднем больше 37 тысяч событий, что не помещается в контекст llm. авторы придумали механизм сжатия истории: они оставляют только самые информативные события — покупки, добавления в корзину, избранное, поисковые запросы, просмотр отзывов и подробных описаний. все эти данные дополнительно агрегируются по времени: ближайшие дни учитываются подробно, а более старые периоды объединяются сначала в месяцы, а затем и в годы. так история превращается в понятный текстовый нарратив, который можно подать на вход модели. параллельно alibaba разработали task alignment framework. они сформулировали 16 задач — от простых (например, определить категорию товара по запросу) до более сложных (выделение ключевых характеристик, определение релевантности). llm обучали постепенно, чтобы адаптировать её к специфике рекомендательного домена. вдобавок сделали self-training evolution: модели генерировали гипотезы, которые затем фильтровали, чтобы убрать галлюцинации или слишком общие интересы, и использовали отобранное для дообучения. в итоге система научилась извлекать из истории осмысленные интересы, а 98% пользователей теперь помещаются в лимит контекста и на каждого удаётся предсказать в среднем 16 интересов. tag prediction на основе предсказанных интересов следующая модель формирует так называемые теги — текстовые описания того, что пользователь, возможно, захочет купить. это не конкретные товары, а их обобщённые характеристики: например, «outdoor waterproof hiking boots». к тегам есть требования: они должны опираться на историю и интересы пользователя, быть конкретными, свежими и релевантными сезону. в среднем нужно получить не меньше пятидесяти тегов. для обучения используют два шага. сначала pre-alignment, когда из названий товаров в истории составляются кандидаты для тегов. затем self-training: система дообучается на собственных же генерациях, но перед этим данные чистят и перебалансируюют. это нужно, чтобы популярные категории не полностью доминировали и модель не теряла разнообразие. такой подход оказался эффективным: вырос hit rate — совпадения между предсказанными тегами и реальными товарами, которые позже были куплены или просмотрены. item retrieval следующий этап — сопоставление тегов с конкретными товарами. здесь alibaba разработали архитектуру с тремя башнями: пользовательской, товарной и теговой. она учитывает как семантическую близость, так и коллаборативные сигналы. для обучения используют выборку с положительными и отрицательными примерами: система учится различать товары из нужной категории и из посторонних. на этапе инференса представления из разных башен объединяются, что позволяет более точно матчить интересы и товары. personalized explanation наконец, один из самых заметных элементов — генерация объяснений. вместо того чтобы каждый раз формировать объяснение заново для пары «пользователь-товар», в alibaba сделали ставку на связку «интерес-товар». это экономит ресурсы и сохраняет персонализацию. датасет для обучения объяснений собирали через другую llm и фильтровали от галлюцинаций. дополнительный self-training помог адаптировать модель к новым ситуациям. в итоге рекомендации сопровождаются короткими и понятными комментариями вроде «мы показали вам этот товар, потому что вы недавно искали похожие вещи для путешествий». в итоге, recgpt — это не просто «llm в рексистеме», а целый пайплайн: от сжатия пользовательской истории и извлечения интересов до генерации тегов, матчинга и интерпретируемых объяснений. @recsyschannel разбор подготовил ❣ виктор януш recgpt technical report, 2/2 в первой части разбора рассказали об идее и результатах recgpt. теперь — детали реализации. как мы уже упомянули, система состоит из четырёх ключевых компонентов. user interest mining главная трудность оказалась в том, что у пользователей слишком длинные истории — в среднем больше 37 тысяч событий, что не помещается в контекст llm. авторы придумали механизм сжатия истории: они оставляют только самые информативные события — покупки, добавления в корзину, избранное, поисковые запросы, просмотр отзывов и подробных описаний. все эти данные дополнительно агрегируются по времени: ближайшие дни учитываются подробно, а более старые периоды объединяются сначала в месяцы, а затем и в годы. так история превращается в понятный текстовый нарратив, который можно подать на вход модели. параллельно alibaba разработали task alignment framework. они сформулировали 16 задач — от простых (например, определить категорию товара по запросу) до более сложных (выделение ключевых характеристик, определение релевантности). llm обучали постепенно, чтобы адаптировать её к специфике рекомендательного домена. вдобавок сделали self-training evolution: модели генерировали гипотезы, которые затем фильтровали, чтобы убрать галлюцинации или слишком общие интересы, и использовали отобранное для дообучения. в итоге система научилась извлекать из истории осмысленные интересы, а 98% пользователей теперь помещаются в лимит контекста и на каждого удаётся предсказать в среднем 16 интересов. tag prediction на основе предсказанных интересов следующая модель формирует так называемые теги — текстовые описания того, что пользователь, возможно, захочет купить. это не конкретные товары, а их обобщённые характеристики: например, «outdoor waterproof hiking boots». к тегам есть требования: они должны опираться на историю и интересы пользователя, быть конкретными, свежими и релевантными сезону. в среднем нужно получить не меньше пятидесяти тегов. для обучения используют два шага. сначала pre-alignment, когда из названий товаров в истории составляются кандидаты для тегов. затем self-training: система дообучается на собственных же генерациях, но перед этим данные чистят и перебалансируюют. это нужно, чтобы популярные категории не полностью доминировали и модель не теряла разнообразие. такой подход оказался эффективным: вырос hit rate — совпадения между предсказанными тегами и реальными товарами, которые позже были куплены или просмотрены. item retrieval следующий этап — сопоставление тегов с конкретными товарами. здесь alibaba разработали архитектуру с тремя башнями: пользовательской, товарной и теговой. она учитывает как семантическую близость, так и коллаборативные сигналы. для обучения используют выборку с положительными и отрицательными примерами: система учится различать товары из нужной категории и из посторонних. на этапе инференса представления из разных башен объединяются, что позволяет более точно матчить интересы и товары. personalized explanation наконец, один из самых заметных элементов — генерация объяснений. вместо того чтобы каждый раз формировать объяснение заново для пары «пользователь-товар», в alibaba сделали ставку на связку «интерес-товар». это экономит ресурсы и сохраняет персонализацию. датасет для обучения объяснений собирали через другую llm и фильтровали от галлюцинаций. дополнительный self-training помог адаптировать модель к новым ситуациям. в итоге рекомендации сопровождаются короткими и понятными комментариями вроде «мы показали вам этот товар, потому что вы недавно искали похожие вещи для путешествий». в итоге, recgpt — это не просто «llm в рексистеме», а целый пайплайн: от сжатия пользовательской истории и извлечения интересов до генерации тегов, матчинга и интерпретируемых объяснений. @recsyschannel разбор подготовил ❣ виктор януш">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-08T09:01:49+00:00" href="./posts/146.html">2025-09-08 09:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>RecGPT Technical Report, 2/2 <br></strong><br>В первой части разбора <a href="https://t.me/RecSysChannel/145" rel="nofollow noopener noreferrer">рассказали</a> об идее и результатах RecGPT. Теперь — детали реализации. Как мы уже упомянули, система состоит из четырёх ключевых компонентов.<br><br><strong>User Interest Mining</strong><br><br>Главная трудность оказалась в том, что у пользователей слишком длинные истории — в среднем больше 37 тысяч событий, что не помещается в контекст LLM. Авторы придумали механизм сжатия истории: они оставляют только самые информативные события — покупки, добавления в корзину, избранное, поисковые запросы, просмотр отзывов и подробных описаний. Все эти данные дополнительно агрегируются по времени: ближайшие дни учитываются подробно, а более старые периоды объединяются сначала в месяцы, а затем и в годы. Так история превращается в понятный текстовый нарратив, который можно подать на вход модели.<br><br>Параллельно Alibaba разработали task alignment framework. Они сформулировали 16 задач — от простых (например, определить категорию товара по запросу) до более сложных (выделение ключевых характеристик, определение релевантности). LLM обучали постепенно, чтобы адаптировать её к специфике рекомендательного домена. <br><br>Вдобавок сделали self-training evolution: модели генерировали гипотезы, которые затем фильтровали, чтобы убрать галлюцинации или слишком общие интересы, и использовали отобранное для дообучения. В итоге система научилась извлекать из истории осмысленные интересы, а 98% пользователей теперь помещаются в лимит контекста и на каждого удаётся предсказать в среднем 16 интересов.<br><br><strong>Tag Prediction</strong><br><br>На основе предсказанных интересов следующая модель формирует так называемые теги — текстовые описания того, что пользователь, возможно, захочет купить. Это не конкретные товары, а их обобщённые характеристики: например, «outdoor waterproof hiking boots». К тегам есть требования: они должны опираться на историю и интересы пользователя, быть конкретными, свежими и релевантными сезону. В среднем нужно получить не меньше пятидесяти тегов.<br><br>Для обучения используют два шага. Сначала pre-alignment, когда из названий товаров в истории составляются кандидаты для тегов. Затем self-training: система дообучается на собственных же генерациях, но перед этим данные чистят и перебалансируюют. Это нужно, чтобы популярные категории не полностью доминировали и модель не теряла разнообразие. Такой подход оказался эффективным: вырос hit rate — совпадения между предсказанными тегами и реальными товарами, которые позже были куплены или просмотрены.<br><br><strong>Item Retrieval<br></strong><br>Следующий этап — сопоставление тегов с конкретными товарами. Здесь Alibaba разработали архитектуру с тремя башнями: пользовательской, товарной и теговой. Она учитывает как семантическую близость, так и коллаборативные сигналы. Для обучения используют выборку с положительными и отрицательными примерами: система учится различать товары из нужной категории и из посторонних. На этапе инференса представления из разных башен объединяются, что позволяет более точно матчить интересы и товары.<br><br><strong>Personalized Explanation</strong><br><br>Наконец, один из самых заметных элементов — генерация объяснений. Вместо того чтобы каждый раз формировать объяснение заново для пары «пользователь-товар», в Alibaba сделали ставку на связку «интерес-товар». Это экономит ресурсы и сохраняет персонализацию. Датасет для обучения объяснений собирали через другую LLM и фильтровали от галлюцинаций. Дополнительный self-training помог адаптировать модель к новым ситуациям. В итоге рекомендации сопровождаются короткими и понятными комментариями вроде «Мы показали вам этот товар, потому что вы недавно искали похожие вещи для путешествий».<br><br>В итоге, RecGPT — это не просто «LLM в рексистеме», а целый пайплайн: от сжатия пользовательской истории и извлечения интересов до генерации тегов, матчинга и интерпретируемых объяснений.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Виктор Януш</div>
      <div class="actions">
        <span>2 011 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/146" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/146.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="145" data-search="recgpt technical report, 1/2 сегодня начинаем разбор недавнего техрепорта от alibaba о новом подходе к рекомендациям recgpt. в нём авторы предлагают по максимуму задействовать большие языковые модели. классические рекомендательные системы учатся в основном на логах кликов. такой подход приводит к ряду ограничений: формируются «пузыри», когда пользователю постоянно показывают одно и то же; сложно работать с длинным хвостом товаров; возникают разные bias&#x27;ы (например, популярности). но главное — при таком обучении теряется семантическая информация, а люди выбирают товары не только на основе кликов, а исходя из более сложных мотивов и контекстов. в качестве решения alibaba предлагают использовать llm с ризонингом, чтобы модель не просто фиксировала клики, а пыталась понять, почему пользователь может захотеть тот или иной товар. но и тут свои сложности: — llm нужно адаптировать к конкретному домену; — важно укладываться в ограничения по времени отклика и вычислительным ресурсам; — по-прежнему сложно интегрироваться в индустриальные системы. пайплайн recgpt состоит из четырёх частей: 1. user interest mining — извлечение интересов пользователя из истории; 2. tag prediction — генерация тегов (описаний желаемых товаров); 3. item retrieval — сопоставление тегов с реальными товарами; 4. personalized explanation — генерация объяснений, почему система рекомендует этот товар. каждый этап можно интерпретировать — это полезно и для пользователей (доверие к системе), и для разработчиков (удобнее отлаживать). recgpt внедрили в сценарий guess what you like (беззапросные рекомендации на taobao.com). в результате получили рост ctr, просмотров страниц и доли активных пользователей, а ещё увеличили разнообразие по категориям. улучшения заметили и мерчанты: товары стали лучше доходить до целевой аудитории. alibaba заявляют, что их решение — первый в мире успешный деплой reasoning-llm в рекомендательную систему. в следующей части — подробнее об архитектуре рексистемы. @recsyschannel разбор подготовил ❣ виктор януш recgpt technical report, 1/2 сегодня начинаем разбор недавнего техрепорта от alibaba о новом подходе к рекомендациям recgpt. в нём авторы предлагают по максимуму задействовать большие языковые модели. классические рекомендательные системы учатся в основном на логах кликов. такой подход приводит к ряду ограничений: формируются «пузыри», когда пользователю постоянно показывают одно и то же; сложно работать с длинным хвостом товаров; возникают разные bias&amp;#x27;ы (например, популярности). но главное — при таком обучении теряется семантическая информация, а люди выбирают товары не только на основе кликов, а исходя из более сложных мотивов и контекстов. в качестве решения alibaba предлагают использовать llm с ризонингом, чтобы модель не просто фиксировала клики, а пыталась понять, почему пользователь может захотеть тот или иной товар. но и тут свои сложности: — llm нужно адаптировать к конкретному домену; — важно укладываться в ограничения по времени отклика и вычислительным ресурсам; — по-прежнему сложно интегрироваться в индустриальные системы. пайплайн recgpt состоит из четырёх частей: 1. user interest mining — извлечение интересов пользователя из истории; 2. tag prediction — генерация тегов (описаний желаемых товаров); 3. item retrieval — сопоставление тегов с реальными товарами; 4. personalized explanation — генерация объяснений, почему система рекомендует этот товар. каждый этап можно интерпретировать — это полезно и для пользователей (доверие к системе), и для разработчиков (удобнее отлаживать). recgpt внедрили в сценарий guess what you like (беззапросные рекомендации на taobao.com ). в результате получили рост ctr, просмотров страниц и доли активных пользователей, а ещё увеличили разнообразие по категориям. улучшения заметили и мерчанты: товары стали лучше доходить до целевой аудитории. alibaba заявляют, что их решение — первый в мире успешный деплой reasoning-llm в рекомендательную систему. в следующей части — подробнее об архитектуре рексистемы. @recsyschannel разбор подготовил ❣ виктор януш">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-05T09:06:11+00:00" href="./posts/145.html">2025-09-05 09:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>RecGPT Technical Report, 1/2 </strong><br><strong><br></strong>Сегодня начинаем разбор недавнего <a href="https://arxiv.org/pdf/2507.22879" rel="nofollow noopener noreferrer">техрепорта</a> от Alibaba о новом подходе к рекомендациям RecGPT. В нём авторы предлагают по максимуму задействовать большие языковые модели.<br><br>Классические рекомендательные системы учатся в основном на логах кликов. Такой подход приводит к ряду ограничений: формируются «пузыри», когда пользователю постоянно показывают одно и то же; сложно работать с длинным хвостом товаров; возникают разные bias&#x27;ы (например, популярности). Но главное — при таком обучении теряется семантическая информация, а люди выбирают товары не только на основе кликов, а исходя из более сложных мотивов и контекстов.<br><br>В качестве решения Alibaba предлагают использовать LLM с ризонингом, чтобы модель не просто фиксировала клики, а пыталась понять, почему пользователь может захотеть тот или иной товар. <br><br>Но и тут свои сложности:<br><br>— LLM нужно адаптировать к конкретному домену;<br>— важно укладываться в ограничения по времени отклика и вычислительным ресурсам;<br>— по-прежнему сложно интегрироваться в индустриальные системы.<br><br><strong>Пайплайн RecGPT состоит из четырёх частей:</strong><br><br>1. User Interest Mining — извлечение интересов пользователя из истории;<br>2. Tag Prediction — генерация тегов (описаний желаемых товаров);<br>3. Item Retrieval — сопоставление тегов с реальными товарами;<br>4. Personalized Explanation — генерация объяснений, почему система рекомендует этот товар.<br><br>Каждый этап можно интерпретировать — это полезно и для пользователей (доверие к системе), и для разработчиков (удобнее отлаживать).<br><br>RecGPT внедрили в сценарий Guess What You Like (беззапросные рекомендации на <a rel="nofollow noopener noreferrer">taobao.com</a>). В результате получили рост CTR, просмотров страниц и доли активных пользователей, а ещё увеличили разнообразие по категориям. Улучшения заметили и мерчанты: товары стали лучше доходить до целевой аудитории.<br><br>Alibaba заявляют, что их решение — первый в мире успешный деплой reasoning-LLM в рекомендательную систему. <br><br>В следующей части — подробнее об архитектуре рексистемы.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Виктор Януш<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/145_480.webp" srcset="../assets/media/thumbs/145_480.webp 480w, ../assets/media/145.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="145" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 948 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/145" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/145.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="144" data-search="training compute-optimal large language models сегодня разберём статью 2022 года от deepmind, известную также по названию модели chinchilla. работа посвящена проблеме правильного распределения фиксированного компьюта между увеличением размера модели и числа токенов, на которых она учится, в домене языковых моделей. для связи этих трёх величин существует аппроксимация c = 6nd, где c — компьют, n — число параметров, d — число токенов в датасете. оптимальные n и d масштабируются как c^a и c^b соответственно, где a + b = 1. задача — найти a и b. работа мотивирована статьей 2020 года от openai — scaling laws for neural language models, в которой авторы заключили, что большая часть компьюта должна быть аллоцирована под масштабирование самой модели (a &gt; b). исследователи из deepmind приходят к другому выводу. они выводят законы масштабирования тремя разными способами, и все три приводят к схожим результатам (a ≈ b ≈ 0,5). подход первый: строят график в осях flops — лосс для нескольких моделей с числом параметров от 75m до 10b. каждому числу флопсов ставится в соответствие точка с минимальным лоссом, для которой известно, какому размеру модели и числу пройденных токенов она относится. полученные точки переносят на графики в осях flops — n и flops — d, регрессируют их прямой (в прологарифмированных осях), угол наклона которой задаёт a и b. в итоге: a = b = 0,5. подход второй: фиксируют компьют и варьируют число параметров, что автоматически задаёт число токенов для обучения. для каждого фиксированного компьюта находят такую точку, для которой уменьшение или увеличение числа параметров приводит к ухудшению финального лосса. снова регрессируют эти точки в осях flops — n и flops — d, получая a = 0,49 и b = 0,51. подход тертий: здесь авторы моделируют зависимость l(n, d) финального лосса от размера модели и числа пройденных токенов, используя при этом все результаты (l_final, n, d) из первых двух подходов. благодаря этому выражению, зная компьют, можно найти оптимальное число параметров, которое будет ординатой точки касания вертикальной прямой к линии уровня l(n, d) в осях flops — n (левый график). a и b оказываются равными 0,46 и 0,54 соответственно. главный вывод статьи, — число параметров в модели и число токенов в датасете должны масштабироваться равномерно (то есть как квадратный корень из компьюта). например, при увеличении компьюта в четыре раза обе величины должны вырасти в два раза. ещё один интересный вывод авторов — модель gopher (280b) обучили на недостаточно большом датасете. в качестве доказательства обучают в четыре раза меньшую модель chinchilla (70b) на в четыре раза большем числе токенов, и эта модель оказывается значительно лучше gopher. @recsyschannel разбор подготовил ❣ сергей макеев training compute-optimal large language models сегодня разберём статью 2022 года от deepmind, известную также по названию модели chinchilla. работа посвящена проблеме правильного распределения фиксированного компьюта между увеличением размера модели и числа токенов, на которых она учится, в домене языковых моделей. для связи этих трёх величин существует аппроксимация c = 6nd, где c — компьют, n — число параметров, d — число токенов в датасете. оптимальные n и d масштабируются как c^a и c^b соответственно, где a + b = 1. задача — найти a и b. работа мотивирована статьей 2020 года от openai — scaling laws for neural language models , в которой авторы заключили, что большая часть компьюта должна быть аллоцирована под масштабирование самой модели (a &amp;gt; b). исследователи из deepmind приходят к другому выводу. они выводят законы масштабирования тремя разными способами, и все три приводят к схожим результатам (a ≈ b ≈ 0,5). подход первый: строят график в осях flops — лосс для нескольких моделей с числом параметров от 75m до 10b. каждому числу флопсов ставится в соответствие точка с минимальным лоссом, для которой известно, какому размеру модели и числу пройденных токенов она относится. полученные точки переносят на графики в осях flops — n и flops — d, регрессируют их прямой (в прологарифмированных осях), угол наклона которой задаёт a и b. в итоге: a = b = 0,5. подход второй: фиксируют компьют и варьируют число параметров, что автоматически задаёт число токенов для обучения. для каждого фиксированного компьюта находят такую точку, для которой уменьшение или увеличение числа параметров приводит к ухудшению финального лосса. снова регрессируют эти точки в осях flops — n и flops — d, получая a = 0,49 и b = 0,51. подход тертий: здесь авторы моделируют зависимость l(n, d) финального лосса от размера модели и числа пройденных токенов, используя при этом все результаты (l_final, n, d) из первых двух подходов. благодаря этому выражению, зная компьют, можно найти оптимальное число параметров, которое будет ординатой точки касания вертикальной прямой к линии уровня l(n, d) в осях flops — n (левый график). a и b оказываются равными 0,46 и 0,54 соответственно. главный вывод статьи, — число параметров в модели и число токенов в датасете должны масштабироваться равномерно (то есть как квадратный корень из компьюта). например, при увеличении компьюта в четыре раза обе величины должны вырасти в два раза. ещё один интересный вывод авторов — модель gopher (280b) обучили на недостаточно большом датасете. в качестве доказательства обучают в четыре раза меньшую модель chinchilla (70b) на в четыре раза большем числе токенов, и эта модель оказывается значительно лучше gopher. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-28T08:05:16+00:00" href="./posts/144.html">2025-08-28 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Training Compute-Optimal Large Language Models<br></strong><br>Сегодня разберём <a href="https://arxiv.org/abs/2203.15556" rel="nofollow noopener noreferrer">статью</a> 2022 года от DeepMind, известную также по названию модели Chinchilla. Работа посвящена проблеме правильного распределения фиксированного компьюта между увеличением размера модели и числа токенов, на которых она учится, в домене языковых моделей. Для связи этих трёх величин существует аппроксимация C = 6ND, где C — компьют, N — число параметров, D — число токенов в датасете. Оптимальные N и D масштабируются как C^a и C^b соответственно, где a + b = 1. Задача — найти a и b.<br><br>Работа мотивирована статьей 2020 года от OpenAI — <a href="https://arxiv.org/abs/2001.08361" rel="nofollow noopener noreferrer">Scaling Laws for Neural Language Models</a>, в которой авторы заключили, что большая часть компьюта должна быть аллоцирована под масштабирование самой модели (a &gt; b). Исследователи из DeepMind приходят к другому выводу. Они выводят законы масштабирования тремя разными способами, и все три приводят к схожим результатам (a ≈ b ≈ 0,5).<br><br>Подход первый: строят график в осях FLOPs — лосс для нескольких моделей с числом параметров от 75M до 10B. Каждому числу флопсов ставится в соответствие точка с минимальным лоссом, для которой известно, какому размеру модели и числу пройденных токенов она относится. Полученные точки переносят на графики в осях FLOPs — N и FLOPs — D, регрессируют их прямой (в прологарифмированных осях), угол наклона которой задаёт a и b. В итоге: a = b = 0,5.<br><br>Подход второй: фиксируют компьют и варьируют число параметров, что автоматически задаёт число токенов для обучения. Для каждого фиксированного компьюта находят такую точку, для которой уменьшение или увеличение числа параметров приводит к ухудшению финального лосса. Снова регрессируют эти точки в осях FLOPs — N и FLOPs — D, получая a = 0,49 и b = 0,51.<br><br>Подход тертий: здесь авторы моделируют зависимость L(N, D) финального лосса от размера модели и числа пройденных токенов, используя при этом все результаты (L_final, N, D) из первых двух подходов. Благодаря этому выражению, зная компьют, можно найти оптимальное число параметров, которое будет ординатой точки касания вертикальной прямой к линии уровня L(N, D) в осях FLOPs — N (левый график). a и b оказываются равными 0,46 и 0,54 соответственно.<br><br>Главный вывод статьи, — число параметров в модели и число токенов в датасете должны масштабироваться равномерно (то есть как квадратный корень из компьюта). Например, при увеличении компьюта в четыре раза обе величины должны вырасти в два раза. <br><br>Ещё один интересный вывод авторов — модель Gopher (280B) обучили на недостаточно большом датасете. В качестве доказательства обучают в четыре раза меньшую модель Chinchilla (70B) на в четыре раза большем числе токенов, и эта модель оказывается значительно лучше Gopher.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/144_480.webp" srcset="../assets/media/thumbs/144_480.webp 480w, ../assets/media/144.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="144" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 775 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/144" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/144.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="143" data-search="pinfm: foundation model for user activity sequences at a billion-scale visual discovery platform [2/2] продолжаем разбирать статью от pinterest. авторы не делятся внутренними параметрами модели, не уточняют, какого размера декодер и как всё обучалось. однако они приводят масштабы всей системы — 20 миллиардов параметров. судя по всему, большая часть этих параметров — матрица эмбеддингов. то есть модель в итоге получилась небольшой. отмечают, что в качестве энкодера выбрали архитектуру gpt2 и не увидели улучшений от применения hstu-энкодера. обучающую последовательность сформировали из 16 тысяч пользовательских взаимодействий, нарезав их на подпоследовательности длиной несколько сотен событий. каждое событие кодируют обучаемыми эмбеддингами пина, поверхности и типа взаимодействия, итоговый токен события — сумма этих трёх эмбеддингов. напоминает то, как формируются токены в argus: де-факто есть те же context, item и action, но в весьма ограниченном варианте. в остальном архитектура вышла стандартной. но вот решаемую задачу авторы определяют весьма интересно. в качестве таргетов берут только позитивные события (при этом последовательность формируется с включением негативов), делают это с помощью sampled softmax (почему-то без logq-коррекции). в этом сетапе на стадии претрейна предсказывают: – следующий позитивный токен; – следующие позитивные токены в некотором временном окне; – позитивные события, но во временном окне downstream-ранжирующей модели. получившийся лосс суммируют. на файнтюне используют ещё несколько интересных трюков: выравнивают предсказания файнтюна и ранжирующей модели, добавляют дополнительный сигнал (контентно-коллаборативные графовые эмбеддинги) и обучаемые токены перед кандидатами, а также техники для решения проблемы холодного старта. команда pinterest в очередной раз демонстрирует крутые инфраструктурные решения для жизнеспособность всей системы. в частности, эффективная дедупликация последовательности увеличила на 600% пропускную способность модели по сравнению с flashattention-2. для оптимизации гигантской таблицы эмбеддингов применили агрессивную int4-квантизацию практически без потери качества. в результате получилась сильная модель, хорошо агрегирующая знание о пользователях. это отражается в результатах a/b-тестирования: на рекомендательной ленте на главной удалось добиться роста числа сохранений пинов на 2,6%, а для свежих пинов — на 5,7%. @recsyschannel разбор подготовил ❣ руслан кулиев pinfm: foundation model for user activity sequences at a billion-scale visual discovery platform [2/2] продолжаем разбирать статью от pinterest. авторы не делятся внутренними параметрами модели, не уточняют, какого размера декодер и как всё обучалось. однако они приводят масштабы всей системы — 20 миллиардов параметров. судя по всему, большая часть этих параметров — матрица эмбеддингов. то есть модель в итоге получилась небольшой. отмечают, что в качестве энкодера выбрали архитектуру gpt2 и не увидели улучшений от применения hstu-энкодера. обучающую последовательность сформировали из 16 тысяч пользовательских взаимодействий, нарезав их на подпоследовательности длиной несколько сотен событий. каждое событие кодируют обучаемыми эмбеддингами пина, поверхности и типа взаимодействия, итоговый токен события — сумма этих трёх эмбеддингов. напоминает то, как формируются токены в argus : де-факто есть те же context, item и action, но в весьма ограниченном варианте. в остальном архитектура вышла стандартной. но вот решаемую задачу авторы определяют весьма интересно. в качестве таргетов берут только позитивные события (при этом последовательность формируется с включением негативов), делают это с помощью sampled softmax (почему-то без logq-коррекции ). в этом сетапе на стадии претрейна предсказывают: – следующий позитивный токен; – следующие позитивные токены в некотором временном окне; – позитивные события, но во временном окне downstream-ранжирующей модели. получившийся лосс суммируют. на файнтюне используют ещё несколько интересных трюков: выравнивают предсказания файнтюна и ранжирующей модели, добавляют дополнительный сигнал (контентно-коллаборативные графовые эмбеддинги) и обучаемые токены перед кандидатами, а также техники для решения проблемы холодного старта. команда pinterest в очередной раз демонстрирует крутые инфраструктурные решения для жизнеспособность всей системы. в частности, эффективная дедупликация последовательности увеличила на 600% пропускную способность модели по сравнению с flashattention-2. для оптимизации гигантской таблицы эмбеддингов применили агрессивную int4-квантизацию практически без потери качества. в результате получилась сильная модель, хорошо агрегирующая знание о пользователях. это отражается в результатах a/b-тестирования: на рекомендательной ленте на главной удалось добиться роста числа сохранений пинов на 2,6%, а для свежих пинов — на 5,7%. @recsyschannel разбор подготовил ❣ руслан кулиев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-25T08:01:58+00:00" href="./posts/143.html">2025-08-25 08:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform [2/2]</strong><br><br>Продолжаем <a href="https://t.me/RecSysChannel/142" rel="nofollow noopener noreferrer">разбирать статью</a> от Pinterest. Авторы не делятся внутренними параметрами модели, не уточняют, какого размера декодер и как всё обучалось. Однако они приводят масштабы всей системы — 20 миллиардов параметров. Судя по всему, большая часть этих параметров — матрица эмбеддингов. То есть модель в итоге получилась небольшой.<br><br>Отмечают, что в качестве энкодера выбрали архитектуру GPT2 и не увидели улучшений от применения HSTU-энкодера. Обучающую последовательность сформировали из 16 тысяч пользовательских взаимодействий, нарезав их на подпоследовательности длиной несколько сотен событий. Каждое событие кодируют обучаемыми эмбеддингами пина, поверхности и типа взаимодействия, итоговый токен события — сумма этих трёх эмбеддингов. Напоминает то, как формируются токены в <a href="https://t.me/RecSysChannel/133" rel="nofollow noopener noreferrer">Argus</a>: де-факто есть те же context, item и action, но в весьма ограниченном варианте.<br><br>В остальном архитектура вышла стандартной. Но вот решаемую задачу авторы определяют весьма интересно. В качестве таргетов берут только позитивные события (при этом последовательность формируется с включением негативов), делают это с помощью Sampled Softmax (почему-то без <a href="https://arxiv.org/abs/2507.09331" rel="nofollow noopener noreferrer">LogQ-коррекции</a>). В этом сетапе на стадии претрейна предсказывают: <br><br>– следующий позитивный токен;<br>– следующие позитивные токены в некотором временном окне;<br>– позитивные события, но во временном окне downstream-ранжирующей модели.<br><br>Получившийся лосс суммируют.<br><br>На файнтюне используют ещё несколько интересных трюков: выравнивают предсказания файнтюна и ранжирующей модели, добавляют дополнительный сигнал (контентно-коллаборативные графовые эмбеддинги) и обучаемые токены перед кандидатами, а также техники для решения проблемы холодного старта.<br><br>Команда Pinterest в очередной раз демонстрирует крутые инфраструктурные решения для жизнеспособность всей системы. В частности, эффективная дедупликация последовательности увеличила на 600% пропускную способность модели по сравнению с FlashAttention-2. Для оптимизации гигантской таблицы эмбеддингов применили агрессивную int4-квантизацию практически без потери качества.<br><br>В результате получилась сильная модель, хорошо агрегирующая знание о пользователях. Это отражается в результатах A/B-тестирования: на рекомендательной ленте на главной удалось добиться роста числа сохранений пинов на 2,6%, а для свежих пинов — на 5,7%.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Руслан Кулиев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/143_480.webp" srcset="../assets/media/thumbs/143_480.webp 480w, ../assets/media/143.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="143" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 735 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/143" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/143.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link disabled" href="#">←</a>
        <a class="page-link current" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-2.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 218, "media": [{"kind": "photo", "path": "../assets/media/218.jpg", "thumb": "../assets/media/thumbs/218_480.webp", "size": 151961, "mime": "image/jpeg", "name": null}]}, {"id": 217, "media": [{"kind": "photo", "path": "../assets/media/217.jpg", "thumb": "../assets/media/thumbs/217_480.webp", "size": 71435, "mime": "image/jpeg", "name": null}]}, {"id": 210, "media": [{"kind": "photo", "path": "../assets/media/210.jpg", "thumb": "../assets/media/thumbs/210_480.webp", "size": 88602, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/211.jpg", "thumb": "../assets/media/thumbs/211_480.webp", "size": 106818, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/212.jpg", "thumb": "../assets/media/thumbs/212_480.webp", "size": 103137, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/213.jpg", "thumb": "../assets/media/thumbs/213_480.webp", "size": 83974, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/214.jpg", "thumb": "../assets/media/thumbs/214_480.webp", "size": 117187, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/215.jpg", "thumb": "../assets/media/thumbs/215_480.webp", "size": 98307, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/216.jpg", "thumb": "../assets/media/thumbs/216_480.webp", "size": 115031, "mime": "image/jpeg", "name": null}]}, {"id": 209, "media": []}, {"id": 208, "media": []}, {"id": 207, "media": []}, {"id": 206, "media": [{"kind": "photo", "path": "../assets/media/206.jpg", "thumb": "../assets/media/thumbs/206_480.webp", "size": 60939, "mime": "image/jpeg", "name": null}]}, {"id": 205, "media": [{"kind": "photo", "path": "../assets/media/205.jpg", "thumb": "../assets/media/thumbs/205_480.webp", "size": 123714, "mime": "image/jpeg", "name": null}]}, {"id": 204, "media": [{"kind": "photo", "path": "../assets/media/204.jpg", "thumb": "../assets/media/thumbs/204_480.webp", "size": 150325, "mime": "image/jpeg", "name": null}]}, {"id": 203, "media": [{"kind": "photo", "path": "../assets/media/203.jpg", "thumb": "../assets/media/thumbs/203_480.webp", "size": 87060, "mime": "image/jpeg", "name": null}]}, {"id": 202, "media": [{"kind": "photo", "path": "../assets/media/202.jpg", "thumb": "../assets/media/thumbs/202_480.webp", "size": 75141, "mime": "image/jpeg", "name": null}]}, {"id": 201, "media": [{"kind": "photo", "path": "../assets/media/201.jpg", "thumb": "../assets/media/thumbs/201_480.webp", "size": 35737, "mime": "image/jpeg", "name": null}]}, {"id": 198, "media": [{"kind": "photo", "path": "../assets/media/198.jpg", "thumb": "../assets/media/thumbs/198_480.webp", "size": 143770, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/199.jpg", "thumb": "../assets/media/thumbs/199_480.webp", "size": 166995, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/200.jpg", "thumb": "../assets/media/thumbs/200_480.webp", "size": 126341, "mime": "image/jpeg", "name": null}]}, {"id": 194, "media": [{"kind": "photo", "path": "../assets/media/194.jpg", "thumb": "../assets/media/thumbs/194_480.webp", "size": 153287, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/195.jpg", "thumb": "../assets/media/thumbs/195_480.webp", "size": 165074, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/196.jpg", "thumb": "../assets/media/thumbs/196_480.webp", "size": 162337, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/197.jpg", "thumb": "../assets/media/thumbs/197_480.webp", "size": 147993, "mime": "image/jpeg", "name": null}]}, {"id": 193, "media": [{"kind": "photo", "path": "../assets/media/193.jpg", "thumb": "../assets/media/thumbs/193_480.webp", "size": 62227, "mime": "image/jpeg", "name": null}]}, {"id": 192, "media": [{"kind": "photo", "path": "../assets/media/192.jpg", "thumb": "../assets/media/thumbs/192_480.webp", "size": 70312, "mime": "image/jpeg", "name": null}]}, {"id": 191, "media": []}, {"id": 190, "media": [{"kind": "photo", "path": "../assets/media/190.jpg", "thumb": "../assets/media/thumbs/190_480.webp", "size": 98131, "mime": "image/jpeg", "name": null}]}, {"id": 181, "media": [{"kind": "photo", "path": "../assets/media/181.jpg", "thumb": "../assets/media/thumbs/181_480.webp", "size": 106668, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/182.jpg", "thumb": "../assets/media/thumbs/182_480.webp", "size": 161019, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/183.jpg", "thumb": "../assets/media/thumbs/183_480.webp", "size": 156988, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/184.jpg", "thumb": "../assets/media/thumbs/184_480.webp", "size": 117866, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/185.jpg", "thumb": "../assets/media/thumbs/185_480.webp", "size": 129112, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/186.jpg", "thumb": "../assets/media/thumbs/186_480.webp", "size": 117666, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/187.jpg", "thumb": "../assets/media/thumbs/187_480.webp", "size": 166228, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/188.jpg", "thumb": "../assets/media/thumbs/188_480.webp", "size": 144887, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/189.jpg", "thumb": "../assets/media/thumbs/189_480.webp", "size": 89498, "mime": "image/jpeg", "name": null}]}, {"id": 180, "media": [{"kind": "photo", "path": "../assets/media/180.jpg", "thumb": "../assets/media/thumbs/180_480.webp", "size": 66656, "mime": "image/jpeg", "name": null}]}, {"id": 175, "media": [{"kind": "photo", "path": "../assets/media/175.jpg", "thumb": "../assets/media/thumbs/175_480.webp", "size": 123765, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/176.jpg", "thumb": "../assets/media/thumbs/176_480.webp", "size": 143444, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/177.jpg", "thumb": "../assets/media/thumbs/177_480.webp", "size": 112458, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/178.jpg", "thumb": "../assets/media/thumbs/178_480.webp", "size": 163739, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/179.jpg", "thumb": "../assets/media/thumbs/179_480.webp", "size": 117840, "mime": "image/jpeg", "name": null}]}, {"id": 171, "media": [{"kind": "photo", "path": "../assets/media/171.jpg", "thumb": "../assets/media/thumbs/171_480.webp", "size": 175518, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/172.jpg", "thumb": "../assets/media/thumbs/172_480.webp", "size": 167857, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/173.jpg", "thumb": "../assets/media/thumbs/173_480.webp", "size": 196669, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/174.jpg", "thumb": "../assets/media/thumbs/174_480.webp", "size": 171626, "mime": "image/jpeg", "name": null}]}, {"id": 162, "media": [{"kind": "photo", "path": "../assets/media/162.jpg", "thumb": "../assets/media/thumbs/162_480.webp", "size": 72720, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/163.jpg", "thumb": "../assets/media/thumbs/163_480.webp", "size": 95732, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/164.jpg", "thumb": "../assets/media/thumbs/164_480.webp", "size": 76044, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/165.jpg", "thumb": "../assets/media/thumbs/165_480.webp", "size": 93372, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/166.jpg", "thumb": "../assets/media/thumbs/166_480.webp", "size": 79015, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/167.jpg", "thumb": "../assets/media/thumbs/167_480.webp", "size": 224265, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/168.jpg", "thumb": "../assets/media/thumbs/168_480.webp", "size": 120344, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/169.jpg", "thumb": "../assets/media/thumbs/169_480.webp", "size": 166727, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/170.jpg", "thumb": "../assets/media/thumbs/170_480.webp", "size": 179306, "mime": "image/jpeg", "name": null}]}, {"id": 160, "media": [{"kind": "photo", "path": "../assets/media/160.jpg", "thumb": "../assets/media/thumbs/160_480.webp", "size": 124714, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/161.jpg", "thumb": "../assets/media/thumbs/161_480.webp", "size": 150698, "mime": "image/jpeg", "name": null}]}, {"id": 148, "media": [{"kind": "photo", "path": "../assets/media/148.jpg", "thumb": "../assets/media/thumbs/148_480.webp", "size": 106862, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/149.jpg", "thumb": "../assets/media/thumbs/149_480.webp", "size": 99901, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/150.jpg", "thumb": "../assets/media/thumbs/150_480.webp", "size": 112913, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/151.jpg", "thumb": "../assets/media/thumbs/151_480.webp", "size": 118837, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/152.jpg", "thumb": "../assets/media/thumbs/152_480.webp", "size": 77191, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/153.jpg", "thumb": "../assets/media/thumbs/153_480.webp", "size": 107863, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/154.jpg", "thumb": "../assets/media/thumbs/154_480.webp", "size": 86941, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/155.jpg", "thumb": "../assets/media/thumbs/155_480.webp", "size": 85112, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/156.jpg", "thumb": "../assets/media/thumbs/156_480.webp", "size": 88440, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/157.jpg", "thumb": "../assets/media/thumbs/157_480.webp", "size": 114074, "mime": "image/jpeg", "name": null}]}, {"id": 147, "media": [{"kind": "photo", "path": "../assets/media/147.jpg", "thumb": "../assets/media/thumbs/147_480.webp", "size": 147042, "mime": "image/jpeg", "name": null}]}, {"id": 146, "media": []}, {"id": 145, "media": [{"kind": "photo", "path": "../assets/media/145.jpg", "thumb": "../assets/media/thumbs/145_480.webp", "size": 108244, "mime": "image/jpeg", "name": null}]}, {"id": 144, "media": [{"kind": "photo", "path": "../assets/media/144.jpg", "thumb": "../assets/media/thumbs/144_480.webp", "size": 108164, "mime": "image/jpeg", "name": null}]}, {"id": 143, "media": [{"kind": "photo", "path": "../assets/media/143.jpg", "thumb": "../assets/media/thumbs/143_480.webp", "size": 54197, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "Рекомендательная [RecSys Channel]", "username": "RecSysChannel", "channel": "RecSysChannel", "last_sync_utc": "2026-02-07T07:21:08Z", "posts_count": 105, "last_seen_message_id": 218, "stats": {"new": 104, "updated": 2, "media_downloaded": 104}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
