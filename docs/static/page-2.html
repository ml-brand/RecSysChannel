<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Рекомендательная [RecSys Channel] — статическая версия (стр. 2/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-03T16%3A34%3A48Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-03T16%3A34%3A48Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-03T16%3A34%3A48Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Рекомендательная [RecSys Channel]</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+3NrSk0BmQ-QzZTMy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="141" data-search="top-k off-policy correction for a reinforce recommender system reinforcement learning — подход, который логично применять для рекомендаций. при этом работ об использовании rl-алгоритмов в этой области не так много. сегодня разберём статью 2019 года с конференции wsdm’19, которая посвящена этой теме. в работе описано одно из первых успешных применений rl в рекомендательных системах, внедренное в youtube на миллионы пользователей и многомиллионные каталоги видео. как recsys сформулировать в терминах rl взаимодействие пользователя можно смоделировать как марковский процесс принятия решений: — состояние — контекст взаимодействия и история пользователя; — действие — рекомендуемый кандидат (видео и т. п.); — награда — полезность показа (клик, лайк, время просмотра). политика π(a|s) выбирает кандидатов так, чтобы максимизировать долгосрочную полезность. дизайн награды в работе авторы рассматривают горизонт оптимизации внутри одной пользовательской сессии: цель — суммарная полезность за сессию, а не мгновенная. на практике удобно использовать гибридную награду (сочетание клика и времени просмотра), например: r = α·1_click + β·log(1 + watch_sec) reinforce политику π(a|s) моделируют в виде параметрической функции от состояния (истории пользователя), которая выдаёт распределение на действиях. в качестве модели берут рекуррентную нейронную сеть. политику обучают с помощью алгоритма reinforce. это on-policy-алгоритм, поэтому обновление весов корректно только на данных, собранных текущей политикой. поскольку это требует сложной инфраструктуры, обучение проводят на залогированных данных. off-policy correction залогированные данные получены от предыдущей версии рекомендательной системы β(a|s), которую авторы называют поведенческой политикой. это приводит к смещению в оценке градиента. чтобы компенсировать смещение, используют importance sampling. для моделирования β(a|s) применяют ту же архитектуру, что и для π(a|s), но обучают только на логах и не пропускают градиенты этой «головы» в общий backbone модели. для обеих политик при обучении используется sampled softmax. top-k correction на youtube показывают сразу k элементов на одной странице, то есть политика подбирает не одного кандидата, а набор. делается предположение, что каждый из k элементов сэмплируется независимо из π(a|s), поэтому от вероятности π(a|s) переходят к вероятности попадания на страницу: α(a|s) = 1 − (1 − π(a|s))^k online a/b-тест полученную политику π(a|s) использовали как один из кандидатогенераторов основного алгоритма рекомендаций youtube. применение off-policy correction увеличило число просмотренных видео примерно на +0,5%. добавление top-k correction увеличило общее время просмотра видео на +0,8–0,9%. @recsyschannel разбор подготовил ❣ артём матвеев top-k off-policy correction for a reinforce recommender system reinforcement learning — подход, который логично применять для рекомендаций. при этом работ об использовании rl-алгоритмов в этой области не так много. сегодня разберём статью 2019 года с конференции wsdm’19, которая посвящена этой теме. в работе описано одно из первых успешных применений rl в рекомендательных системах, внедренное в youtube на миллионы пользователей и многомиллионные каталоги видео. как recsys сформулировать в терминах rl взаимодействие пользователя можно смоделировать как марковский процесс принятия решений: — состояние — контекст взаимодействия и история пользователя; — действие — рекомендуемый кандидат (видео и т. п.); — награда — полезность показа (клик, лайк, время просмотра). политика π(a|s) выбирает кандидатов так, чтобы максимизировать долгосрочную полезность. дизайн награды в работе авторы рассматривают горизонт оптимизации внутри одной пользовательской сессии: цель — суммарная полезность за сессию, а не мгновенная. на практике удобно использовать гибридную награду (сочетание клика и времени просмотра), например: r = α·1_click + β·log(1 + watch_sec) reinforce политику π(a|s) моделируют в виде параметрической функции от состояния (истории пользователя), которая выдаёт распределение на действиях. в качестве модели берут рекуррентную нейронную сеть. политику обучают с помощью алгоритма reinforce. это on-policy-алгоритм, поэтому обновление весов корректно только на данных, собранных текущей политикой. поскольку это требует сложной инфраструктуры, обучение проводят на залогированных данных. off-policy correction залогированные данные получены от предыдущей версии рекомендательной системы β(a|s), которую авторы называют поведенческой политикой. это приводит к смещению в оценке градиента. чтобы компенсировать смещение, используют importance sampling. для моделирования β(a|s) применяют ту же архитектуру, что и для π(a|s), но обучают только на логах и не пропускают градиенты этой «головы» в общий backbone модели. для обеих политик при обучении используется sampled softmax . top-k correction на youtube показывают сразу k элементов на одной странице, то есть политика подбирает не одного кандидата, а набор. делается предположение, что каждый из k элементов сэмплируется независимо из π(a|s), поэтому от вероятности π(a|s) переходят к вероятности попадания на страницу: α(a|s) = 1 − (1 − π(a|s))^k online a/b-тест полученную политику π(a|s) использовали как один из кандидатогенераторов основного алгоритма рекомендаций youtube. применение off-policy correction увеличило число просмотренных видео примерно на +0,5%. добавление top-k correction увеличило общее время просмотра видео на +0,8–0,9%. @recsyschannel разбор подготовил ❣ артём матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-18T07:02:44+00:00" href="./posts/141.html">2025-08-18 07:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Top-K Off-Policy Correction for a REINFORCE Recommender System</strong><br><br>Reinforcement Learning — подход, который логично применять для рекомендаций. При этом работ об использовании RL-алгоритмов в этой области не так много. Сегодня разберём <a href="https://arxiv.org/pdf/1812.02353" rel="nofollow noopener noreferrer">статью</a> 2019 года с конференции WSDM’19, которая посвящена этой теме. В работе описано одно из первых успешных применений RL в рекомендательных системах, внедренное в YouTube на миллионы пользователей и многомиллионные каталоги видео.<br><br><strong>Как RecSys сформулировать в терминах RL<br></strong><br>Взаимодействие пользователя можно смоделировать как марковский процесс принятия решений:<br>— состояние — контекст взаимодействия и история пользователя;<br>— действие — рекомендуемый кандидат (видео и т. п.);<br>— награда — полезность показа (клик, лайк, время просмотра).<br>Политика π(a|s) выбирает кандидатов так, чтобы максимизировать долгосрочную полезность.<br><br><strong>Дизайн награды<br></strong><br>В работе авторы рассматривают горизонт оптимизации внутри одной пользовательской сессии: цель — суммарная полезность за сессию, а не мгновенная. На практике удобно использовать гибридную награду (сочетание клика и времени просмотра), например:<br><br>r = α·1_click + β·log(1 + watch_sec)<br><br><strong>REINFORCE</strong><br><br>Политику π(a|s) моделируют в виде параметрической функции от состояния (истории пользователя), которая выдаёт распределение на действиях. В качестве модели берут рекуррентную нейронную сеть. Политику обучают с помощью алгоритма REINFORCE. Это on-policy-алгоритм, поэтому обновление весов корректно только на данных, собранных текущей политикой. Поскольку это требует сложной инфраструктуры, обучение проводят на залогированных данных. <br><br><strong>Off-policy correction<br></strong><br>Залогированные данные получены от предыдущей версии рекомендательной системы β(a|s), которую авторы называют поведенческой политикой. Это приводит к смещению в оценке градиента. Чтобы компенсировать смещение, используют Importance Sampling. Для моделирования β(a|s) применяют ту же архитектуру, что и для π(a|s), но обучают только на логах и не пропускают градиенты этой «головы» в общий backbone модели. Для обеих политик при обучении используется <a href="https://arxiv.org/abs/2507.09331" rel="nofollow noopener noreferrer">Sampled Softmax</a>.<br><br><strong>Top-K correction<br></strong><br>На YouTube показывают сразу K элементов на одной странице, то есть политика подбирает не одного кандидата, а набор. Делается предположение, что каждый из K элементов сэмплируется независимо из π(a|s), поэтому от вероятности π(a|s) переходят к вероятности попадания на страницу:<br><br>α(a|s) = 1 − (1 − π(a|s))^K<br><br><strong>Online A/B-тест<br></strong><br>Полученную политику π(a|s) использовали как один из кандидатогенераторов основного алгоритма рекомендаций YouTube. Применение off-policy correction увеличило число просмотренных видео примерно на +0,5%. Добавление Top-K correction увеличило общее время просмотра видео на +0,8–0,9%.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Артём Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/141_480.webp" srcset="../assets/media/thumbs/141_480.webp 480w, ../assets/media/141.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="141" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 806 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/141" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/141.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="138" data-search="что интересного показали на конференции kdd 2025 в торонто прошла конференция kdd 2025, посвященная поиску знаний и анализу данных. на мероприятии, как водится, представили немало интересных публикаций. а мы, как водится, выбрали самые любопытные из них. tat: temporal-aligned transformer for multi-horizon peak demand forecasting статья amazon о прогнозировании временных рядов (спроса). авторы предлагают решение на основе трансформера, в котором используется, в том числе, информация о праздниках и днях со всплесками спроса. сообщают о двузначных числах прироста точности в предсказании пиков. automated query-product relevance labeling using large language models for e-commerce search статья walmart о том, как инженеры сделали фреймворк для авторазметки соответствия товара запросу. утверждают, что работает лучше ручной разметки (асессорам пора искать работу). dv365: extremely long user history modeling at instagram* крутая статья meta* — возможно, самая революционная в прикладном плане. инженеры компании сделали офлайн-профиль пользователя размером в среднем 40к, так как масштабировать hstu дальше сложно и дорого. жертвуют свежестью данных и делают ставку на стабильные интересы пользователей. получили +0,7% таймспента от внедрения эмбедда в использующих его моделях. mini-game lifetime value prediction in wechat статья wechat о предсказании ltv в играх. в основе graph representation learning, а также используют интересный подход к zero-inflated lognormal distribution modeling. компания meta, владеющая instagram, признана экстремистской; её деятельность в россии запрещена. интересное увидел ❣ сергей мить @recsyschannel что интересного показали на конференции kdd 2025 в торонто прошла конференция kdd 2025, посвященная поиску знаний и анализу данных. на мероприятии, как водится, представили немало интересных публикаций. а мы, как водится, выбрали самые любопытные из них. tat: temporal-aligned transformer for multi-horizon peak demand forecasting статья amazon о прогнозировании временных рядов (спроса). авторы предлагают решение на основе трансформера, в котором используется, в том числе, информация о праздниках и днях со всплесками спроса. сообщают о двузначных числах прироста точности в предсказании пиков. automated query-product relevance labeling using large language models for e-commerce search статья walmart о том, как инженеры сделали фреймворк для авторазметки соответствия товара запросу. утверждают, что работает лучше ручной разметки (асессорам пора искать работу). dv365: extremely long user history modeling at instagram * крутая статья meta* — возможно, самая революционная в прикладном плане. инженеры компании сделали офлайн-профиль пользователя размером в среднем 40к, так как масштабировать hstu дальше сложно и дорого. жертвуют свежестью данных и делают ставку на стабильные интересы пользователей. получили +0,7% таймспента от внедрения эмбедда в использующих его моделях. mini-game lifetime value prediction in wechat статья wechat о предсказании ltv в играх. в основе graph representation learning, а также используют интересный подход к zero-inflated lognormal distribution modeling. компания meta, владеющая instagram, признана экстремистской; её деятельность в россии запрещена. интересное увидел ❣ сергей мить @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-14T11:56:40+00:00" href="./posts/138.html">2025-08-14 11:56 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что интересного показали на конференции KDD 2025</strong><br><br>В Торонто прошла конференция KDD 2025, посвященная поиску знаний и анализу данных. На мероприятии, как водится, представили немало интересных публикаций. А мы, как водится, выбрали самые любопытные из них. <br><br><a href="https://arxiv.org/abs/2507.10349" rel="nofollow noopener noreferrer"><strong> TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting</strong></a><br><br>Статья Amazon о прогнозировании временных рядов (спроса). Авторы предлагают решение на основе трансформера, в котором используется, в том числе, информация о праздниках и днях со всплесками спроса. Сообщают о двузначных числах прироста точности в предсказании пиков.<br><br><a href="https://arxiv.org/abs/2502.15990v1" rel="nofollow noopener noreferrer"><strong>Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search</strong></a><br><br>Статья Walmart о том, как инженеры сделали фреймворк для авторазметки соответствия товара запросу. Утверждают, что работает лучше ручной разметки (асессорам пора искать работу).<br><br><a href="https://arxiv.org/abs/2506.00450" rel="nofollow noopener noreferrer"><strong>DV365: Extremely Long User History Modeling at Instagram</strong></a>* <br><br>Крутая статья Meta* — возможно, самая революционная в прикладном плане. Инженеры компании сделали офлайн-профиль пользователя размером в среднем 40к, так как масштабировать HSTU дальше сложно и дорого. Жертвуют свежестью данных и делают ставку на стабильные интересы пользователей. Получили +0,7% таймспента от внедрения эмбедда в использующих его моделях.<br><br><a href="https://arxiv.org/abs/2506.11037" rel="nofollow noopener noreferrer"><strong>Mini-Game Lifetime Value Prediction in WeChat</strong></a><br><br>Статья WeChat о предсказании LTV в играх. В основе graph representation learning, а также используют интересный подход к zero-inflated lognormal distribution modeling.<br><br><em>Компания Meta, владеющая Instagram, признана экстремистской; её деятельность в России запрещена. </em><br><br><em>Интересное увидел </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Мить</em><br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/138_480.webp" srcset="../assets/media/thumbs/138_480.webp 480w, ../assets/media/138.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/139_480.webp" srcset="../assets/media/thumbs/139_480.webp 480w, ../assets/media/139.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/140_480.webp" srcset="../assets/media/thumbs/140_480.webp 480w, ../assets/media/140.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="2" /></div></div>
      <div class="actions">
        <span>2 071 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/138" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/138.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="137" data-search="blending sequential embeddings, graphs, and engineered features: 4th place solution in recsys challenge 2025 сегодня рассказываем о статье, в которой описано решение от команды исследователей из яндекса, получившее в этом году четвёртое место на конкурсе recsys challenge. статью также приняли на конференцию recsys 2025. челлендж был посвящён области e-commerce. в этом направлении рекомендательные модели обучают предсказывать разные виды сигналов: конверсии, релевантные товары и их категории, сумму, которую потратит клиент, и многое другое. целью челленджа было обучить эмбеддинг пользователя, который объединил бы разнородные сигналы. затем организаторы использовали этот эмбеддинг, чтобы обучить независимые модели под шесть разных задач, вроде тех, что описаны выше. как видно на картинке, для построения такого эмбеддинга предлагается сконкатенировать векторы от четырёх моделей: трансформера, выбор которого мотивирован подходом argus, графовой нейросети twhin, dcn-v2-эмбеддингов и стандартизованных счётчиков. взаимодействия пользователей, предоставленные участникам, носят упорядоченный последовательный характер, поэтому важная часть решения — модель, кодирующая последовательности, — трансформер. в качестве истории пользователя брались все типы событий: добавления и удаления из корзины, покупки, посещённые страницы и запросы. трансформер в генеративной постановке учился предсказывать тип следующего взаимодействия, время до него, следующую посещённую страницу, а также следующий товар. dcn-v2-модель училась поверх эмбеддинга из трансформера и множества счётчиков, прошедших через кусочно-линейное кодирование, предсказывать отток клиентов, а также актуальные товары и категории, с которыми провзаимодействует пользователь. графовая модель twhin обучалась предсказывать связи (добавления в корзину и покупки) между пользователем и товаром. счётчики считались по разным временным промежуткам, тематическим кластерам и ценовым сегментам, а для учёта временных зависимостей использовалось экспоненциальное взвешивание. подробный разбор всех счётчиков доступен в приложении к статье. получившийся ансамбль показал качество, сопоставимое с более сложными решениями (из десятков моделей), и занял четвёртое место в финальном лидерборде. @recsyschannel разбор подготовил ❣ сергей макеев blending sequential embeddings, graphs, and engineered features: 4th place solution in recsys challenge 2025 сегодня рассказываем о статье , в которой описано решение от команды исследователей из яндекса, получившее в этом году четвёртое место на конкурсе recsys challenge . статью также приняли на конференцию recsys 2025 . челлендж был посвящён области e-commerce. в этом направлении рекомендательные модели обучают предсказывать разные виды сигналов: конверсии, релевантные товары и их категории, сумму, которую потратит клиент, и многое другое. целью челленджа было обучить эмбеддинг пользователя, который объединил бы разнородные сигналы. затем организаторы использовали этот эмбеддинг, чтобы обучить независимые модели под шесть разных задач, вроде тех, что описаны выше. как видно на картинке, для построения такого эмбеддинга предлагается сконкатенировать векторы от четырёх моделей: трансформера, выбор которого мотивирован подходом argus , графовой нейросети twhin, dcn-v2-эмбеддингов и стандартизованных счётчиков. взаимодействия пользователей, предоставленные участникам, носят упорядоченный последовательный характер, поэтому важная часть решения — модель, кодирующая последовательности, — трансформер. в качестве истории пользователя брались все типы событий: добавления и удаления из корзины, покупки, посещённые страницы и запросы. трансформер в генеративной постановке учился предсказывать тип следующего взаимодействия, время до него, следующую посещённую страницу, а также следующий товар. dcn-v2-модель училась поверх эмбеддинга из трансформера и множества счётчиков, прошедших через кусочно-линейное кодирование, предсказывать отток клиентов, а также актуальные товары и категории, с которыми провзаимодействует пользователь. графовая модель twhin обучалась предсказывать связи (добавления в корзину и покупки) между пользователем и товаром. счётчики считались по разным временным промежуткам, тематическим кластерам и ценовым сегментам, а для учёта временных зависимостей использовалось экспоненциальное взвешивание. подробный разбор всех счётчиков доступен в приложении к статье . получившийся ансамбль показал качество, сопоставимое с более сложными решениями (из десятков моделей), и занял четвёртое место в финальном лидерборде. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-12T09:10:01+00:00" href="./posts/137.html">2025-08-12 09:10 UTC</a></div>
      </div>
      <div class="post-body"><strong>Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025<br></strong><br>Сегодня рассказываем <a href="https://arxiv.org/abs/2508.06970" rel="nofollow noopener noreferrer">о статье</a>, в которой описано решение от команды исследователей из Яндекса, получившее в этом году четвёртое место на конкурсе <a href="https://recsys.acm.org/recsys25/challenge/" rel="nofollow noopener noreferrer">RecSys Challenge</a>. Статью также приняли на конференцию <a href="https://recsys.acm.org/recsys25/" rel="nofollow noopener noreferrer">RecSys 2025</a>. <br><br>Челлендж был посвящён области e-commerce. В этом направлении рекомендательные модели обучают предсказывать разные виды сигналов: конверсии, релевантные товары и их категории, сумму, которую потратит клиент, и многое другое. Целью челленджа было обучить эмбеддинг пользователя, который объединил бы разнородные сигналы. Затем организаторы использовали этот эмбеддинг, чтобы обучить независимые модели под шесть разных задач, вроде тех, что описаны выше. <br><br>Как видно на картинке, для построения такого эмбеддинга предлагается сконкатенировать векторы от четырёх моделей: трансформера, выбор которого мотивирован подходом <a href="https://t.me/RecSysChannel/133" rel="nofollow noopener noreferrer">ARGUS</a>, графовой нейросети TwHIN, DCN-v2-эмбеддингов и стандартизованных счётчиков. <br><br>Взаимодействия пользователей, предоставленные участникам, носят упорядоченный последовательный характер, поэтому важная часть решения — модель, кодирующая последовательности, — трансформер. В качестве истории пользователя брались все типы событий: добавления и удаления из корзины, покупки, посещённые страницы и запросы. <br><br>Трансформер в генеративной постановке учился предсказывать тип следующего взаимодействия, время до него, следующую посещённую страницу, а также следующий товар. DCN-v2-модель училась поверх эмбеддинга из трансформера и множества счётчиков, прошедших через кусочно-линейное кодирование, предсказывать отток клиентов, а также актуальные товары и категории, с которыми провзаимодействует пользователь. Графовая модель TwHIN обучалась предсказывать связи (добавления в корзину и покупки) между пользователем и товаром. Счётчики считались по разным временным промежуткам, тематическим кластерам и ценовым сегментам, а для учёта временных зависимостей использовалось экспоненциальное взвешивание. Подробный разбор всех счётчиков доступен <a href="https://arxiv.org/abs/2508.06970" rel="nofollow noopener noreferrer">в приложении к статье</a>. <br><br>Получившийся ансамбль показал качество, сопоставимое с более сложными решениями (из десятков моделей), и занял четвёртое место в финальном лидерборде. <br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> </em>Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/137_480.webp" srcset="../assets/media/thumbs/137_480.webp 480w, ../assets/media/137.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 876 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/137" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/137.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="133" data-search="scaling recommender transformers to one billion parameters инженеры из группы исследования перспективных рекомендательных технологий выложили на arxiv статью о подходе argus, которому ранее посвятили рассказ на датафесте и пост на хабре. сейчас статья находится на ревью на kdd’26, но текст уже доступен для всех желающих. в статье команда авторов делится опытом по масштабированию рекомендательных трансформеров, вдохновлённым нашумевшей работой actions speak louder than words. в моделях sequential recommendation можно выделить четыре оси масштабирования: число параметров в таблице эмбеддингов, длина истории пользователя, размер датасета и количество параметров в трансформере. в то время как матрицы эмбеддингов могут содержать миллиарды параметров, а датасеты достигать триллионов токенов, размеры индустриальных трансформеров всё ещё остаются чрезвычайно малы в сравнении с языковыми моделями — сотни миллионов параметров. авторам удалось обучить трансформер с миллиардом параметров на датасете из яндекс музыки и добиться прироста метрик. команда верит, что для успешного масштабирования рекомендательный трансформер должен предобучаться на фундаментальную задачу. оказывается, next item prediction может быть недостаточно — нужно уметь не только имитировать поведение предыдущей рекомендательной модели, породившей взаимодействия, но и корректировать её навыки. другими словами, помимо предсказания следующего взаимодействия полезно научиться оценивать его. естественный способ это сделать — представить историю в виде пар токенов (item, feedback), из айтема предсказывать фидбек, а из фидбека — следующий айтем. поскольку каждое взаимодействие представляется парой токенов, длина истории вырастает в два раза, увеличивая вычислительные затраты. поэтому на практике каждое взаимодействие представляли одним токеном, а предсказание фидбека обуславливали на следующий айтем. поскольку модель предобучается не только на рекомендательном трафике, но и на органическом, да ещё и без задержки (которая появляется при offline-применении), возникает необходимость в дообучении под финальную задачу. для этого авторы в том же авторегрессивном формате обучили модель на попарное ранжирование кандидатов с нужной задержкой. офлайн-эксперименты провели для четырёх размеров трансформера, наращивая число параметров экспоненциально: стартуя с 3,2 млн и заканчивая 1,007 млрд. оказалось, что полученные результаты согласуются с законом масштабирования. argus уже внедрили в яндекс музыку, увеличив вероятность лайка на 6,37% и tlt на 2,26%. внедрение оказалось самым успешным среди всех нейросетей в музыке. а ещё argus внедрили в алису, маркет, лавку, и другие сервисы яндекса. подробнее о решении можно прочитать в статье. статью написали ❣ кирилл хрыльченко, артём матвеев, сергей макеев, владимир байкалов @recsyschannel scaling recommender transformers to one billion parameters инженеры из группы исследования перспективных рекомендательных технологий выложили на arxiv статью о подходе argus, которому ранее посвятили рассказ на датафесте и пост на хабре . сейчас статья находится на ревью на kdd’26 , но текст уже доступен для всех желающих. в статье команда авторов делится опытом по масштабированию рекомендательных трансформеров, вдохновлённым нашумевшей работой actions speak louder than words . в моделях sequential recommendation можно выделить четыре оси масштабирования: число параметров в таблице эмбеддингов, длина истории пользователя, размер датасета и количество параметров в трансформере. в то время как матрицы эмбеддингов могут содержать миллиарды параметров, а датасеты достигать триллионов токенов, размеры индустриальных трансформеров всё ещё остаются чрезвычайно малы в сравнении с языковыми моделями — сотни миллионов параметров. авторам удалось обучить трансформер с миллиардом параметров на датасете из яндекс музыки и добиться прироста метрик. команда верит, что для успешного масштабирования рекомендательный трансформер должен предобучаться на фундаментальную задачу. оказывается, next item prediction может быть недостаточно — нужно уметь не только имитировать поведение предыдущей рекомендательной модели, породившей взаимодействия, но и корректировать её навыки. другими словами, помимо предсказания следующего взаимодействия полезно научиться оценивать его. естественный способ это сделать — представить историю в виде пар токенов (item, feedback), из айтема предсказывать фидбек, а из фидбека — следующий айтем. поскольку каждое взаимодействие представляется парой токенов, длина истории вырастает в два раза, увеличивая вычислительные затраты. поэтому на практике каждое взаимодействие представляли одним токеном, а предсказание фидбека обуславливали на следующий айтем. поскольку модель предобучается не только на рекомендательном трафике, но и на органическом, да ещё и без задержки (которая появляется при offline-применении), возникает необходимость в дообучении под финальную задачу. для этого авторы в том же авторегрессивном формате обучили модель на попарное ранжирование кандидатов с нужной задержкой. офлайн-эксперименты провели для четырёх размеров трансформера, наращивая число параметров экспоненциально: стартуя с 3,2 млн и заканчивая 1,007 млрд. оказалось, что полученные результаты согласуются с законом масштабирования. argus уже внедрили в яндекс музыку, увеличив вероятность лайка на 6,37% и tlt на 2,26%. внедрение оказалось самым успешным среди всех нейросетей в музыке. а ещё argus внедрили в алису, маркет, лавку, и другие сервисы яндекса. подробнее о решении можно прочитать в статье . статью написали ❣ кирилл хрыльченко, артём матвеев, сергей макеев, владимир байкалов @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-25T10:15:13+00:00" href="./posts/133.html">2025-07-25 10:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>Scaling Recommender Transformers to One Billion Parameters<br></strong><br>Инженеры из группы исследования перспективных рекомендательных технологий выложили на <a href="https://www.arxiv.org/abs/2507.15994" rel="nofollow noopener noreferrer">arXiv</a> статью о подходе ARGUS, которому ранее посвятили <a href="https://youtu.be/i6pxQNjtjF4?si=vt_kDSC7jEAUGSz7" rel="nofollow noopener noreferrer">рассказ на Датафесте</a> и <a href="https://habr.com/ru/companies/yandex/articles/919058/" rel="nofollow noopener noreferrer">пост на Хабре</a>. Сейчас статья находится на <a href="https://t.me/inforetriever/239" rel="nofollow noopener noreferrer">ревью на KDD’26</a>, но текст уже доступен для всех желающих.<br><br>В статье команда авторов делится опытом по масштабированию рекомендательных трансформеров, вдохновлённым нашумевшей работой <a href="https://t.me/RecSysChannel/48" rel="nofollow noopener noreferrer">Actions Speak Louder than Words</a>.<br><br>В моделях Sequential Recommendation можно выделить четыре оси масштабирования: число параметров в таблице эмбеддингов, длина истории пользователя, размер датасета и количество параметров в трансформере. В то время как матрицы эмбеддингов могут содержать миллиарды параметров, а датасеты достигать триллионов токенов, размеры индустриальных трансформеров всё ещё остаются чрезвычайно малы в сравнении с языковыми моделями — сотни миллионов параметров. Авторам удалось обучить трансформер с миллиардом параметров на датасете из Яндекс Музыки и добиться прироста метрик. <br><br>Команда верит, что для успешного масштабирования рекомендательный трансформер должен предобучаться на фундаментальную задачу. Оказывается, Next Item Prediction может быть недостаточно — нужно уметь не только имитировать поведение предыдущей рекомендательной модели, породившей взаимодействия, но и корректировать её навыки. Другими словами, помимо предсказания следующего взаимодействия полезно научиться оценивать его. <br><br>Естественный способ это сделать — представить историю в виде пар токенов (item, feedback), из айтема предсказывать фидбек, а из фидбека — следующий айтем. Поскольку каждое взаимодействие представляется парой токенов, длина истории вырастает в два раза, увеличивая вычислительные затраты. Поэтому на практике каждое взаимодействие представляли одним токеном, а предсказание фидбека обуславливали на следующий айтем. <br><br>Поскольку модель предобучается не только на рекомендательном трафике, но и на органическом, да ещё и без задержки (которая появляется при offline-применении), возникает необходимость в дообучении под финальную задачу. Для этого авторы в том же авторегрессивном формате обучили модель на попарное ранжирование кандидатов с нужной задержкой. <br><br>Офлайн-эксперименты провели для четырёх размеров трансформера, наращивая число параметров экспоненциально: стартуя с 3,2 млн и заканчивая 1,007 млрд. Оказалось, что полученные результаты согласуются с законом масштабирования. <br><br>ARGUS уже внедрили в Яндекс Музыку, увеличив вероятность лайка на 6,37% и TLT на 2,26%. Внедрение оказалось самым успешным среди всех нейросетей в Музыке. А ещё ARGUS внедрили в Алису, Маркет, Лавку, и другие сервисы Яндекса.<br><br>Подробнее о решении можно прочитать в <a href="https://www.arxiv.org/abs/2507.15994" rel="nofollow noopener noreferrer">статье</a>.<br><br>Статью написали <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Кирилл Хрыльченко, Артём Матвеев, Сергей Макеев, Владимир Байкалов<br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/133_480.webp" srcset="../assets/media/thumbs/133_480.webp 480w, ../assets/media/133.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="133" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 477 просмотров · 38 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/133" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/133.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="123" data-search="как прошла iclr 2025: впечатления инженеров яндекса подводим итоги конференции — для этого собрали впечатления, тенденции и интересные статьи, отмеченные инженерами, посетившими её. работы, упоминаемые в карточках: - language representations can be what recommenders need: findings and potentials - tabred: analyzing pitfalls and filling the gaps in tabular deep learning benchmarks - tabm: advancing tabular deep learning with parameter-efficient ensembling - slmrec: distilling large language models into small for sequential recommendation - cos: enhancing personalization and mitigating bias with context steering - amulet: realignment during test time for personalized preference adaptation of llms @recsyschannel #yaiclr как прошла iclr 2025: впечатления инженеров яндекса подводим итоги конференции — для этого собрали впечатления, тенденции и интересные статьи, отмеченные инженерами, посетившими её. работы, упоминаемые в карточках: - language representations can be what recommenders need: findings and potentials - tabred: analyzing pitfalls and filling the gaps in tabular deep learning benchmarks - tabm: advancing tabular deep learning with parameter-efficient ensembling - slmrec: distilling large language models into small for sequential recommendation - cos: enhancing personalization and mitigating bias with context steering - amulet: realignment during test time for personalized preference adaptation of llms @recsyschannel #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-15T09:23:49+00:00" href="./posts/123.html">2025-07-15 09:23 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как прошла ICLR 2025: впечатления инженеров Яндекса</strong><br><br>Подводим итоги конференции — для этого собрали впечатления, тенденции и интересные статьи, отмеченные инженерами, посетившими её.<br><br>Работы, упоминаемые в карточках:<br><br>- <a href="https://arxiv.org/abs/2407.05441" rel="nofollow noopener noreferrer">Language Representations Can be What Recommenders Need: Findings and Potentials</a><br>- <a href="https://arxiv.org/abs/2406.19380" rel="nofollow noopener noreferrer">TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks</a><br>- <a href="https://arxiv.org/abs/2410.24210" rel="nofollow noopener noreferrer">TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling</a><br>- <a href="https://arxiv.org/abs/2405.17890" rel="nofollow noopener noreferrer">SLMRec: Distilling Large Language Models into Small for Sequential Recommendation</a><br>- <a href="https://arxiv.org/html/2405.01768v1" rel="nofollow noopener noreferrer">CoS: Enhancing Personalization and Mitigating Bias with Context Steering</a><br>- <a href="https://arxiv.org/abs/2502.19148" rel="nofollow noopener noreferrer">Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs</a><br><br>@RecSysChannel<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/123_480.webp" srcset="../assets/media/thumbs/123_480.webp 480w, ../assets/media/123.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/124_480.webp" srcset="../assets/media/thumbs/124_480.webp 480w, ../assets/media/124.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/125_480.webp" srcset="../assets/media/thumbs/125_480.webp 480w, ../assets/media/125.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/126_480.webp" srcset="../assets/media/thumbs/126_480.webp 480w, ../assets/media/126.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/127_480.webp" srcset="../assets/media/thumbs/127_480.webp 480w, ../assets/media/127.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/128_480.webp" srcset="../assets/media/thumbs/128_480.webp 480w, ../assets/media/128.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/129_480.webp" srcset="../assets/media/thumbs/129_480.webp 480w, ../assets/media/129.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/130_480.webp" srcset="../assets/media/thumbs/130_480.webp 480w, ../assets/media/130.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/131_480.webp" srcset="../assets/media/thumbs/131_480.webp 480w, ../assets/media/131.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/132_480.webp" srcset="../assets/media/thumbs/132_480.webp 480w, ../assets/media/132.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="123" data-image-index="9" /></div></div>
      <div class="actions">
        <span>1 984 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/123" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/123.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="122" data-search="transact v2: lifelong user action sequence modeling on pinterest recommendation разбираем статью от pinterest, в которой говорят об использовании максимально длинной истории действий для улучшения рекомендаций в ленте. задача осложняется жёсткими инфраструктурными ограничениями: pinterest обслуживает более 500 млн пользователей в месяц, а объём возможных кандидатов — миллиарды пинов. при этом инференс должен укладываться в строгие тайминги, несмотря на тысячи параллельных gpu-запросов. pinterest остаётся верен классической трёхстадийной архитектуре: retrieval — scoring — blending. на первом этапе модель отбирает несколько тысяч кандидатов, которые затем проходят pointwise-ранжирование. ранжирующая модель оптимизируется исключительно под фид. особое внимание уделяется тому, как используется длинная история действий — ключевое отличие от предыдущих решений. несмотря на эффектный посыл про «десятки тысяч событий в истории», фактически модель работает не с «сырой» историей, а с её сжатием. история формируется из трёх источников: полной пользовательской активности, событий в рантайме и импрессий. для каждого кандидата модель отбирает ближайшие по контенту события из этих источников (а также несколько последних взаимодействий независимо от контента), формируя итоговую последовательность фиксированной длины — порядка сотен событий. эта сжатая история и обрабатывается в трансформере. модель представляет собой multitask-архитектуру в pointwise-постановке. на вход она получает эмбеддинги, включающие обучаемые параметры, категорию взаимодействия, позиционный эмбеддинг и эмбеддинг пина. последний строится как объединение эмбеддинга кандидата и карточек из истории, к которым кандидат наиболее близок по контенту. трансформер с минимальным числом параметров (два слоя, одна attention-глава, скрытое представление размерности 64) пропускает эту последовательность и генерирует выходные векторы, которые подаются в линейный слой для генерации прогнозов. loss-модель использует два компонента: взвешенную кросс-энтропию по каждому действию (лайк, добавление в избранное и прочее) и sampled softmax loss на задачу next action prediction. в качестве позитивов используются все позитивные взаимодействия в последовательности, а в качестве негативов — показы. авторы отмечают, что подход показывает себя лучше, чем batch sampling. среди архитектурных решений также интересно, что один и тот же пин, встретившийся с разными действиями, кодируется как multi-hot-вектор, а эмбеддинги пинов хранятся в квантизованном виде (int8) и деквантизируются в float16 перед подачей в трансформер. ключевые нововведения — в инфраструктуре. стандартные решения на pytorch оказались неприменимы из-за избыточной материализации данных. разработчики переписали инференс на собственный сервер с кастомными трансформерными ядрами в triton (речь не о сервере nvidia, а о языке для компиляции под gpu). такой подход позволил избежать дополнительных обращений к памяти: квантизованные векторы декодируются, нормализуются и сразу же используются для поиска ближайших соседей. ещё в работе реализовали оптимизации вроде кэширования длинных пользовательских историй в сессии (чтобы избежать их загрузки при каждом реквесте), дедупликации запросов и эффективного распределения памяти между cpu и gpu. всё вместе это дало серьезный прирост производительности: latency снизился в 2–3 раза по сравнению с pytorch, использование памяти тоже оказалось эффективным. переход на собственные ядра позволил сократить время инференса на 85% и расход памяти на 13% при длине последовательности 192. решение выигрывает и у flashattention 2: ядра оказались на 66% быстрее и потребляли на 5% меньше памяти, при этом flashattention 2 не поддерживает пользовательское маскирование токенов. авторы сравнивают эффективность transact v2 с другими моделями, в том числе с первым transact. основной вывод: использование гораздо более длинной пользовательской истории и набор инженерных решений дают заметный прирост качества рекомендаций без потерь в скорости и стабильности. @recsyschannel разбор подготовил ❣ руслан кулиев transact v2: lifelong user action sequence modeling on pinterest recommendation разбираем статью от pinterest, в которой говорят об использовании максимально длинной истории действий для улучшения рекомендаций в ленте. задача осложняется жёсткими инфраструктурными ограничениями: pinterest обслуживает более 500 млн пользователей в месяц, а объём возможных кандидатов — миллиарды пинов. при этом инференс должен укладываться в строгие тайминги, несмотря на тысячи параллельных gpu-запросов. pinterest остаётся верен классической трёхстадийной архитектуре: retrieval — scoring — blending. на первом этапе модель отбирает несколько тысяч кандидатов, которые затем проходят pointwise-ранжирование. ранжирующая модель оптимизируется исключительно под фид. особое внимание уделяется тому, как используется длинная история действий — ключевое отличие от предыдущих решений. несмотря на эффектный посыл про «десятки тысяч событий в истории», фактически модель работает не с «сырой» историей, а с её сжатием. история формируется из трёх источников: полной пользовательской активности, событий в рантайме и импрессий. для каждого кандидата модель отбирает ближайшие по контенту события из этих источников (а также несколько последних взаимодействий независимо от контента), формируя итоговую последовательность фиксированной длины — порядка сотен событий. эта сжатая история и обрабатывается в трансформере. модель представляет собой multitask-архитектуру в pointwise-постановке. на вход она получает эмбеддинги, включающие обучаемые параметры, категорию взаимодействия, позиционный эмбеддинг и эмбеддинг пина. последний строится как объединение эмбеддинга кандидата и карточек из истории, к которым кандидат наиболее близок по контенту. трансформер с минимальным числом параметров (два слоя, одна attention-глава, скрытое представление размерности 64) пропускает эту последовательность и генерирует выходные векторы, которые подаются в линейный слой для генерации прогнозов. loss-модель использует два компонента: взвешенную кросс-энтропию по каждому действию (лайк, добавление в избранное и прочее) и sampled softmax loss на задачу next action prediction. в качестве позитивов используются все позитивные взаимодействия в последовательности, а в качестве негативов — показы. авторы отмечают, что подход показывает себя лучше, чем batch sampling. среди архитектурных решений также интересно, что один и тот же пин, встретившийся с разными действиями, кодируется как multi-hot-вектор, а эмбеддинги пинов хранятся в квантизованном виде (int8) и деквантизируются в float16 перед подачей в трансформер. ключевые нововведения — в инфраструктуре. стандартные решения на pytorch оказались неприменимы из-за избыточной материализации данных. разработчики переписали инференс на собственный сервер с кастомными трансформерными ядрами в triton (речь не о сервере nvidia, а о языке для компиляции под gpu). такой подход позволил избежать дополнительных обращений к памяти: квантизованные векторы декодируются, нормализуются и сразу же используются для поиска ближайших соседей. ещё в работе реализовали оптимизации вроде кэширования длинных пользовательских историй в сессии (чтобы избежать их загрузки при каждом реквесте), дедупликации запросов и эффективного распределения памяти между cpu и gpu. всё вместе это дало серьезный прирост производительности: latency снизился в 2–3 раза по сравнению с pytorch, использование памяти тоже оказалось эффективным. переход на собственные ядра позволил сократить время инференса на 85% и расход памяти на 13% при длине последовательности 192. решение выигрывает и у flashattention 2: ядра оказались на 66% быстрее и потребляли на 5% меньше памяти, при этом flashattention 2 не поддерживает пользовательское маскирование токенов. авторы сравнивают эффективность transact v2 с другими моделями, в том числе с первым transact. основной вывод: использование гораздо более длинной пользовательской истории и набор инженерных решений дают заметный прирост качества рекомендаций без потерь в скорости и стабильности. @recsyschannel разбор подготовил ❣ руслан кулиев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-11T08:01:54+00:00" href="./posts/122.html">2025-07-11 08:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation</strong><br><br>Разбираем<a href="https://arxiv.org/pdf/2506.02267" rel="nofollow noopener noreferrer"> статью</a> от Pinterest, в которой говорят об использовании максимально длинной истории действий для улучшения рекомендаций в ленте. Задача осложняется жёсткими инфраструктурными ограничениями: Pinterest обслуживает более 500 млн пользователей в месяц, а объём возможных кандидатов — миллиарды пинов. При этом инференс должен укладываться в строгие тайминги, несмотря на тысячи параллельных GPU-запросов.<br><br>Pinterest остаётся верен классической трёхстадийной архитектуре: retrieval — scoring — blending. На первом этапе модель отбирает несколько тысяч кандидатов, которые затем проходят pointwise-ранжирование. Ранжирующая модель оптимизируется исключительно под фид. Особое внимание уделяется тому, как используется длинная история действий  — ключевое отличие от предыдущих решений.<br><br>Несмотря на эффектный посыл про «десятки тысяч событий в истории», фактически модель работает не с «сырой» историей, а с её сжатием. История формируется из трёх источников: полной пользовательской активности, событий в рантайме и импрессий. Для каждого кандидата модель отбирает ближайшие по контенту события из этих источников (а также несколько последних взаимодействий независимо от контента), формируя итоговую последовательность фиксированной длины — порядка сотен событий. Эта сжатая история и обрабатывается в трансформере.<br><br>Модель представляет собой multitask-архитектуру в pointwise-постановке. На вход она получает эмбеддинги, включающие обучаемые параметры, категорию взаимодействия, позиционный эмбеддинг и эмбеддинг пина. Последний строится как объединение эмбеддинга кандидата и карточек из истории, к которым кандидат наиболее близок по контенту. Трансформер с минимальным числом параметров (два слоя, одна attention-глава, скрытое представление размерности 64) пропускает эту последовательность и генерирует выходные векторы, которые подаются в линейный слой для генерации прогнозов.<br><br>Loss-модель использует два компонента: взвешенную кросс-энтропию по каждому действию (лайк, добавление в избранное и прочее) и sampled softmax loss на задачу next action prediction. В качестве позитивов используются все позитивные взаимодействия в последовательности, а в качестве негативов — показы. Авторы отмечают, что подход показывает себя лучше, чем batch sampling. Среди архитектурных решений также интересно, что один и тот же пин, встретившийся с разными действиями, кодируется как multi-hot-вектор, а эмбеддинги пинов хранятся в квантизованном виде (int8) и деквантизируются в float16 перед подачей в трансформер.<br><br>Ключевые нововведения — в инфраструктуре. Стандартные решения на PyTorch оказались неприменимы из-за избыточной материализации данных. Разработчики переписали инференс на собственный сервер с кастомными трансформерными ядрами в Triton (речь не о сервере NVIDIA, а о языке для компиляции под GPU). Такой подход позволил избежать дополнительных обращений к памяти: квантизованные векторы декодируются, нормализуются и сразу же используются для поиска ближайших соседей.<br><br>Ещё в работе реализовали оптимизации вроде кэширования длинных пользовательских историй в сессии (чтобы избежать их загрузки при каждом реквесте), дедупликации запросов и эффективного распределения памяти между CPU и GPU. Всё вместе это дало серьезный прирост производительности: latency снизился в 2–3 раза по сравнению с PyTorch, использование памяти тоже оказалось эффективным. Переход на собственные ядра позволил сократить время инференса на 85% и расход памяти на 13% при длине последовательности 192. Решение выигрывает и у FlashAttention 2: ядра оказались на 66% быстрее и потребляли на 5% меньше памяти, при этом FlashAttention 2 не поддерживает пользовательское маскирование токенов.<br><br>Авторы сравнивают эффективность TransAct V2 с другими моделями, в том числе с первым TransAct. Основной вывод: использование гораздо более длинной пользовательской истории и набор инженерных решений дают заметный прирост качества рекомендаций без потерь в скорости и стабильности.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Руслан Кулиев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/122_480.webp" srcset="../assets/media/thumbs/122_480.webp 480w, ../assets/media/122.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="122" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 124 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/122" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/122.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="121" data-search="мы с отличными новостями! статью о датасете yambda приняли на oral конференции recsys 2025. поздравляем команду рекомендательных технологий яндекса! мы с отличными новостями! статью о датасете yambda приняли на oral конференции recsys 2025. поздравляем команду рекомендательных технологий яндекса!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-04T16:45:06+00:00" href="./posts/121.html">2025-07-04 16:45 UTC</a></div>
      </div>
      <div class="post-body">Мы с отличными новостями! <a href="http://arxiv.org/abs/2505.22238" rel="nofollow noopener noreferrer">Статью о датасете Yambda</a> приняли на Oral конференции RecSys 2025. Поздравляем команду рекомендательных технологий Яндекса!</div>
      <div class="actions">
        <span>1 946 просмотров · 69 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/121" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/121.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="120" data-search="">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-04T16:45:06+00:00" href="./posts/120.html">2025-07-04 16:45 UTC</a></div>
      </div>
      <div class="post-body"><p class="muted">[без текста]</p><div class="media"><video controls preload="metadata" src="../assets/media/120_video.mp4"></video></div></div>
      <div class="actions">
        <span>1 823 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/120" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/120.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="119" data-search="scaling transformers for discriminative recommendation via generative pretraining тема масштабирования моделей в рекомендательных системах продолжает набирать популярность. недавно alibaba представила работу о масштабировании ранжирующих моделей для персональных рекомендаций товаров на aliexpress. о ней и поговорим. в ml выделяют два класса вероятностных моделей: — дискриминативные — моделируют условное распределение p(y|x) (предсказывают метку y по данным x). примеры: логистическая регрессия, большинство моделей для ранжирования. — генеративные — моделируют совместное распределение p(x, y), что позволяет генерировать данные. примеры: gpt, диффузионные модели. авторы фокусируются на дискриминативных ранжирующих моделях, предсказывающих ctr и cvr. однако при попытке масштабировать трансформер, обучаемый только на дискриминативную задачу, наблюдается переобучение. это связано с сильной разреженностью позитивного таргета для ранжирования: увеличение модели ведёт к деградации качества. решение — добавить генеративное предобучание (метод назван gpsd — generative pretraining for scalable discriminative recommendation), а затем — дискриминативное дообучение. ключевое преимущество заключается в том, что генеративное предобучание не страдает от сильного разрыва в обобщающей способности между обучением и валидацией, что и открывает путь к масштабированию. технические детали 1. генеративное предобучение — используется архитектура трансформера с каузальной маской над историей взаимодействий. — модель решает две задачи: предсказание следующего товара (sampled softmax loss с равномерно семплированными негативами из каталога) и предсказание его категории. — при кодировании товаров применяются обучаемые эмбеддинги для каждого айтема, а также дополнительные фичи, такие как категория. — агрегация делается путём суммирования. 2. дискриминативное дообучение — добавляется cls-токен в историю. его выходной вектор используется как представление пользователя. — это представление конкатенируется с фичами кандидата (товара, для которого предсказывается ctr) и подается на вход mlp. 3. стратегия переноса весов — наилучшие результаты даёт инициализация и заморозка матриц эмбеддингов (item embeddings) с этапа генеративного предобучения. — веса самого трансформера можно инициализировать как предобученными, так и случайными значениями — результат сопоставим. ключевые результаты — без генеративного предобучения: увеличение модели не даёт прироста качества (auc) из-за переобучения, наблюдается даже деградация. — с gpsd: устойчивое масштабирование — рост auc при увеличении размера модели от 13 тысяч до 300 млн параметров. выведен степенной закон зависимости auc от числа параметров. — a/b-тест на рекомендательной платформе aliexpress: в продакшен выведена модель с тремя слоями трансформера и скрытой размерностью 160 (очень компактная). результат: +7,97% gmv, +1,79% покупок. замечания 1. использованные модели и датасеты — небольшие, что немного подрывает веру в результаты. 2. при масштабировании одновременно с dense-частью (трансформер) увеличивались и sparse-часть (матрицы эмбеддингов), что также могло быть фактором роста качества. для более честного замера её размер нужно было зафиксировать. @recsyschannel разбор подготовил ❣ артём матвеев scaling transformers for discriminative recommendation via generative pretraining тема масштабирования моделей в рекомендательных системах продолжает набирать популярность. недавно alibaba представила работу о масштабировании ранжирующих моделей для персональных рекомендаций товаров на aliexpress. о ней и поговорим. в ml выделяют два класса вероятностных моделей: — дискриминативные — моделируют условное распределение p(y|x) (предсказывают метку y по данным x). примеры: логистическая регрессия, большинство моделей для ранжирования. — генеративные — моделируют совместное распределение p(x, y), что позволяет генерировать данные. примеры: gpt, диффузионные модели. авторы фокусируются на дискриминативных ранжирующих моделях, предсказывающих ctr и cvr. однако при попытке масштабировать трансформер, обучаемый только на дискриминативную задачу, наблюдается переобучение. это связано с сильной разреженностью позитивного таргета для ранжирования: увеличение модели ведёт к деградации качества. решение — добавить генеративное предобучание (метод назван gpsd — generative pretraining for scalable discriminative recommendation), а затем — дискриминативное дообучение. ключевое преимущество заключается в том, что генеративное предобучание не страдает от сильного разрыва в обобщающей способности между обучением и валидацией, что и открывает путь к масштабированию. технические детали 1. генеративное предобучение — используется архитектура трансформера с каузальной маской над историей взаимодействий. — модель решает две задачи: предсказание следующего товара (sampled softmax loss с равномерно семплированными негативами из каталога) и предсказание его категории. — при кодировании товаров применяются обучаемые эмбеддинги для каждого айтема, а также дополнительные фичи, такие как категория. — агрегация делается путём суммирования. 2. дискриминативное дообучение — добавляется cls-токен в историю. его выходной вектор используется как представление пользователя. — это представление конкатенируется с фичами кандидата (товара, для которого предсказывается ctr) и подается на вход mlp. 3. стратегия переноса весов — наилучшие результаты даёт инициализация и заморозка матриц эмбеддингов (item embeddings) с этапа генеративного предобучения. — веса самого трансформера можно инициализировать как предобученными, так и случайными значениями — результат сопоставим. ключевые результаты — без генеративного предобучения: увеличение модели не даёт прироста качества (auc) из-за переобучения, наблюдается даже деградация. — с gpsd: устойчивое масштабирование — рост auc при увеличении размера модели от 13 тысяч до 300 млн параметров. выведен степенной закон зависимости auc от числа параметров. — a/b-тест на рекомендательной платформе aliexpress: в продакшен выведена модель с тремя слоями трансформера и скрытой размерностью 160 (очень компактная). результат: +7,97% gmv, +1,79% покупок. замечания 1. использованные модели и датасеты — небольшие, что немного подрывает веру в результаты. 2. при масштабировании одновременно с dense-частью (трансформер) увеличивались и sparse-часть (матрицы эмбеддингов), что также могло быть фактором роста качества. для более честного замера её размер нужно было зафиксировать. @recsyschannel разбор подготовил ❣ артём матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-03T08:04:03+00:00" href="./posts/119.html">2025-07-03 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Scaling Transformers for Discriminative Recommendation via Generative Pretraining</strong><br><br>Тема масштабирования моделей в рекомендательных системах продолжает набирать популярность. Недавно Alibaba представила <a href="https://arxiv.org/pdf/2506.03699" rel="nofollow noopener noreferrer">работу</a> о масштабировании ранжирующих моделей для персональных рекомендаций товаров на AliExpress. О ней и поговорим.<br><br>В ML выделяют два класса вероятностных моделей:<br><br>— Дискриминативные — моделируют условное распределение p(y|x) (предсказывают метку y по данным x). Примеры: логистическая регрессия, большинство моделей для ранжирования.<br>— Генеративные — моделируют совместное распределение p(x, y), что позволяет генерировать данные. Примеры: GPT, диффузионные модели.<br><br>Авторы фокусируются на дискриминативных ранжирующих моделях, предсказывающих CTR и CVR. Однако при попытке масштабировать трансформер, обучаемый только на дискриминативную задачу, наблюдается переобучение. Это связано с сильной разреженностью позитивного таргета для ранжирования: увеличение модели ведёт к деградации качества.<br><br>Решение — добавить генеративное предобучание (метод назван GPSD — Generative Pretraining for Scalable Discriminative Recommendation), а затем — дискриминативное дообучение. Ключевое преимущество заключается в том, что генеративное предобучание не страдает от сильного разрыва в обобщающей способности между обучением и валидацией, что и открывает путь к масштабированию.<br><br><strong>Технические детали</strong><br><br>1.  Генеративное предобучение<br><br>— Используется архитектура трансформера с каузальной маской над историей взаимодействий.<br>— Модель решает две задачи: предсказание следующего товара (Sampled Softmax Loss с равномерно семплированными негативами из каталога) и предсказание его категории.<br>— При кодировании товаров применяются обучаемые эмбеддинги для каждого айтема, а также дополнительные фичи, такие как категория.<br>— Агрегация делается путём суммирования.<br><br>2.  Дискриминативное дообучение<br><br>— Добавляется CLS-токен в историю. Его выходной вектор используется как представление пользователя.<br>— Это представление конкатенируется с фичами кандидата (товара, для которого предсказывается CTR) и подается на вход MLP.<br><br>3.  Стратегия переноса весов<br><br>— Наилучшие результаты даёт инициализация и заморозка матриц эмбеддингов (item embeddings) с этапа генеративного предобучения.<br>— Веса самого трансформера можно инициализировать как предобученными, так и случайными значениями — результат сопоставим.<br><br><strong>Ключевые результаты<br></strong><br>— Без генеративного предобучения: увеличение модели не даёт прироста качества (AUC) из-за переобучения, наблюдается даже деградация.<br>— С GPSD: устойчивое масштабирование — рост AUC при увеличении размера модели от 13 тысяч до 300 млн параметров. Выведен степенной закон зависимости AUC от числа параметров.<br>— A/B-тест на рекомендательной платформе AliExpress: в продакшен выведена модель с тремя слоями трансформера и скрытой размерностью 160 (очень компактная). <br>Результат: +7,97% GMV, +1,79% покупок.<br><br><strong>Замечания</strong><br><br>1.  Использованные модели и датасеты — небольшие, что немного подрывает веру в результаты.<br><br>2.  При масштабировании одновременно с dense-частью (трансформер) увеличивались и sparse-часть (матрицы эмбеддингов), что также могло быть фактором роста качества. Для более честного замера её размер нужно было зафиксировать.<br><br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Артём Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/119_480.webp" srcset="../assets/media/thumbs/119_480.webp 480w, ../assets/media/119.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 864 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/119" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/119.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="117" data-search="preference diffusion и decoupled embeddings: две статьи о масштабируемых рекомендациях сегодня разбираем ещё две статьи с iclr — о диффузионных моделях в рекомендациях и о борьбе с градиентными конфликтами в длинных пользовательских историях. preference diffusion for recommendation авторы пробуют использовать диффузионные модели в рекомендательных системах. изначально это направление кажется не вполне очевидным: если с изображением ясно, как его зашумить, то что значит «наложить шум» на эмбеддинг айтема или пользователя — не совсем понятно. авторы основываются на более ранней статье — dreamrec — и развивают её идею. в dreamrec использовали диффузионку как генератор: сначала генерировали «идеальный» вектор айтема, а потом искали ближайший из базы. в этой статье пошли дальше: встроили диффузионную модель в стандартный стек рекомендательных систем и учли важные инженерные моменты. во-первых, mse заменили на косинусное расстояние в лоссе. во-вторых, стали учитывать негативы в обучении, чтобы модель не просто приближалась к позитивному айтему, но и отличала его от негативных. вместо того чтобы обрабатывать сотни негативов по отдельности (что тяжело вычислительно), авторы сэмплируют 256 негативов, усредняют, берут центроид — и используют как один «усреднённый негатив». такая тактика резко снижает нагрузку, но сохраняет информативность. по словам одной из соавторов, ан чжан, идея эффективного добавления негативов и упрощение вычислений — главный вклад статьи в индустрию — без этого диффузионка в рекомендациях просто не взлетает. ещё одно улучшение касается больших размерностей эмбеддингов. авторы показали, что такие модели начинают работать только на размерностях больше 2 тысяч. привычные 64 или 128 не дают никакого результата — лосс почти не убывает. итог: модель обучается быстрее, чем в предыдущих подходах. её удалось встроить в классический пайплайн даже без больших кандидатов (в отличие от alpharec). long-sequence recommendation models need decoupled embeddings интересная работа от команды из tencent. у них большая рекомендательная система с очень длинными пользовательскими историями и огромным числом айтемов. это накладывает ограничения и по вычислениям, и по архитектуре. они используют трансформер, который сначала применяет attention к длинной истории, чтобы выбрать важные элементы, и уже по ним строит итоговую репрезентацию. в стандартном подходе одни и те же эмбеддинги используются и для блока attention, и для блока representation. авторы показывают, что в таком случае возникает конфликт между градиентами: одна часть модели (например, attention) толкает эмбеддинги в одну сторону, другая (representation) — в другую. в статье подсчитали, как часто градиенты конфликтуют — оказалось, больше чем в половине случаев. ещё исследователи измеряют, сколько лосса проходит через каждую часть — и оказывается, что representation тянет на себя ощутимо больше, чем attention. это приводит к перекосу: одна часть доминирует, другая «умирает». авторы пробуют решить это простыми способами — например, добавить линейные преобразования до и после эмбеддингов. но это не помогает. несмотря на раздельную обработку, на вход всё равно идут одинаковые эмбедды, и конфликт сохраняется. тогда исследователи делают жёсткое разнесение: делят эмбеддинг на две части — одна идёт в attention, другая — в representation. причём первая в 3–4 раза меньше, потому что attention всё равно получает меньше градиентного сигнала, и для него достаточно компактного представления. это решение устраняет конфликт, ускоряет инфернес и не ухудшает качество. визуально это хорошо видно на графиках: чем больше разнесение и уменьшение attention-части, тем выше эффективность. интересный побочный эффект — за счёт того, что attention работает на меньших векторах, система становится до 50% быстрее. авторы утверждают, что решение уже внедрено в продакшн и работает там на больших масштабах. @recsyschannel обзор подготовил ❣ василий астахов #yaiclr preference diffusion и decoupled embeddings: две статьи о масштабируемых рекомендациях сегодня разбираем ещё две статьи с iclr — о диффузионных моделях в рекомендациях и о борьбе с градиентными конфликтами в длинных пользовательских историях. preference diffusion for recommendation авторы пробуют использовать диффузионные модели в рекомендательных системах. изначально это направление кажется не вполне очевидным: если с изображением ясно, как его зашумить, то что значит «наложить шум» на эмбеддинг айтема или пользователя — не совсем понятно. авторы основываются на более ранней статье — dreamrec — и развивают её идею. в dreamrec использовали диффузионку как генератор: сначала генерировали «идеальный» вектор айтема, а потом искали ближайший из базы. в этой статье пошли дальше: встроили диффузионную модель в стандартный стек рекомендательных систем и учли важные инженерные моменты. во-первых, mse заменили на косинусное расстояние в лоссе. во-вторых, стали учитывать негативы в обучении, чтобы модель не просто приближалась к позитивному айтему, но и отличала его от негативных. вместо того чтобы обрабатывать сотни негативов по отдельности (что тяжело вычислительно), авторы сэмплируют 256 негативов, усредняют, берут центроид — и используют как один «усреднённый негатив». такая тактика резко снижает нагрузку, но сохраняет информативность. по словам одной из соавторов, ан чжан, идея эффективного добавления негативов и упрощение вычислений — главный вклад статьи в индустрию — без этого диффузионка в рекомендациях просто не взлетает. ещё одно улучшение касается больших размерностей эмбеддингов. авторы показали, что такие модели начинают работать только на размерностях больше 2 тысяч. привычные 64 или 128 не дают никакого результата — лосс почти не убывает. итог: модель обучается быстрее, чем в предыдущих подходах. её удалось встроить в классический пайплайн даже без больших кандидатов (в отличие от alpharec). long-sequence recommendation models need decoupled embeddings интересная работа от команды из tencent. у них большая рекомендательная система с очень длинными пользовательскими историями и огромным числом айтемов. это накладывает ограничения и по вычислениям, и по архитектуре. они используют трансформер, который сначала применяет attention к длинной истории, чтобы выбрать важные элементы, и уже по ним строит итоговую репрезентацию. в стандартном подходе одни и те же эмбеддинги используются и для блока attention, и для блока representation. авторы показывают, что в таком случае возникает конфликт между градиентами: одна часть модели (например, attention) толкает эмбеддинги в одну сторону, другая (representation) — в другую. в статье подсчитали, как часто градиенты конфликтуют — оказалось, больше чем в половине случаев. ещё исследователи измеряют, сколько лосса проходит через каждую часть — и оказывается, что representation тянет на себя ощутимо больше, чем attention. это приводит к перекосу: одна часть доминирует, другая «умирает». авторы пробуют решить это простыми способами — например, добавить линейные преобразования до и после эмбеддингов. но это не помогает. несмотря на раздельную обработку, на вход всё равно идут одинаковые эмбедды, и конфликт сохраняется. тогда исследователи делают жёсткое разнесение: делят эмбеддинг на две части — одна идёт в attention, другая — в representation. причём первая в 3–4 раза меньше, потому что attention всё равно получает меньше градиентного сигнала, и для него достаточно компактного представления. это решение устраняет конфликт, ускоряет инфернес и не ухудшает качество. визуально это хорошо видно на графиках: чем больше разнесение и уменьшение attention-части, тем выше эффективность. интересный побочный эффект — за счёт того, что attention работает на меньших векторах, система становится до 50% быстрее. авторы утверждают, что решение уже внедрено в продакшн и работает там на больших масштабах. @recsyschannel обзор подготовил ❣ василий астахов #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-30T08:04:13+00:00" href="./posts/117.html">2025-06-30 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Preference Diffusion и Decoupled Embeddings: две статьи о масштабируемых рекомендациях </strong><br><br>Сегодня разбираем ещё две статьи с ICLR — о диффузионных моделях в рекомендациях и о борьбе с градиентными конфликтами в длинных пользовательских историях.<br><br><a href="https://arxiv.org/abs/2410.13117" rel="nofollow noopener noreferrer"><strong>Preference Diffusion for Recommendation<br></strong></a><br>Авторы пробуют использовать диффузионные модели в рекомендательных системах. Изначально это направление кажется не вполне очевидным: если с изображением ясно, как его зашумить, то что значит «наложить шум» на эмбеддинг айтема или пользователя — не совсем понятно.<br><br>Авторы основываются на более ранней статье — <a href="https://arxiv.org/abs/2310.20453" rel="nofollow noopener noreferrer">DreamRec</a> — и развивают её идею. В DreamRec использовали диффузионку как генератор: сначала генерировали «идеальный» вектор айтема, а потом искали ближайший из базы. В этой статье пошли дальше: встроили диффузионную модель в стандартный стек рекомендательных систем и учли важные инженерные моменты. <br><br>Во-первых, MSE заменили на косинусное расстояние в лоссе. Во-вторых, стали учитывать негативы в обучении, чтобы модель не просто приближалась к позитивному айтему, но и отличала его от негативных.<br><br>Вместо того чтобы обрабатывать сотни негативов по отдельности (что тяжело вычислительно), авторы сэмплируют 256 негативов, усредняют, берут центроид — и используют как один «усреднённый негатив». Такая тактика резко снижает нагрузку, но сохраняет информативность. По словам одной из соавторов, Ан Чжан, идея эффективного добавления негативов и упрощение вычислений — главный вклад статьи в индустрию — без этого диффузионка в рекомендациях просто не взлетает.<br><br>Ещё одно улучшение касается больших размерностей эмбеддингов. Авторы показали, что такие модели начинают работать только на размерностях больше 2 тысяч. Привычные 64 или 128 не дают никакого результата — лосс почти не убывает. <br><br>Итог: модель обучается быстрее, чем в предыдущих подходах. Её удалось встроить в классический пайплайн даже без больших кандидатов (в отличие от AlphaRec).<br><br><a href="https://arxiv.org/abs/2410.02604" rel="nofollow noopener noreferrer"><strong>Long-Sequence Recommendation Models Need Decoupled Embeddings</strong></a><br><br>Интересная работа от команды из Tencent. У них большая рекомендательная система  с очень длинными пользовательскими историями и огромным числом айтемов. Это накладывает ограничения и по вычислениям, и по архитектуре. Они используют трансформер, который сначала применяет attention к длинной истории, чтобы выбрать важные элементы, и уже по ним строит итоговую репрезентацию. <br><br>В стандартном подходе одни и те же эмбеддинги используются и для блока attention, и для блока representation. <br><br>Авторы показывают, что в таком случае возникает конфликт между градиентами: одна часть модели (например, attention) толкает эмбеддинги в одну сторону, другая (representation) — в другую. В статье подсчитали, как часто градиенты конфликтуют — оказалось, больше чем в половине случаев.<br><br>Ещё исследователи измеряют, сколько лосса проходит через каждую часть — и оказывается, что representation тянет на себя ощутимо больше, чем attention. Это приводит к перекосу: одна часть доминирует, другая «умирает».<br><br>Авторы пробуют решить это простыми способами — например, добавить линейные преобразования до и после эмбеддингов. Но это не помогает. Несмотря на раздельную обработку, на вход всё равно идут одинаковые эмбедды, и конфликт сохраняется.<br><br>Тогда исследователи делают жёсткое разнесение: делят эмбеддинг на две части — одна идёт в attention, другая — в representation. Причём первая в 3–4 раза меньше, потому что attention всё равно получает меньше градиентного сигнала, и для него достаточно компактного представления. Это решение устраняет конфликт, ускоряет инфернес и не ухудшает качество. Визуально это хорошо видно на графиках: чем больше разнесение и уменьшение attention-части, тем выше эффективность.<br><br>Интересный побочный эффект — за счёт того, что attention работает на меньших векторах, система становится до 50% быстрее. <br><br>Авторы утверждают, что решение уже внедрено в продакшн и работает там на больших масштабах. <br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Василий Астахов<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/117_480.webp" srcset="../assets/media/thumbs/117_480.webp 480w, ../assets/media/117.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="117" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/118_480.webp" srcset="../assets/media/thumbs/118_480.webp 480w, ../assets/media/118.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="117" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 397 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/117" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/117.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="107" data-search="scaling law в рекомендательных системах законы масштабирования вышли за рамки nlp и успешно применяются в рекомендательных системах. в наших карточках исследователь владимир байкалов затронул последние работы на эту тему. с обзором прошлых статей можно ознакомиться в этом посте. работы, упомянутые в карточках: - language models are unsupervised multitask learners - scaling laws for neural language models - training compute-optimal large language models - actions speak louder than words: trillion-parameter sequential transducers for generative recommendations - scaling new frontiers: insights into large recommendation models - unlocking scaling law in industrial recommendation systems with a three-step paradigm based large user model - scalable cross-entropy loss for sequential recommendations with large item catalogs - разбор статьи hstu в канале «рекомендательная» @recsyschannel обзор подготовил ❣ владимир байкалов scaling law в рекомендательных системах законы масштабирования вышли за рамки nlp и успешно применяются в рекомендательных системах. в наших карточках исследователь владимир байкалов затронул последние работы на эту тему. с обзором прошлых статей можно ознакомиться в этом посте . работы, упомянутые в карточках: - language models are unsupervised multitask learners - scaling laws for neural language models - training compute-optimal large language models - actions speak louder than words: trillion-parameter sequential transducers for generative recommendations - scaling new frontiers: insights into large recommendation models - unlocking scaling law in industrial recommendation systems with a three-step paradigm based large user model - scalable cross-entropy loss for sequential recommendations with large item catalogs - разбор статьи hstu в канале «рекомендательная» @recsyschannel обзор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-26T12:20:16+00:00" href="./posts/107.html">2025-06-26 12:20 UTC</a></div>
      </div>
      <div class="post-body"><strong>Scaling law в рекомендательных системах<br></strong><br>Законы масштабирования вышли за рамки NLP и успешно применяются в рекомендательных системах. В наших карточках исследователь Владимир Байкалов затронул последние работы на эту тему. С обзором прошлых статей можно ознакомиться <a href="https://t.me/inforetriever/82" rel="nofollow noopener noreferrer">в этом посте</a>.<br><br>Работы, упомянутые в карточках:<br>- <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow noopener noreferrer">Language Models are Unsupervised Multitask Learners</a><br>- <a href="https://arxiv.org/abs/2001.08361" rel="nofollow noopener noreferrer">Scaling Laws for Neural Language Models</a><br>- <a href="https://arxiv.org/abs/2203.15556" rel="nofollow noopener noreferrer">Training Compute-Optimal Large Language Models</a><br>- <a href="https://arxiv.org/abs/2402.17152" rel="nofollow noopener noreferrer">Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations</a><br>- <a href="https://arxiv.org/abs/2412.00714" rel="nofollow noopener noreferrer">Scaling New Frontiers: Insights into Large Recommendation Models</a><br>- <a href="https://arxiv.org/abs/2502.08309" rel="nofollow noopener noreferrer">Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model</a><br>- <a href="https://dl.acm.org/doi/10.1145/3640457.3688140" rel="nofollow noopener noreferrer">Scalable Cross-Entropy Loss for Sequential Recommendations with Large Item Catalogs</a><br>- <a href="https://t.me/RecSysChannel/48" rel="nofollow noopener noreferrer">Разбор статьи HSTU в канале «Рекомендательная»</a><br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/107_480.webp" srcset="../assets/media/thumbs/107_480.webp 480w, ../assets/media/107.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/108_480.webp" srcset="../assets/media/thumbs/108_480.webp 480w, ../assets/media/108.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/109_480.webp" srcset="../assets/media/thumbs/109_480.webp 480w, ../assets/media/109.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/110_480.webp" srcset="../assets/media/thumbs/110_480.webp 480w, ../assets/media/110.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/111_480.webp" srcset="../assets/media/thumbs/111_480.webp 480w, ../assets/media/111.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/112_480.webp" srcset="../assets/media/thumbs/112_480.webp 480w, ../assets/media/112.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/113_480.webp" srcset="../assets/media/thumbs/113_480.webp 480w, ../assets/media/113.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/114_480.webp" srcset="../assets/media/thumbs/114_480.webp 480w, ../assets/media/114.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/115_480.webp" srcset="../assets/media/thumbs/115_480.webp 480w, ../assets/media/115.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/116_480.webp" srcset="../assets/media/thumbs/116_480.webp 480w, ../assets/media/116.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="107" data-image-index="9" /></div></div>
      <div class="actions">
        <span>1 907 просмотров · 40 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/107" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/107.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="105" data-search="conservative rl и contextgnn: два подхода к рекомендациям с iclr 2025 iclr прошла, но неразобранные статьи о recsys остались. василий астахов, руководитель службы перспективных исследований и дизайна механизмов, отобрал пять работ на тему рекомендательных систем — части мы уже касались в подборках, сейчас хотим остановиться на них подробнее. looking into user’s long-term interests through the lens of conservative evidential learning статья на тему rl в рекомендациях. в ней рассматривают две постановки: классическую — где используют rl, чтобы лучше исследовать предпочтения пользователя во времени; нестандартную — когда rl помогает даже в оффлайн-задаче рекомендаций, без интерактивного взаимодействия. из более ранних работ мы знаем, что оффлайн-прогнозатор не всегда хорошо догадывается о полезных латентных признаках. rl, за счёт обучения с другой формулировкой, может «подсказать» модели, какие сигналы стоит запоминать — даже если они редкие. аналогичный эффект и в этой статье. хотя rl-сценарий тут продвинутый — авторы предлагают подход ecq-l (evidential conservative q-learning). в нём комбинируется несколько идей. evidential learning — вместо классического эксплорейшена, они учатся на уменьшение неопределённости. то есть выбирают айтемы, которые дают больше информации (не просто максимальный reward, а максимальное снижение неуверенности). conservative learning — модель не переоценивает редкие положительные примеры. если какая-то рекомендация «сработала», но по данным это было маловероятно, её вес занижается. это сделано, чтобы не переобучаться на случайные удачи. например, пользователь смотрел романтические фильмы, а вы случайно порекомендовали хоррор, и он понравился. ecq-l в этом случае не будет придавать слишком большого значения этому событию, потому что оно слабо объясняется историей. это и есть суть conservative-части подхода — модель целится не просто в reward, а в нижнюю границу его оценки, основанную на уверенности. архитектура довольно сложная: один модуль отвечает за обновление состояния; другой — за выбор действия (какой айтем рекомендовать), третий — за то, насколько выбранный айтем надёжен по текущему распределению. также используются разные типы лосса — отдельно на состояние, на действие, на оценку и на неопределённость. авторы показывают хорошие метрики — как в классическом rl-сценарии, так и в оффлайн-постановке. contextgnn: beyond two-tower recommendation systems главный посыл статьи: двухбашенные модели работают не очень хорошо, особенно когда есть плотные взаимодействия между пользователем и айтемом. авторы предлагают решение, которое работает в одну стадию, без двухфазного ранжирования, и при этом способно учитывать и известные, и новые айтемы. архитектура состоит из двух веток. первая работает с айтемами, с которыми пользователь уже взаимодействовал. используется графовая модель, которая обучается на конкретных связях пользователя и айтемов, учитывая категории, типы взаимодействий. это более «умная» часть, хорошо работающая в зонах, где есть история. вторая ветка — простая двухбашенная модель, работает с айтемами, которых пользователь пока не видел. здесь задача — пробовать предсказать интерес к новому, опираясь только на общую репрезентацию пользователя и айтема. ещё есть третий модуль, который учится предсказывать, чего хочет пользователь в данный момент. и на основе этой мотивации система решает, какую из двух моделей использовать сильнее, или как взвесить их выходы при финальном ранжировании. что показывают эксперименты: - если в датасете пользователь в основном «ходит по кругу», то выигрывает первая ветка — графовая. - если он часто пробует новое — вторая модель начинает давать вклад. - их основная модель всегда оказывается лучше, чем каждая по отдельности. в целом, это не радикально новая архитектура, но хорошее объединение знакомых подходов: модуль, который учится на известном (gnn); модуль, который работает с новым (two-tower); модуль-медиатор, который учится понимать, чего хочет пользователь сейчас. @recsyschannel обзор подготовил ❣ василий астахов #yaiclr conservative rl и contextgnn: два подхода к рекомендациям с iclr 2025 iclr прошла, но неразобранные статьи о recsys остались. василий астахов, руководитель службы перспективных исследований и дизайна механизмов, отобрал пять работ на тему рекомендательных систем — части мы уже касались в подборках, сейчас хотим остановиться на них подробнее. looking into user’s long-term interests through the lens of conservative evidential learning статья на тему rl в рекомендациях. в ней рассматривают две постановки: классическую — где используют rl, чтобы лучше исследовать предпочтения пользователя во времени; нестандартную — когда rl помогает даже в оффлайн-задаче рекомендаций, без интерактивного взаимодействия. из более ранних работ мы знаем, что оффлайн-прогнозатор не всегда хорошо догадывается о полезных латентных признаках. rl, за счёт обучения с другой формулировкой, может «подсказать» модели, какие сигналы стоит запоминать — даже если они редкие. аналогичный эффект и в этой статье. хотя rl-сценарий тут продвинутый — авторы предлагают подход ecq-l (evidential conservative q-learning). в нём комбинируется несколько идей. evidential learning — вместо классического эксплорейшена, они учатся на уменьшение неопределённости. то есть выбирают айтемы, которые дают больше информации (не просто максимальный reward, а максимальное снижение неуверенности). conservative learning — модель не переоценивает редкие положительные примеры. если какая-то рекомендация «сработала», но по данным это было маловероятно, её вес занижается. это сделано, чтобы не переобучаться на случайные удачи. например, пользователь смотрел романтические фильмы, а вы случайно порекомендовали хоррор, и он понравился. ecq-l в этом случае не будет придавать слишком большого значения этому событию, потому что оно слабо объясняется историей. это и есть суть conservative-части подхода — модель целится не просто в reward, а в нижнюю границу его оценки, основанную на уверенности. архитектура довольно сложная: один модуль отвечает за обновление состояния; другой — за выбор действия (какой айтем рекомендовать), третий — за то, насколько выбранный айтем надёжен по текущему распределению. также используются разные типы лосса — отдельно на состояние, на действие, на оценку и на неопределённость. авторы показывают хорошие метрики — как в классическом rl-сценарии, так и в оффлайн-постановке. contextgnn: beyond two-tower recommendation systems главный посыл статьи: двухбашенные модели работают не очень хорошо, особенно когда есть плотные взаимодействия между пользователем и айтемом. авторы предлагают решение, которое работает в одну стадию, без двухфазного ранжирования, и при этом способно учитывать и известные, и новые айтемы. архитектура состоит из двух веток. первая работает с айтемами, с которыми пользователь уже взаимодействовал. используется графовая модель, которая обучается на конкретных связях пользователя и айтемов, учитывая категории, типы взаимодействий. это более «умная» часть, хорошо работающая в зонах, где есть история. вторая ветка — простая двухбашенная модель, работает с айтемами, которых пользователь пока не видел. здесь задача — пробовать предсказать интерес к новому, опираясь только на общую репрезентацию пользователя и айтема. ещё есть третий модуль, который учится предсказывать, чего хочет пользователь в данный момент. и на основе этой мотивации система решает, какую из двух моделей использовать сильнее, или как взвесить их выходы при финальном ранжировании. что показывают эксперименты: - если в датасете пользователь в основном «ходит по кругу», то выигрывает первая ветка — графовая. - если он часто пробует новое — вторая модель начинает давать вклад. - их основная модель всегда оказывается лучше, чем каждая по отдельности. в целом, это не радикально новая архитектура, но хорошее объединение знакомых подходов: модуль, который учится на известном (gnn); модуль, который работает с новым (two-tower); модуль-медиатор, который учится понимать, чего хочет пользователь сейчас. @recsyschannel обзор подготовил ❣ василий астахов #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-17T08:07:01+00:00" href="./posts/105.html">2025-06-17 08:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Conservative RL и ContextGNN: два подхода к рекомендациям с ICLR 2025</strong><br><br>ICLR прошла, но неразобранные статьи о RecSys остались. Василий Астахов, руководитель службы перспективных исследований и дизайна механизмов, отобрал пять работ на тему рекомендательных систем — части мы уже касались в подборках, сейчас хотим остановиться на них подробнее.<br><br><a href="https://openreview.net/pdf?id=o99Yn1wN9J" rel="nofollow noopener noreferrer"><strong>Looking into User’s Long-term Interests through the Lens of Conservative Evidential Learning</strong></a><br><br>Статья на тему RL в рекомендациях. В ней рассматривают две постановки: классическую — где используют RL, чтобы лучше исследовать предпочтения пользователя во времени; нестандартную — когда RL помогает даже в оффлайн-задаче рекомендаций, без интерактивного взаимодействия.<br><br>Из более ранних работ мы знаем, что оффлайн-прогнозатор не всегда хорошо догадывается о полезных латентных признаках. RL, за счёт обучения с другой формулировкой, может «подсказать» модели, какие сигналы стоит запоминать — даже если они редкие.<br><br>Аналогичный эффект и в этой статье. Хотя RL-сценарий тут продвинутый — авторы предлагают подход ECQ-L (Evidential Conservative Q-Learning). В нём комбинируется несколько идей.<br><br>Evidential learning — вместо классического эксплорейшена, они учатся на уменьшение неопределённости. То есть выбирают айтемы, которые дают больше информации (не просто максимальный reward, а максимальное снижение неуверенности).<br><br>Conservative learning — модель не переоценивает редкие положительные примеры. Если какая-то рекомендация «сработала», но по данным это было маловероятно, её вес занижается. Это сделано, чтобы не переобучаться на случайные удачи. Например, пользователь смотрел романтические фильмы, а вы случайно порекомендовали хоррор, и он понравился. ECQ-L в этом случае не будет придавать слишком большого значения этому событию, потому что оно слабо объясняется историей.<br><br>Это и есть суть conservative-части подхода — модель целится не просто в reward, а в нижнюю границу его оценки, основанную на уверенности.<br><br>Архитектура довольно сложная: один модуль отвечает за обновление состояния; другой — за выбор действия (какой айтем рекомендовать), третий — за то, насколько выбранный айтем надёжен по текущему распределению.<br><br>Также используются разные типы лосса — отдельно на состояние, на действие, на оценку и на неопределённость.<br><br>Авторы показывают хорошие метрики — как в классическом RL-сценарии, так и в оффлайн-постановке.<br><br><a href="https://arxiv.org/abs/2411.19513" rel="nofollow noopener noreferrer"><strong>ContextGNN: Beyond Two-Tower Recommendation Systems</strong></a><br><br>Главный посыл статьи: двухбашенные модели работают не очень хорошо, особенно когда есть плотные взаимодействия между пользователем и айтемом. Авторы предлагают решение, которое работает в одну стадию, без двухфазного ранжирования, и при этом способно учитывать и известные, и новые айтемы.<br><br>Архитектура состоит из двух веток. Первая работает с айтемами, с которыми пользователь уже взаимодействовал. Используется графовая модель, которая обучается на конкретных связях пользователя и айтемов, учитывая категории, типы взаимодействий. Это более «умная» часть, хорошо работающая в зонах, где есть история.<br><br>Вторая ветка — простая двухбашенная модель, работает с айтемами, которых пользователь пока не видел. Здесь задача — пробовать предсказать интерес к новому, опираясь только на общую репрезентацию пользователя и айтема. Ещё есть третий модуль, который учится предсказывать, чего хочет пользователь в данный момент. И на основе этой мотивации система решает, какую из двух моделей использовать сильнее, или как взвесить их выходы при финальном ранжировании.<br><br>Что показывают эксперименты:<br><br>- Если в датасете пользователь в основном «ходит по кругу», то выигрывает первая ветка — графовая.<br>- Если он часто пробует новое — вторая модель начинает давать вклад.<br>- Их основная модель всегда оказывается лучше, чем каждая по отдельности.<br><br>В целом, это не радикально новая архитектура, но хорошее объединение знакомых подходов: модуль, который учится на известном (GNN); модуль, который работает с новым (two-tower); модуль-медиатор, который учится понимать, чего хочет пользователь сейчас.<br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Василий Астахов<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/105_480.webp" srcset="../assets/media/thumbs/105_480.webp 480w, ../assets/media/105.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="105" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/106_480.webp" srcset="../assets/media/thumbs/106_480.webp 480w, ../assets/media/106.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="105" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 848 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/105" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/105.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="104" data-search="language representations can be what recommenders need: findings and potentials разбираем одну из самых интересных статей на тему рекомендательных систем с iclr 2025. её авторы задаются вопросом: действительно ли llm неявно кодируют информацию о предпочтениях пользователей. в работе предлагается использовать эмбеддинги айтемов из llm для улучшения качества рекомендаций. в начале статьи упоминается, что llm хорошо показывают себя во многих доменах, а также приводится обзор вариантов их применения к ранжированию в рекомендательных системах. один из таких вариантов — использовать замороженную языковую модель для получения эмбеддинга текста/названия айтема, а затем — дообучать линейный слой для подсчёта итогового представления айтема. таким образом можно извлечь и представление пользователя, усреднив эмбеддинги айтемов из его истории взаимодействий. представления пользователей и айтемов затем используются для получения скора ранжирования (например, с помощью dot-product). этот подход немного улучшает качество существующих методов, и авторы задумываются о причинах этого улучшения. они отмечают, что фильмы на разные темы близки в пространстве языковых эмбеддов (например, запросы пользователей) и в пространстве непосредственно айтемов (например, названия фильмов). внимание акцентируют на линейном отображении, которое позволяет кластеризовать эмбеддинги айтемов, отображая схожесть пользователей, которые ими интересуются. в статье рассуждают о нескольких вариантах кодирования айтемов: с использованием id и матриц эмбеддов, с использованием llm. у первого подхода есть недостатки: например, плохая переносимость эмбеддингов между доменами и отсутствие явной возможности распознавать намерения пользователей. их и призван нивелировать второй подход. главные вопросы, на которые хотят ответить в статье: кодируют ли llm коллаборативный сигнал и насколько наличие сигнала зависит от размеров модели. сравниваясь с существующими методами на модельных датасетах, авторы приходят к выводам о превосходстве представлений, полученных с помощью llm, над id-based подходом. также утверждают, что с увеличением размера модели увеличивается репрезентативность пользовательских интересов. ключевая идея — итеративное построение эмбеддингов пользователей и айтемов по графу взаимодействий с использованием нелинейностей и представлений llm — в статье этот метод называется alpharec. для обучения моделей авторы предлагают использовать случайно сэмплированные негативы из числа тех айтемов, с которыми пользователи не взаимодействовали. на рассмотренных датасетах alpharec обходит существующие алгоритмы как по качеству, так и по необходимым вычислительным мощностям. ещё одно преимущество этого фреймворка — возможность предоставить готовые эмбеддинги для инициализации другими алгоритмами ранжирования. в конце статьи авторы рассматривают применение пользовательского интента (например, запрос с описанием фильма, который пользователь хотел бы потенциально посмотреть) для улучшения качества рекомендаций. использование alpharec в этом случае позволяет получить результаты, кратно превосходящие другие методы. однако датасет для такого исследования был сгенерирован синтетически с помощью асессоров и не защищен от ликов — то есть, скорее всего, он не означает, что в случае использования чат-бота или поискового запроса предложенный алгоритм будет настолько же хорош. @recsyschannel обзор подготовила ❣ маргарита мишустина #yaiclr language representations can be what recommenders need: findings and potentials разбираем одну из самых интересных статей на тему рекомендательных систем с iclr 2025. её авторы задаются вопросом: действительно ли llm неявно кодируют информацию о предпочтениях пользователей. в работе предлагается использовать эмбеддинги айтемов из llm для улучшения качества рекомендаций. в начале статьи упоминается, что llm хорошо показывают себя во многих доменах, а также приводится обзор вариантов их применения к ранжированию в рекомендательных системах. один из таких вариантов — использовать замороженную языковую модель для получения эмбеддинга текста/названия айтема, а затем — дообучать линейный слой для подсчёта итогового представления айтема. таким образом можно извлечь и представление пользователя, усреднив эмбеддинги айтемов из его истории взаимодействий. представления пользователей и айтемов затем используются для получения скора ранжирования (например, с помощью dot-product). этот подход немного улучшает качество существующих методов, и авторы задумываются о причинах этого улучшения. они отмечают, что фильмы на разные темы близки в пространстве языковых эмбеддов (например, запросы пользователей) и в пространстве непосредственно айтемов (например, названия фильмов). внимание акцентируют на линейном отображении, которое позволяет кластеризовать эмбеддинги айтемов, отображая схожесть пользователей, которые ими интересуются. в статье рассуждают о нескольких вариантах кодирования айтемов: с использованием id и матриц эмбеддов, с использованием llm. у первого подхода есть недостатки: например, плохая переносимость эмбеддингов между доменами и отсутствие явной возможности распознавать намерения пользователей. их и призван нивелировать второй подход. главные вопросы, на которые хотят ответить в статье: кодируют ли llm коллаборативный сигнал и насколько наличие сигнала зависит от размеров модели. сравниваясь с существующими методами на модельных датасетах, авторы приходят к выводам о превосходстве представлений, полученных с помощью llm, над id-based подходом. также утверждают, что с увеличением размера модели увеличивается репрезентативность пользовательских интересов. ключевая идея — итеративное построение эмбеддингов пользователей и айтемов по графу взаимодействий с использованием нелинейностей и представлений llm — в статье этот метод называется alpharec. для обучения моделей авторы предлагают использовать случайно сэмплированные негативы из числа тех айтемов, с которыми пользователи не взаимодействовали. на рассмотренных датасетах alpharec обходит существующие алгоритмы как по качеству, так и по необходимым вычислительным мощностям. ещё одно преимущество этого фреймворка — возможность предоставить готовые эмбеддинги для инициализации другими алгоритмами ранжирования. в конце статьи авторы рассматривают применение пользовательского интента (например, запрос с описанием фильма, который пользователь хотел бы потенциально посмотреть) для улучшения качества рекомендаций. использование alpharec в этом случае позволяет получить результаты, кратно превосходящие другие методы. однако датасет для такого исследования был сгенерирован синтетически с помощью асессоров и не защищен от ликов — то есть, скорее всего, он не означает, что в случае использования чат-бота или поискового запроса предложенный алгоритм будет настолько же хорош. @recsyschannel обзор подготовила ❣ маргарита мишустина #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-06T08:03:58+00:00" href="./posts/104.html">2025-06-06 08:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Language Representations Can be What Recommenders Need: Findings and Potentials</strong><br><br>Разбираем одну из самых <a href="https://arxiv.org/abs/2407.05441" rel="nofollow noopener noreferrer">интересных статей</a> на тему рекомендательных систем с ICLR 2025. Её авторы задаются вопросом: действительно ли LLM неявно кодируют информацию о предпочтениях пользователей. В работе предлагается использовать эмбеддинги айтемов из LLM для улучшения качества рекомендаций.<br><br>В начале статьи упоминается, что LLM хорошо показывают себя во многих доменах, а также приводится обзор вариантов их применения к ранжированию в рекомендательных системах. Один из таких вариантов — использовать замороженную языковую модель для получения эмбеддинга текста/названия айтема, а затем — дообучать линейный слой для подсчёта итогового представления айтема.<br><br>Таким образом можно извлечь и представление пользователя, усреднив эмбеддинги айтемов из его истории взаимодействий. Представления пользователей и айтемов затем используются для получения скора ранжирования (например, с помощью dot-product). <br><br>Этот подход немного улучшает качество существующих методов, и авторы задумываются о причинах этого улучшения. Они отмечают, что фильмы на разные темы близки в пространстве языковых эмбеддов (например, запросы пользователей) и в пространстве непосредственно айтемов (например, названия фильмов). Внимание акцентируют на линейном отображении, которое позволяет кластеризовать эмбеддинги айтемов, отображая схожесть пользователей, которые ими интересуются.<br><br>В статье рассуждают о нескольких вариантах кодирования айтемов: с использованием ID и матриц эмбеддов, с использованием LLM. У первого подхода есть недостатки: например, плохая переносимость эмбеддингов между доменами и отсутствие явной возможности распознавать намерения пользователей. Их и призван нивелировать второй подход.<br><br>Главные вопросы, на которые хотят ответить в статье: кодируют ли LLM коллаборативный сигнал и насколько наличие сигнала зависит от размеров модели. Сравниваясь с существующими методами на модельных датасетах, авторы приходят к выводам о превосходстве представлений, полученных с помощью LLM, над ID-based подходом. Также утверждают, что с увеличением размера модели увеличивается репрезентативность пользовательских интересов.<br><br>Ключевая идея — итеративное построение эмбеддингов пользователей и айтемов по графу взаимодействий с использованием нелинейностей и представлений LLM — в статье этот метод называется AlphaRec. Для обучения моделей авторы предлагают использовать случайно сэмплированные негативы из числа тех айтемов, с которыми пользователи не взаимодействовали. На рассмотренных датасетах AlphaRec обходит существующие алгоритмы как по качеству, так и по необходимым вычислительным мощностям. Ещё одно преимущество этого фреймворка — возможность предоставить готовые эмбеддинги для инициализации другими алгоритмами ранжирования.<br><br>В конце статьи авторы рассматривают применение пользовательского интента (например, запрос с описанием фильма, который пользователь хотел бы потенциально посмотреть) для улучшения качества рекомендаций. Использование AlphaRec в этом случае позволяет получить результаты, кратно превосходящие другие методы. Однако датасет для такого исследования был сгенерирован синтетически с помощью асессоров и не защищен от ликов — то есть, скорее всего, он не означает, что в случае использования чат-бота или поискового запроса предложенный алгоритм будет настолько же хорош.<br><br>@RecSysChannel<br>Обзор подготовила <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Маргарита Мишустина<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/104_480.webp" srcset="../assets/media/thumbs/104_480.webp 480w, ../assets/media/104.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="104" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 902 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/104" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/104.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="103" data-search="исследователи яндекса выложили в опенсорс yambda — датасет на 5 млрд событий в открытом доступе появился yandex music billion-interactions dataset (yambda) — один из крупнейших в мире датасетов в области рекомендательных систем. в этом посте рассказываем, зачем он нужен и какие у него ключевые особенности. в последние годы рекомендации вышли на плато по сравнению с более быстро развивающимся областями, такими как llm. исследователям недоступны терабайты данных, которые нужны для развития рекомендательных систем, а коммерческие платформы редко делятся данными. поэтому приходится использовать устаревшие и маленькие наборы. модели, обученные на таких данных, теряют эффективность при масштабировании. существующие доступные датасеты, такие как movielens, netflix prize dataset, amazon reviews, music4all-onion, steam и несколько других имеют ряд недостатков. например, сравнительно небольшой размер делает их нерепрезентативным для коммерческих масштабов, а фокус на явных сигналах ограничивает полезность для моделирования реальных последовательных взаимодействий. чтобы решить эти проблемы и дать исследователям больше возможностей для разработки и тестирования новых гипотез в рекомендациях, исследователи яндекса выложили в опенсорс свой датасет yambda. ключевые особенности yambda: — содержит 4,79 млрд обезличенных взаимодействий пользователей с музыкальными треками в яндекс музыке. — есть три версии: полная (5 млрд событий) и уменьшенные (500 млн и 50 млн событий). — включает два основных типа взаимодействий: неявную обратную связь (прослушивания) и явную обратную связь (лайки, дизлайки, анлайки и андизлайки). — для большинства треков есть нейросетевые вектора, сгенерированные с помощью свёрточной нейронной сети (cnn), что позволяет учитывать некоторые характеристики музыкальных треков. — включены анонимизированные признаки метаданных треков, такие как длительность, содержание вложений, исполнитель и альбом. — каждое событие помечено флагом is_organic, который позволяет различать органические действия пользователей и действия, вызванные рекомендациями алгоритма. — все события имеют временные метки, что позволяет проводить анализ временных последовательностей и оценивать алгоритмы в условиях, приближённых к реальным. — данные распределены в формате apache parquet, что обеспечивает совместимость с распределёнными системами обработки данных (например, hadoop, spark) и современными аналитическими инструментами (например, polars, pandas). методы оценки в отличие от метода leave-one-out (loo), который исключает последнее положительное взаимодействие пользователя из обучающей выборки для предсказания, yambda-5b использует глобальный временной сплит (global temporal split, gts). преимущество gts в том, что он сохраняет временную последовательность событий, предотвращая нарушение временных зависимостей между тренировочным и тестовым наборами данных. это позволяет более точно оценить, как модель будет работать в реальных условиях, когда доступ к будущим данным ограничен или невозможен. вместе с датасетом представлены baseline-алгоритмы (mostpop, decaypop, itemknn, ials, bpr, sansa, sasrec). они служат отправной точкой для сравнения эффективности новых подходов в области рекомендательных систем. используются следующие метрики: — ndcg@k (normalized discounted cumulative gain) — оценивает качество ранжирования рекомендаций. — recall@k — измеряет способность алгоритма генерировать релевантные рекомендации из общего набора возможных рекомендаций. — coverage@k — показывает, насколько широко представлен каталог элементов в рекомендации. датасет и код для оценочных бейзлайнов уже доступны на hugging face, а статья — на arxiv. upd: появилось видео выступления саши плошкина на acm recsys — оставим его здесь для истории 💫 статью подготовили ❣ александр плошкин, владислав тыцкий, алексей письменный, владимир байкалов, евгений тайчинов, артём пермяков, даниил бурлаков, евгений крофто, николай савушкин @recsyschannel исследователи яндекса выложили в опенсорс yambda — датасет на 5 млрд событий в открытом доступе появился yandex music billion-interactions dataset (yambda) — один из крупнейших в мире датасетов в области рекомендательных систем. в этом посте рассказываем, зачем он нужен и какие у него ключевые особенности. в последние годы рекомендации вышли на плато по сравнению с более быстро развивающимся областями, такими как llm. исследователям недоступны терабайты данных, которые нужны для развития рекомендательных систем, а коммерческие платформы редко делятся данными. поэтому приходится использовать устаревшие и маленькие наборы. модели, обученные на таких данных, теряют эффективность при масштабировании. существующие доступные датасеты, такие как movielens, netflix prize dataset, amazon reviews, music4all-onion, steam и несколько других имеют ряд недостатков. например, сравнительно небольшой размер делает их нерепрезентативным для коммерческих масштабов, а фокус на явных сигналах ограничивает полезность для моделирования реальных последовательных взаимодействий. чтобы решить эти проблемы и дать исследователям больше возможностей для разработки и тестирования новых гипотез в рекомендациях, исследователи яндекса выложили в опенсорс свой датасет yambda. ключевые особенности yambda: — содержит 4,79 млрд обезличенных взаимодействий пользователей с музыкальными треками в яндекс музыке. — есть три версии: полная (5 млрд событий) и уменьшенные (500 млн и 50 млн событий). — включает два основных типа взаимодействий: неявную обратную связь (прослушивания) и явную обратную связь (лайки, дизлайки, анлайки и андизлайки). — для большинства треков есть нейросетевые вектора, сгенерированные с помощью свёрточной нейронной сети (cnn), что позволяет учитывать некоторые характеристики музыкальных треков. — включены анонимизированные признаки метаданных треков, такие как длительность, содержание вложений, исполнитель и альбом. — каждое событие помечено флагом is_organic, который позволяет различать органические действия пользователей и действия, вызванные рекомендациями алгоритма. — все события имеют временные метки, что позволяет проводить анализ временных последовательностей и оценивать алгоритмы в условиях, приближённых к реальным. — данные распределены в формате apache parquet, что обеспечивает совместимость с распределёнными системами обработки данных (например, hadoop, spark) и современными аналитическими инструментами (например, polars, pandas). методы оценки в отличие от метода leave-one-out (loo), который исключает последнее положительное взаимодействие пользователя из обучающей выборки для предсказания, yambda-5b использует глобальный временной сплит (global temporal split, gts). преимущество gts в том, что он сохраняет временную последовательность событий, предотвращая нарушение временных зависимостей между тренировочным и тестовым наборами данных. это позволяет более точно оценить, как модель будет работать в реальных условиях, когда доступ к будущим данным ограничен или невозможен. вместе с датасетом представлены baseline-алгоритмы (mostpop, decaypop, itemknn, ials, bpr, sansa, sasrec). они служат отправной точкой для сравнения эффективности новых подходов в области рекомендательных систем. используются следующие метрики: — ndcg@k (normalized discounted cumulative gain) — оценивает качество ранжирования рекомендаций. — recall@k — измеряет способность алгоритма генерировать релевантные рекомендации из общего набора возможных рекомендаций. — coverage@k — показывает, насколько широко представлен каталог элементов в рекомендации. датасет и код для оценочных бейзлайнов уже доступны на hugging face , а статья — на arxiv . upd: появилось видео выступления саши плошкина на acm recsys — оставим его здесь для истории 💫 статью подготовили ❣ александр плошкин, владислав тыцкий, алексей письменный, владимир байкалов, евгений тайчинов, артём пермяков, даниил бурлаков, евгений крофто, николай савушкин @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-29T08:08:40+00:00" href="./posts/103.html">2025-05-29 08:08 UTC</a></div>
      </div>
      <div class="post-body"><strong>Исследователи Яндекса выложили в опенсорс Yambda — датасет на 5 млрд событий</strong><br><br>В открытом доступе <a href="https://huggingface.co/datasets/yandex/yambda" rel="nofollow noopener noreferrer">появился</a> Yandex Music Billion-Interactions Dataset (Yambda) — один из крупнейших в мире датасетов в области рекомендательных систем. В этом посте рассказываем, зачем он нужен и какие у него ключевые особенности. <br><br>В последние годы рекомендации вышли на плато по сравнению с более быстро развивающимся областями, такими как LLM. Исследователям недоступны терабайты данных, которые нужны для развития рекомендательных систем, а коммерческие платформы редко делятся данными. Поэтому приходится использовать устаревшие и маленькие наборы. Модели, обученные на таких данных, теряют эффективность при масштабировании.<br><br>Существующие доступные датасеты, такие как MovieLens, Netflix Prize dataset, Amazon Reviews, Music4All-Onion, Steam и несколько других имеют ряд недостатков. Например, сравнительно небольшой размер делает их нерепрезентативным для коммерческих масштабов, а фокус на явных сигналах ограничивает полезность для моделирования реальных последовательных взаимодействий.<br><br>Чтобы решить эти проблемы и дать исследователям больше возможностей для разработки и тестирования новых гипотез в рекомендациях, исследователи  Яндекса выложили в опенсорс свой датасет Yambda. <br><br><strong>Ключевые особенности Yambda: </strong><br><br>— Содержит 4,79 млрд обезличенных взаимодействий пользователей с музыкальными треками в Яндекс Музыке.<br>— Есть три версии: полная (5 млрд событий) и уменьшенные (500 млн и 50 млн<br>событий).<br>— Включает два основных типа взаимодействий: неявную обратную связь (прослушивания) и явную обратную связь (лайки, дизлайки, анлайки и андизлайки).<br>— Для большинства треков есть нейросетевые вектора, сгенерированные с помощью свёрточной нейронной сети (CNN), что позволяет учитывать некоторые характеристики музыкальных треков.<br>— Включены анонимизированные признаки метаданных треков, такие как длительность, содержание вложений, исполнитель и альбом.<br>— Каждое событие помечено флагом is_organic, который позволяет различать органические действия пользователей и действия, вызванные рекомендациями алгоритма.<br>— Все события имеют временные метки, что позволяет проводить анализ временных последовательностей и оценивать алгоритмы в условиях, приближённых к реальным.<br>— Данные распределены в формате Apache Parquet, что обеспечивает совместимость с распределёнными системами обработки данных (например, Hadoop, Spark) и современными аналитическими инструментами (например, Polars, Pandas).<br><br><strong>Методы оценки</strong><br><br>В отличие от метода Leave-One-Out (LOO), который исключает последнее положительное взаимодействие пользователя из обучающей выборки для предсказания, Yambda-5B использует глобальный временной сплит (Global Temporal Split, GTS). Преимущество GTS в том, что он сохраняет временную последовательность событий, предотвращая нарушение временных зависимостей между тренировочным и тестовым наборами данных. Это позволяет более точно оценить, как модель будет работать в реальных условиях, когда доступ к будущим данным ограничен или невозможен.<br><br>Вместе с датасетом представлены baseline-алгоритмы (MostPop, DecayPop, ItemKNN, iALS, BPR, SANSA, SASRec). Они служат отправной точкой для сравнения эффективности новых подходов в области рекомендательных систем.<br><br>Используются следующие метрики:<br><br>— NDCG@k (Normalized Discounted Cumulative Gain) — оценивает качество ранжирования рекомендаций.<br>— Recall@k — измеряет способность алгоритма генерировать релевантные рекомендации из общего набора возможных рекомендаций.<br>— Coverage@k — показывает, насколько широко представлен каталог элементов в рекомендации.<br><br>Датасет и код для оценочных бейзлайнов уже доступны на <a href="https://huggingface.co/datasets/yandex/yambda" rel="nofollow noopener noreferrer">Hugging Face</a>, а статья — <a href="https://arxiv.org/abs/2505.22238" rel="nofollow noopener noreferrer">на arXiv</a>. <br><br><strong><em>UPD: Появилось </em></strong><a href="https://www.youtube.com/watch?v=hMYdT9fEZUY" rel="nofollow noopener noreferrer"><strong><em>видео выступления</em></strong></a><strong><em> Саши Плошкина на ACM RecSys — оставим его</em> <em>здесь для истории </em></strong><strong><em><tg-emoji emoji-id="5341774057236876922">💫</tg-emoji></em></strong><br><br>Статью подготовили <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> </em>Александр Плошкин, Владислав Тыцкий, Алексей Письменный, Владимир Байкалов, Евгений Тайчинов, Артём Пермяков, Даниил Бурлаков, Евгений Крофто, Николай Савушкин<br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/103_480.webp" srcset="../assets/media/thumbs/103_480.webp 480w, ../assets/media/103.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="103" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 099 просмотров · 52 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/103" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/103.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="102" data-search="что делают в мире: llm &amp; recsys. часть 2/2 в одном из предыдущих постов мы обсуждали, что llm оказывает значительное влияние на recsys, и это проявляется не только в переносе архитектурных решений, но и в непосредственном применении языковых моделей. сегодня обсудим несколько примечательных статей, вышедших в последние месяцы и демонстрирующих, как применение языковых моделей способно улучшать качество рекомендаций. real-time ad retrieval via llm-generative commercial intention for sponsored search advertisin статья от tencent, в которой llm используются для кандидатогенерации в рекламе. исследования последних лет показывали эффективность llm в этом направлении, но подходы сводились к следующему: в офлайне строятся индексы документов, а в онлайне на основе запроса llm генерирует подходящий индекс. такой подход концептуально неплох, но имеет ряд недостатков с точки зрения как качества, так и эффективности инференса. tencenet же делают следующее: в офлайне с помощью llm генерируют «коммерческие предложения» (ci) для рекламного корпуса, строят динамический индекс формата {ci: рекламные объявления} так, что одному ci ставится в соответствие сразу пачка объявлений. в онлайне же — отдельной затюненной llm генерирует ci для запроса и по соответствующему ci достаёт объявления-кандидаты из офлайн-хранилища. такой формат хранения ключей позволяет значительно лучше утилизировать способности llm к обработке, внезапно, естественного языка и отлично себя показывает на онлайн-метриках: на различных поверхностях прирост gmv составил от 5,02% до 6,37%. the blessing of reasoning: llm-based contrastive explanations in black-box recommender systems гигантская статья с участием небезызвестной minmin chen. представим, что нам удалось построить хорошую рекомендательную модель, которая прекрасно работает в продакшене. но как для самих исследователей, так и для внешнего мира (и внутренних заказчиков) может быть интересно и важно, почему система приняла именно такое решение. это банально интересно, позволяет лучше понять аудиторию, да и просто это отличные вводные для улучшений в будущем. к сожалению, ответить на вопрос почему крайне сложно, особенно если модель нейросетевая — сложность архитектуры просто не позволяет связать входы и выходы модели и составить понятную интерпретацию. но буквально по соседству продолжается бурное развитие llm — с глубоким знанием о мире и потрясающей способностью к рассуждению. мы можем делегировать reasoning-моделям задачу «понимания» пользователя и поиск наиболее важных точек соприкосновения между ним и товаром-кандидатом, чтобы получить обоснование релевантности. авторы показывают, что помимо хорошей объясняющей способности, добавление в рекомендательную систему знания llm о мире также позволяет добиться лучшего качества на публичных датасетах. llm-alignment live-streaming recommendation статья от kuaishou, где авторы пытаются объединить recsys, llm, мультимодальность — и всё это упаковать в реалтаймовый сценарий. сначала происходит подготовка языковой модели для стриминга: используют 100b модель для разметки 30-секундных видеофрагметов, на основе которых тюнят 7b-модель для быстрого инференса, чтобы в реалтайме строить высокоинформативные эмбеддинги, которые далее передаются рекомендательной модели. llm-эмбеддинги выравнивают с рекомендательными id-based-эмбеддингами с помощью отдельного гейтинг-механизма, чтобы получить итоговое представление, связывающее рекомендательный сигнал автора, пользователя и llm-знание о происходящем на стриме. полученный единый эмбеддинг переводят в semantic id и используют в итоговой модели ранжирования. a/b-эксперимент показал рост времени просмотра на двух стриминговых платформах на 0,07% и 0,17%, число лайков на 2,5% и 2,8% соответственно на стадии ранжирования. при этом особенно сильный рост числа показов наблюдается для контент-мейкеров с хвоста распределения, с числом подписчиков до порядка 100~1000. @recsyschannel обзор подготовил ❣ руслан кулиев что делают в мире: llm &amp;amp; recsys. часть 2/2 в одном из предыдущих постов мы обсуждали, что llm оказывает значительное влияние на recsys, и это проявляется не только в переносе архитектурных решений, но и в непосредственном применении языковых моделей. сегодня обсудим несколько примечательных статей, вышедших в последние месяцы и демонстрирующих, как применение языковых моделей способно улучшать качество рекомендаций. real-time ad retrieval via llm-generative commercial intention for sponsored search advertisin статья от tencent, в которой llm используются для кандидатогенерации в рекламе. исследования последних лет показывали эффективность llm в этом направлении, но подходы сводились к следующему: в офлайне строятся индексы документов, а в онлайне на основе запроса llm генерирует подходящий индекс. такой подход концептуально неплох, но имеет ряд недостатков с точки зрения как качества, так и эффективности инференса. tencenet же делают следующее: в офлайне с помощью llm генерируют «коммерческие предложения» (ci) для рекламного корпуса, строят динамический индекс формата {ci: рекламные объявления} так, что одному ci ставится в соответствие сразу пачка объявлений. в онлайне же — отдельной затюненной llm генерирует ci для запроса и по соответствующему ci достаёт объявления-кандидаты из офлайн-хранилища. такой формат хранения ключей позволяет значительно лучше утилизировать способности llm к обработке, внезапно, естественного языка и отлично себя показывает на онлайн-метриках: на различных поверхностях прирост gmv составил от 5,02% до 6,37%. the blessing of reasoning: llm-based contrastive explanations in black-box recommender systems гигантская статья с участием небезызвестной minmin chen. представим, что нам удалось построить хорошую рекомендательную модель, которая прекрасно работает в продакшене. но как для самих исследователей, так и для внешнего мира (и внутренних заказчиков) может быть интересно и важно, почему система приняла именно такое решение. это банально интересно, позволяет лучше понять аудиторию, да и просто это отличные вводные для улучшений в будущем. к сожалению, ответить на вопрос почему крайне сложно, особенно если модель нейросетевая — сложность архитектуры просто не позволяет связать входы и выходы модели и составить понятную интерпретацию. но буквально по соседству продолжается бурное развитие llm — с глубоким знанием о мире и потрясающей способностью к рассуждению. мы можем делегировать reasoning-моделям задачу «понимания» пользователя и поиск наиболее важных точек соприкосновения между ним и товаром-кандидатом, чтобы получить обоснование релевантности. авторы показывают, что помимо хорошей объясняющей способности, добавление в рекомендательную систему знания llm о мире также позволяет добиться лучшего качества на публичных датасетах. llm-alignment live-streaming recommendation статья от kuaishou, где авторы пытаются объединить recsys, llm, мультимодальность — и всё это упаковать в реалтаймовый сценарий. сначала происходит подготовка языковой модели для стриминга: используют 100b модель для разметки 30-секундных видеофрагметов, на основе которых тюнят 7b-модель для быстрого инференса, чтобы в реалтайме строить высокоинформативные эмбеддинги, которые далее передаются рекомендательной модели. llm-эмбеддинги выравнивают с рекомендательными id-based-эмбеддингами с помощью отдельного гейтинг-механизма, чтобы получить итоговое представление, связывающее рекомендательный сигнал автора, пользователя и llm-знание о происходящем на стриме. полученный единый эмбеддинг переводят в semantic id и используют в итоговой модели ранжирования. a/b-эксперимент показал рост времени просмотра на двух стриминговых платформах на 0,07% и 0,17%, число лайков на 2,5% и 2,8% соответственно на стадии ранжирования. при этом особенно сильный рост числа показов наблюдается для контент-мейкеров с хвоста распределения, с числом подписчиков до порядка 100~1000. @recsyschannel обзор подготовил ❣ руслан кулиев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-22T08:35:26+00:00" href="./posts/102.html">2025-05-22 08:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что делают в мире: LLM &amp; RecSys. Часть 2/2<br></strong><br><a href="https://t.me/RecSysChannel/81" rel="nofollow noopener noreferrer">В одном из предыдущих постов</a> мы обсуждали, что LLM оказывает значительное влияние на RecSys, и это проявляется не только в переносе архитектурных решений, но и в непосредственном применении языковых моделей. Сегодня обсудим несколько примечательных статей, вышедших в последние месяцы и демонстрирующих, как применение языковых моделей способно улучшать качество рекомендаций.<br><br><a href="https://arxiv.org/pdf/2504.01304" rel="nofollow noopener noreferrer"><strong>Real-time Ad retrieval via LLM-generative Commercial Intention for Sponsored Search Advertisin</strong></a><br><br>Статья от Tencent, в которой LLM используются для кандидатогенерации в рекламе. Исследования последних лет показывали эффективность LLM в этом направлении, но подходы сводились к следующему: в офлайне строятся индексы документов, а в онлайне на основе запроса LLM генерирует подходящий индекс. Такой подход концептуально неплох, но имеет ряд недостатков с точки зрения как качества, так и эффективности инференса. Tencenet же делают следующее: в офлайне с помощью LLM генерируют «коммерческие предложения» (CI) для рекламного корпуса, строят динамический индекс формата {CI: Рекламные объявления} так, что одному CI ставится в соответствие сразу пачка объявлений. В онлайне же — отдельной затюненной LLM генерирует CI для запроса и по соответствующему CI достаёт объявления-кандидаты из офлайн-хранилища. Такой формат хранения ключей позволяет значительно лучше утилизировать способности LLM к обработке, внезапно, естественного языка и отлично себя показывает на онлайн-метриках: на различных поверхностях прирост GMV составил от 5,02% до 6,37%.<br><br><a href="https://arxiv.org/abs/2502.16759v1" rel="nofollow noopener noreferrer"><strong>The Blessing of Reasoning: LLM-Based Contrastive Explanations in Black-Box Recommender Systems</strong></a> <br><br>Гигантская статья с участием небезызвестной Minmin Chen. Представим, что нам удалось построить хорошую рекомендательную модель, которая прекрасно работает в продакшене. Но как для самих исследователей, так и для внешнего мира (и внутренних заказчиков) может быть интересно и важно, почему система приняла именно такое решение. Это банально интересно, позволяет лучше понять аудиторию, да и просто это отличные вводные для улучшений в будущем. К сожалению, ответить на вопрос почему крайне сложно, особенно если модель нейросетевая — сложность архитектуры просто не позволяет связать входы и выходы модели и составить понятную интерпретацию. Но буквально по соседству продолжается бурное развитие LLM — с глубоким знанием о мире и потрясающей способностью к рассуждению. Мы можем делегировать reasoning-моделям задачу «понимания» пользователя и поиск наиболее важных точек соприкосновения между ним и товаром-кандидатом, чтобы получить обоснование релевантности. Авторы показывают, что помимо хорошей объясняющей способности, добавление в рекомендательную систему знания LLM о мире также позволяет добиться лучшего качества на публичных датасетах.<br><br><a href="https://arxiv.org/abs/2504.05217" rel="nofollow noopener noreferrer"><strong>LLM-Alignment Live-Streaming Recommendation</strong></a><br><br>Статья от Kuaishou, где авторы пытаются объединить RecSys, LLM, мультимодальность — и всё это упаковать в реалтаймовый сценарий. Сначала происходит подготовка языковой модели для стриминга: используют 100B модель для разметки 30-секундных видеофрагметов, на основе которых тюнят 7B-модель для быстрого инференса, чтобы в реалтайме строить высокоинформативные эмбеддинги, которые далее передаются рекомендательной модели. LLM-эмбеддинги выравнивают с рекомендательными id-based-эмбеддингами с помощью отдельного гейтинг-механизма, чтобы получить итоговое представление, связывающее рекомендательный сигнал автора, пользователя и LLM-знание о происходящем на стриме. Полученный единый эмбеддинг переводят в Semantic ID и используют в итоговой модели ранжирования. A/B-эксперимент показал рост времени просмотра на двух стриминговых платформах на 0,07% и 0,17%, число лайков на 2,5% и 2,8% соответственно на стадии ранжирования. При этом особенно сильный рост числа показов наблюдается для контент-мейкеров с хвоста распределения, с числом подписчиков до порядка 100~1000.<br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Руслан Кулиев</div>
      <div class="actions">
        <span>2 206 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/102" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/102.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="101" data-search="unified embedding: battle-tested feature representations for web-scale ml systems сейчас в recsys много говорят о семантических id для кодирования айтемов. у нас в «рекомендательной» уже были материалы о разных алгоритмах с этой техникой: • recommender systems with generative retrieval • from features to transformers: redefining ranking for scalable impact • onerec: unifying retrieve and rank with generative recommender and iterative preference alignment • learnable item tokenization for generative recommendation но также вспомним классические методы кодирования айтемов. один из популярных и очень мощных на практике — multisize unified embeddings. он предоставляет способ кодирования набора категориальных признаков произвольной кардинальности в единый вектор. как это работает. пусть дан набор айтемов d={x₁,…,x_n}. каждый из них описан t категориальными признаками x = [v₁,…,v_t], где vₜ∈vₜ. классические подходы к кодированию 1. collisionless (без коллизий) – для каждого признака t и каждого его значения vₜ хранится отдельный вектор-эмбеддинг. – плюс: нет коллизий. – минус: память растёт пропорционально сумме кардинальностей всех vₜ. 2. hash table (feature hashing) – каждое значение одного признака хешируется в таблицу размера m. – плюс: фиксированный объём памяти, независимо от числа значений. – минус: внутрипризнаковые коллизии искажают градиенты и ухудшают качество. подход unified table авторы предлагают объединить всё в одну общую хеш-таблицу размера m, но использовать для каждого признака свою хеш-функцию hₜ: – плюс: только одна таблица, всего два гиперпараметра (m и параметры хеша). – минус: появляются межпризнаковые коллизии, когда значения разных признаков попадают в один и тот же бакет. полученные эмбеддинги всех признаков конкатенируют, а затем подают в последующую нейросеть. теоретический анализ из статьи показывает: – межпризнаковые коллизии (случай t≠s, когда hₜ(v) и hₛ(u) уходят в один бакет) нейтрализуются последующей нейросетью: для каждой группы модель учит почти ортогональные проекции. такие коллизии не влияют на качество. – внутрипризнаковые коллизии (разные v₁,v₂∈vₜ хешируются в один бакет) создают устойчивое смещение градиента и ухудшают качество решаемой задачи. улучшение: multisize unified table для каждого признака t вместо одного хеша используют сразу k независимых хеш-функций hₜ¹…hₜᵏ→[1…m]. – «плохие» внутрипризнаковые коллизии почти исчезают; – объём памяти остаётся таким же, как в unified table. итог multisize unified embeddings дают качество, сопоставимое с отдельными таблицами эмбеддингов, но требуют в разы меньше памяти и отлично масштабируются на web-scale. @recsyschannel разбор подготовил ❣ артём матвеев unified embedding: battle-tested feature representations for web-scale ml systems сейчас в recsys много говорят о семантических id для кодирования айтемов. у нас в «рекомендательной» уже были материалы о разных алгоритмах с этой техникой: • recommender systems with generative retrieval • from features to transformers: redefining ranking for scalable impact • onerec: unifying retrieve and rank with generative recommender and iterative preference alignment • learnable item tokenization for generative recommendation но также вспомним классические методы кодирования айтемов. один из популярных и очень мощных на практике — multisize unified embeddings . он предоставляет способ кодирования набора категориальных признаков произвольной кардинальности в единый вектор. как это работает. пусть дан набор айтемов d={x₁,…,x_n}. каждый из них описан t категориальными признаками x = [v₁,…,v_t], где vₜ∈vₜ. классические подходы к кодированию 1. collisionless (без коллизий) – для каждого признака t и каждого его значения vₜ хранится отдельный вектор-эмбеддинг. – плюс: нет коллизий. – минус: память растёт пропорционально сумме кардинальностей всех vₜ. 2. hash table (feature hashing) – каждое значение одного признака хешируется в таблицу размера m. – плюс: фиксированный объём памяти, независимо от числа значений. – минус: внутрипризнаковые коллизии искажают градиенты и ухудшают качество. подход unified table авторы предлагают объединить всё в одну общую хеш-таблицу размера m, но использовать для каждого признака свою хеш-функцию hₜ: – плюс: только одна таблица, всего два гиперпараметра (m и параметры хеша). – минус: появляются межпризнаковые коллизии, когда значения разных признаков попадают в один и тот же бакет. полученные эмбеддинги всех признаков конкатенируют, а затем подают в последующую нейросеть. теоретический анализ из статьи показывает: – межпризнаковые коллизии (случай t≠s, когда hₜ(v) и hₛ(u) уходят в один бакет) нейтрализуются последующей нейросетью: для каждой группы модель учит почти ортогональные проекции. такие коллизии не влияют на качество. – внутрипризнаковые коллизии (разные v₁,v₂∈vₜ хешируются в один бакет) создают устойчивое смещение градиента и ухудшают качество решаемой задачи. улучшение: multisize unified table для каждого признака t вместо одного хеша используют сразу k независимых хеш-функций hₜ¹…hₜᵏ→[1…m]. – «плохие» внутрипризнаковые коллизии почти исчезают; – объём памяти остаётся таким же, как в unified table. итог multisize unified embeddings дают качество, сопоставимое с отдельными таблицами эмбеддингов, но требуют в разы меньше памяти и отлично масштабируются на web-scale. @recsyschannel разбор подготовил ❣ артём матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-14T08:03:31+00:00" href="./posts/101.html">2025-05-14 08:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems</strong><br><br>Сейчас в RecSys много говорят о семантических ID для кодирования айтемов. У нас в «Рекомендательной» уже были материалы о разных алгоритмах с этой техникой:<br><br>• <a href="https://t.me/RecSysChannel/52" rel="nofollow noopener noreferrer">Recommender Systems with Generative Retrieval<br></a>• <a href="https://t.me/RecSysChannel/76" rel="nofollow noopener noreferrer">From Features to Transformers: Redefining Ranking for Scalable Impact</a><br>• <a href="https://t.me/RecSysChannel/79" rel="nofollow noopener noreferrer">OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment</a><br>• <a href="https://t.me/RecSysChannel/100" rel="nofollow noopener noreferrer">Learnable Item Tokenization for Generative Recommendation</a><br><br>Но также вспомним классические методы кодирования айтемов. Один из популярных и очень мощных на практике — <a href="https://arxiv.org/abs/2305.12102" rel="nofollow noopener noreferrer">Multisize Unified Embeddings</a>. Он предоставляет способ кодирования набора категориальных признаков произвольной кардинальности в единый вектор.<br><br>Как это работает. <br><br>Пусть дан набор айтемов D={x₁,…,x_N}. Каждый из них описан T категориальными признаками x = [v₁,…,v_T], где vₜ∈Vₜ. <br><br><strong>Классические подходы к кодированию</strong><br><br>1. Collisionless (без коллизий)  <br>– Для каждого признака t и каждого его значения vₜ хранится отдельный вектор-эмбеддинг.  <br>– Плюс: нет коллизий.  <br>– Минус: память растёт пропорционально сумме кардинальностей всех Vₜ.<br><br>2. Hash Table (feature hashing)  <br>– Каждое значение одного признака хешируется в таблицу размера M.  <br>– Плюс: фиксированный объём памяти, независимо от числа значений.  <br>– Минус: внутрипризнаковые коллизии искажают градиенты и ухудшают качество.<br><br><strong>Подход Unified Table  <br></strong><br>Авторы предлагают объединить всё в одну общую хеш-таблицу размера M, но использовать для каждого признака свою хеш-функцию hₜ:  <br>– Плюс: только одна таблица, всего два гиперпараметра (M и параметры хеша).  <br>– Минус: появляются межпризнаковые коллизии, когда значения разных признаков попадают в один и тот же бакет.<br><br>Полученные эмбеддинги всех признаков конкатенируют, а затем подают в последующую нейросеть.<br><br>Теоретический анализ из статьи показывает:<br>– Межпризнаковые коллизии (случай t≠s, когда hₜ(v) и hₛ(u) уходят в один бакет) нейтрализуются последующей нейросетью: для каждой группы модель учит почти ортогональные проекции. Такие коллизии не влияют на качество.<br>– Внутрипризнаковые коллизии (разные v₁,v₂∈Vₜ хешируются в один бакет) создают устойчивое смещение градиента и ухудшают качество решаемой задачи.<br><br><strong>Улучшение: Multisize Unified Table  </strong><br><br>Для каждого признака t вместо одного хеша используют сразу k независимых хеш-функций hₜ¹…hₜᵏ→[1…M].  <br>– «Плохие» внутрипризнаковые коллизии почти исчезают;  <br>– Объём памяти остаётся таким же, как в Unified Table.<br><br><strong>Итог  </strong><br><br>Multisize Unified Embeddings дают качество, сопоставимое с отдельными таблицами эмбеддингов, но требуют в разы меньше памяти и отлично масштабируются на web-scale.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji>  Артём Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/101_480.webp" srcset="../assets/media/thumbs/101_480.webp 480w, ../assets/media/101.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="101" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 094 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/101" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/101.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="100" data-search="learnable item tokenization for generative recommendation тренд на семантические id развивается уже более полугода. начало положила статья tiger, в которой для генеративного ретривала контентные эмбеддинги айтемов квантовали с помощью rq-vae. статья вышла ещё в 2023 году, но популярность к подходу начала приходить только после конференции recsys в 2024-м. в сегодняшней статье авторы предлагают модификацию алгоритма квантизации — learnable item tokenization for generative recommendation (letter). новый подход основан на трёх идеях: 1. сохранение иерархичности семантической квантизации; 2. контрастивное сближение квантизаций и коллаборативных эмбеддингов (полученных через предобученный sasrec или lightgcn); 3. сглаживание распределений айтемов по центроидам кодбуков. еще одно отличие от tiger — для того чтобы генерировать валидные коды, используются префиксные деревья по аналогии со статьей how to index item ids for recommendation foundation models. отдельное спасибо авторам хочется выразить за подробный ablation study числа кодов в иерархии квантизации: они отмечают, что увеличение числа кодов не всегда улучшает работу модели из-за накопления ошибки при авторегрессивном инференсе без teacher forcing. очень полезны и данные о числах эмбеддингов в кодбуках. несмотря на большой вклад статьи в развитие семантической квантизации, у этой техники всё ещё остаются нерешенные проблемы. для его реализации нужны: 1. предобученная контентная модель (в их случае это llama-7b); 2. предобученная коллаборативная модель (например, sasrec или lightgcn); 3. другой подход к экспериментам — сейчас они, как правило, проводятся на открытых датасетах без time-split, из-за этого применимость метода в индустрии пока под вопросом. @recsyschannel разбор подготовил ❣ сергей макеев learnable item tokenization for generative recommendation тренд на семантические id развивается уже более полугода. начало положила статья tiger , в которой для генеративного ретривала контентные эмбеддинги айтемов квантовали с помощью rq-vae. статья вышла ещё в 2023 году, но популярность к подходу начала приходить только после конференции recsys в 2024-м. в сегодняшней статье авторы предлагают модификацию алгоритма квантизации — learnable item tokenization for generative recommendation (letter). новый подход основан на трёх идеях: 1. сохранение иерархичности семантической квантизации; 2. контрастивное сближение квантизаций и коллаборативных эмбеддингов (полученных через предобученный sasrec или lightgcn); 3. сглаживание распределений айтемов по центроидам кодбуков. еще одно отличие от tiger — для того чтобы генерировать валидные коды, используются префиксные деревья по аналогии со статьей how to index item ids for recommendation foundation models . отдельное спасибо авторам хочется выразить за подробный ablation study числа кодов в иерархии квантизации: они отмечают, что увеличение числа кодов не всегда улучшает работу модели из-за накопления ошибки при авторегрессивном инференсе без teacher forcing. очень полезны и данные о числах эмбеддингов в кодбуках. несмотря на большой вклад статьи в развитие семантической квантизации, у этой техники всё ещё остаются нерешенные проблемы. для его реализации нужны: 1. предобученная контентная модель (в их случае это llama-7b); 2. предобученная коллаборативная модель (например, sasrec или lightgcn); 3. другой подход к экспериментам — сейчас они, как правило, проводятся на открытых датасетах без time-split, из-за этого применимость метода в индустрии пока под вопросом. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-06T07:02:21+00:00" href="./posts/100.html">2025-05-06 07:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Learnable Item Tokenization for Generative Recommendation<br></strong><br>Тренд на семантические ID развивается уже более полугода. Начало положила статья <a href="https://arxiv.org/abs/2305.05065" rel="nofollow noopener noreferrer">TIGER</a>, в которой для генеративного ретривала контентные эмбеддинги айтемов квантовали с помощью RQ-VAE. Статья вышла ещё в 2023 году, но популярность к подходу начала приходить только после конференции RecSys в 2024-м.<br><br>В сегодняшней <a href="https://arxiv.org/abs/2405.07314" rel="nofollow noopener noreferrer">статье</a> авторы предлагают модификацию алгоритма квантизации — Learnable Item Tokenization for Generative Recommendation (LETTER). Новый подход основан на трёх идеях: <br><br>1. сохранение иерархичности семантической квантизации; <br>2. контрастивное сближение квантизаций и коллаборативных эмбеддингов (полученных через предобученный SASRec или LightGCN);<br>3. сглаживание распределений айтемов по центроидам кодбуков. <br><br>Еще одно отличие от TIGER — для того чтобы генерировать валидные коды, используются префиксные деревья по аналогии со статьей <a href="https://arxiv.org/pdf/2305.06569" rel="nofollow noopener noreferrer">How to Index Item IDs for Recommendation Foundation Models</a>.<br><br>Отдельное спасибо авторам хочется выразить за подробный ablation study числа кодов в иерархии квантизации: они отмечают, что увеличение числа кодов не всегда улучшает работу модели из-за накопления ошибки при авторегрессивном инференсе без teacher forcing. Очень полезны и данные о числах эмбеддингов в кодбуках. <br><br>Несмотря на большой вклад статьи в развитие семантической квантизации, у этой техники всё ещё остаются нерешенные проблемы. Для его реализации нужны: <br><br>1. предобученная контентная модель (в их случае это LLaMA-7B);<br>2. предобученная коллаборативная модель (например, SASRec или LightGCN);<br>3. другой подход к экспериментам — сейчас они, как правило, проводятся на открытых датасетах без time-split, из-за этого применимость метода в индустрии пока под вопросом. <br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/100_480.webp" srcset="../assets/media/thumbs/100_480.webp 480w, ../assets/media/100.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="100" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 100 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/100" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/100.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="99" data-search="slmrec: distilling large language models into small for sequential recommendation сегодня разбираем статью от исследователей rutgers university и ant group, представленную на iclr 2025. авторы предлагают альтернативу тяжёлым llm в рекомендательных системах. они доказывают, что для sequential recommendation достаточно компактных моделей, если правильно «дистиллировать» знания из больших llm. в статье рассматриваются два подхода интеграции llm в рекомендательные системы: — генеративные методы (g-llmrec): модель предсказывает следующий товар как «следующий токен» в последовательности (аналогично генерации текста). примеры: p5, llara. — методы на основе эмбеддингов (e-llmrec): llm используется как экстрактор признаков. последний скрытый слой модели преобразуется в вектор пользователя, который сравнивается с векторами товаров через скалярное произведение. slmrec относится ко второму типу. авторы применяют llm (говорят о llama-7b) для получения «учителя», а затем дистиллируют его знания в компактную модель через выравнивание промежуточных представлений. архитектура и подход — используют технику knowledge distillation: большая модель (llama-7b) выступает «учителем», а компактная (в 8 раз меньше) — «учеником». — обнаружили, что 75% слоёв в llm избыточны для рекомендательных задач. удаление лишних слоёв почти не влияет на качество. — вводят тройной механизм переноса знаний между учеником и учителем: выравнивание направлений эмбеддингов (через cosine similarity), регуляризация норм векторов и многоуровневый надзор за скрытыми состояниями. надзор вкратце такой: слои группируются в блоки, а на выходах каждого блока добавляются «адаптеры», которые проецируют скрытые состояния ученика в пространство учителя. ученик учится предсказывать выходы всех блоков одновременно, а не только финальный слой. — объединяют слои модели в блоки (по 4–8 слоев) для групповой дистилляции — так ученик учится воспроизводить иерархическое представление данных. — модель обучается только на позитивных взаимодействиях. ключевые фишки — эффективность: slmrec требует всего 13% параметров оригинальной llm, ускоряя обучение в 6,6 раза, а инференс — в 8 раз. — универсальность: метод совместим с другими техниками оптимизации — квантизацией и прунингом. — теоретическое обоснование: авторы математически доказали, что многослойные трансформеры избыточны для задач рекомендаций, и их можно заменить оптимизацией одного шага. эксперименты на данных amazon (одежда, фильмы, музыка, спорт) показали, что slmrec не только догоняет llm по метрикам (hr@10, ndcg), но иногда даже превосходит — вероятно, за счёт снижения шума в глубоких слоях. спорные моменты — неясно, как модель адаптируется к cold start — авторы используют предобученные эмбеддинги, но не проверяют сценарий с новыми пользователями или товарами. — как именно выбирались слои для удаления? в статье сказано: «экспериментально обнаружена избыточность», но нет чётких критериев. например, могла быть использована простая эвристика вроде «среднее значение активаций», что не гарантирует оптимальности. — метод тестировался только на amazon-датасетах (одежда, фильмы), где плотность взаимодействий выше, чем в реальных соцсетях. в системах с миллиардами пользователей и «длинными хвостами» нишевого контента (например, tiktok) эффективность slmrec под вопросом. — хотя инференс быстрее в 8 раз, сама дистилляция требует обучения как учителя (llama-7b), так и ученика. выводы работа предлагает практичный компромисс для продакшена. однако остаётся вопрос: можно ли масштабировать подход до экосистем с миллиардами айтемов, где даже 1b параметров — уже много? авторы обещают исследовать few-shot-обучение в будущем. @recsyschannel обзор подготовил ❣ елисей смирнов #yaiclr slmrec: distilling large language models into small for sequential recommendation сегодня разбираем статью от исследователей rutgers university и ant group, представленную на iclr 2025. авторы предлагают альтернативу тяжёлым llm в рекомендательных системах. они доказывают, что для sequential recommendation достаточно компактных моделей, если правильно «дистиллировать» знания из больших llm. в статье рассматриваются два подхода интеграции llm в рекомендательные системы: — генеративные методы (g-llmrec) : модель предсказывает следующий товар как «следующий токен» в последовательности (аналогично генерации текста). примеры: p5, llara. — методы на основе эмбеддингов (e-llmrec) : llm используется как экстрактор признаков. последний скрытый слой модели преобразуется в вектор пользователя, который сравнивается с векторами товаров через скалярное произведение. slmrec относится ко второму типу. авторы применяют llm (говорят о llama-7b) для получения «учителя», а затем дистиллируют его знания в компактную модель через выравнивание промежуточных представлений. архитектура и подход — используют технику knowledge distillation: большая модель (llama-7b) выступает «учителем», а компактная (в 8 раз меньше) — «учеником». — обнаружили, что 75% слоёв в llm избыточны для рекомендательных задач. удаление лишних слоёв почти не влияет на качество. — вводят тройной механизм переноса знаний между учеником и учителем: выравнивание направлений эмбеддингов (через cosine similarity), регуляризация норм векторов и многоуровневый надзор за скрытыми состояниями. надзор вкратце такой: слои группируются в блоки, а на выходах каждого блока добавляются «адаптеры», которые проецируют скрытые состояния ученика в пространство учителя. ученик учится предсказывать выходы всех блоков одновременно, а не только финальный слой. — объединяют слои модели в блоки (по 4–8 слоев) для групповой дистилляции — так ученик учится воспроизводить иерархическое представление данных. — модель обучается только на позитивных взаимодействиях. ключевые фишки — эффективность: slmrec требует всего 13% параметров оригинальной llm, ускоряя обучение в 6,6 раза, а инференс — в 8 раз. — универсальность: метод совместим с другими техниками оптимизации — квантизацией и прунингом. — теоретическое обоснование: авторы математически доказали, что многослойные трансформеры избыточны для задач рекомендаций, и их можно заменить оптимизацией одного шага. эксперименты на данных amazon (одежда, фильмы, музыка, спорт) показали, что slmrec не только догоняет llm по метрикам (hr@10, ndcg), но иногда даже превосходит — вероятно, за счёт снижения шума в глубоких слоях. спорные моменты — неясно, как модель адаптируется к cold start — авторы используют предобученные эмбеддинги, но не проверяют сценарий с новыми пользователями или товарами. — как именно выбирались слои для удаления? в статье сказано: «экспериментально обнаружена избыточность», но нет чётких критериев. например, могла быть использована простая эвристика вроде «среднее значение активаций», что не гарантирует оптимальности. — метод тестировался только на amazon-датасетах (одежда, фильмы), где плотность взаимодействий выше, чем в реальных соцсетях. в системах с миллиардами пользователей и «длинными хвостами» нишевого контента (например, tiktok) эффективность slmrec под вопросом. — хотя инференс быстрее в 8 раз, сама дистилляция требует обучения как учителя (llama-7b), так и ученика. выводы работа предлагает практичный компромисс для продакшена. однако остаётся вопрос: можно ли масштабировать подход до экосистем с миллиардами айтемов, где даже 1b параметров — уже много? авторы обещают исследовать few-shot-обучение в будущем. @recsyschannel обзор подготовил ❣ елисей смирнов #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-29T08:27:49+00:00" href="./posts/99.html">2025-04-29 08:27 UTC</a></div>
      </div>
      <div class="post-body"><strong>SLMREC: Distilling Large Language Models Into Small For Sequential Recommendation<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/abs/2405.17890" rel="nofollow noopener noreferrer">статью</a> от исследователей Rutgers University и Ant Group, представленную на ICLR 2025. Авторы предлагают альтернативу тяжёлым LLM в рекомендательных системах. Они доказывают, что для sequential recommendation достаточно компактных моделей, если правильно «дистиллировать» знания из больших LLM.<br><br>В статье рассматриваются два подхода интеграции LLM в рекомендательные системы: <br><br>— <strong>Генеративные методы (G-LLMRec)</strong>: Модель предсказывает следующий товар как «следующий токен» в последовательности (аналогично генерации текста). Примеры: P5, LLaRa.<br><br>— <strong>Методы на основе эмбеддингов (E-LLMRec)</strong>: LLM используется как экстрактор признаков. Последний скрытый слой модели преобразуется в вектор пользователя, который сравнивается с векторами товаров через скалярное произведение. SLMRec относится ко второму типу. <br><br>Авторы применяют LLM (говорят о LLaMa-7B) для получения «учителя», а затем дистиллируют его знания в компактную модель через выравнивание промежуточных представлений.<br><br><strong>Архитектура и подход<br></strong><br>— Используют технику knowledge distillation: большая модель (LLaMa-7B) выступает «учителем», а компактная (в 8 раз меньше) — «учеником».<br><br>— Обнаружили, что 75% слоёв в LLM избыточны для рекомендательных задач. Удаление лишних слоёв почти не влияет на качество.<br><br>— Вводят тройной механизм переноса знаний между учеником и учителем: выравнивание направлений эмбеддингов (через cosine similarity), регуляризация норм векторов и многоуровневый надзор за скрытыми состояниями. Надзор вкратце такой: слои группируются в блоки, а на выходах каждого блока добавляются «адаптеры», которые проецируют скрытые состояния ученика в пространство учителя. Ученик учится предсказывать выходы всех блоков одновременно, а не только финальный слой.<br><br>— Объединяют слои модели в блоки (по 4–8 слоев) для групповой дистилляции — так ученик учится воспроизводить иерархическое представление данных.<br><br>— Модель обучается только на позитивных взаимодействиях. <br><br><strong>Ключевые фишки<br></strong><br>— Эффективность: SLMREC требует всего 13% параметров оригинальной LLM, ускоряя обучение в 6,6 раза, а инференс — в 8 раз.<br><br>— Универсальность: метод совместим с другими техниками оптимизации — квантизацией и прунингом.<br><br>— Теоретическое обоснование: авторы математически доказали, что многослойные трансформеры избыточны для задач рекомендаций, и их можно заменить оптимизацией одного шага.<br><br>Эксперименты на данных Amazon (одежда, фильмы, музыка, спорт) показали, что SLMREC не только догоняет LLM по метрикам (HR@10, NDCG), но иногда даже превосходит — вероятно, за счёт снижения шума в глубоких слоях.<br><br><strong>Спорные моменты</strong><br><strong><br></strong>— Неясно, как модель адаптируется к cold start — авторы используют предобученные эмбеддинги, но не проверяют сценарий с новыми пользователями или товарами.<br><br>— Как именно выбирались слои для удаления? В статье сказано: «экспериментально обнаружена избыточность», но нет чётких критериев. Например, могла быть использована простая эвристика вроде «среднее значение активаций», что не гарантирует оптимальности.<br><br>— Метод тестировался только на Amazon-датасетах (одежда, фильмы), где плотность взаимодействий выше, чем в реальных соцсетях. В системах с миллиардами пользователей и «длинными хвостами» нишевого контента (например, TikTok) эффективность SLMREC под вопросом.<br><br>— Хотя инференс быстрее в 8 раз, сама дистилляция требует обучения как учителя (LLaMA-7B), так и ученика. <br><br><strong>Выводы</strong><br><br>Работа предлагает практичный компромисс для продакшена. Однако остаётся вопрос: можно ли масштабировать подход до экосистем с миллиардами айтемов, где даже 1B параметров — уже много? Авторы обещают исследовать few-shot-обучение в будущем.<br><br>@RecSysChannel<br>Обзор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Елисей Смирнов<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/99_480.webp" srcset="../assets/media/thumbs/99_480.webp 480w, ../assets/media/99.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="99" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 070 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/99" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/99.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="98" data-search="завтра — последний день iclr 2025 в сингапуре наши ml-инженеры уже увидели большую часть докладов и постеров на тему рекомендательных систем — впереди новые подборки потенциально полезных работ. а пока напоминаем, что интересного мы успели опубликовать за это время: - подборка статей двух первых дней конференции - фоторепортаж для тех, кто хочет проникнуться вайбом iclr - ещё немного фантастических видов сингапура - интересные статьи третьего дня iclr желаем участникам отличного окончания конференции, а всем остальным — полезного чтения! больше разборов, интересных постеров, фото и видео с iclr вы найдёте в наших других каналах: @timeforcv, @mlunderhood, @stuffynlp, @speechinfo. @recsyschannel #yaiclr завтра — последний день iclr 2025 в сингапуре наши ml-инженеры уже увидели большую часть докладов и постеров на тему рекомендательных систем — впереди новые подборки потенциально полезных работ. а пока напоминаем, что интересного мы успели опубликовать за это время: - подборка статей двух первых дней конференции - фоторепортаж для тех, кто хочет проникнуться вайбом iclr - ещё немного фантастических видов сингапура - интересные статьи третьего дня iclr желаем участникам отличного окончания конференции, а всем остальным — полезного чтения! больше разборов, интересных постеров, фото и видео с iclr вы найдёте в наших других каналах: @timeforcv , @mlunderhood , @stuffynlp , @speechinfo . @recsyschannel #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-27T08:05:09+00:00" href="./posts/98.html">2025-04-27 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Завтра — последний день ICLR 2025 в Сингапуре</strong><br><br>Наши ML-инженеры уже увидели большую часть докладов и постеров на тему рекомендательных систем — впереди новые подборки потенциально полезных работ. А пока напоминаем, что интересного мы успели опубликовать за это время:<br><br>- <a href="https://t.me/RecSysChannel/86" rel="nofollow noopener noreferrer">Подборка статей двух первых дней конференции</a><br>- <a href="https://t.me/RecSysChannel/90" rel="nofollow noopener noreferrer">Фоторепортаж для тех, кто хочет проникнуться вайбом ICLR</a><br>- <a href="https://t.me/RecSysChannel/83" rel="nofollow noopener noreferrer">Ещё немного фантастических видов Сингапура</a><br>- <a href="https://t.me/RecSysChannel/95" rel="nofollow noopener noreferrer">Интересные статьи третьего дня ICLR</a><br><br>Желаем участникам отличного окончания конференции, а всем остальным — полезного чтения!<br><br><em>Больше разборов, интересных постеров, фото и видео с ICLR вы найдёте в наших других каналах: </em><em>@timeforcv</em><em>, </em><em>@MLunderhood</em><em>, </em><em>@stuffyNLP</em><em>, </em><em>@speechinfo</em><em>.</em><br><br>@RecSysChannel<br><br>#YaICLR<div class="media"><video controls preload="metadata" src="../assets/media/98_IMG_6498.MOV.mov"></video></div></div>
      <div class="actions">
        <span>1 658 просмотров · 7 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/98" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/98.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="95" data-search="интересные статьи третьего дня iclr 2025 продолжаем рассказывать о работах на iclr 2025 по теме рекомендательных систем. собрали несколько релевантных постеров и коротко пересказали идеи: от симуляции пользователей для обучения llm до новых бенчмарков на сложные инструкции для ранжирования. language representations can be what recommenders need: findings and potentials авторы берут граф взаимодействий пользователей и айтемов, с помощью llm получают вектора для айтемов и пользователей (усредняя эмбеддинги положительных взаимодействий с айтемами). затем идут «вглубь» до какого-то момента по графу — и получают итоговые вектора. дальше нужно откуда-то семплировать негативы: в исследовании просто взяли случайные строки из датасета, с которыми пользователь не взаимодействовал (автор сказал, так поступили, потому что не хватило explicit-фидбэка). интересный момент про правый нижний угол постера: промпты для movielens генерировали через chatgpt, а потом вручную валидировали (поскольку chatgpt при генерации мог использовать таргетную информацию). при этом скоры получились подозрительно высокие — возможно, результат слегка завышен. ещё автор сказал, что некоторые компании уже видят профит от подхода, но деталей он не раскрыл. bridging jensen gap for max-min group fairness optimization in recommendation авторы делят датасет на группы (в их случае — жанры фильмов), считают внутри каждой группы лосс и на следующей итерации дают больший вес группе с худшим лоссом. cos: enhancing personalization and mitigating bias with context steering статья о том, как добавить контекст к выводу llm без обучения. при этом можно управлять уровнем контекстности (параметром λ). суть метода — измерять влияние контекста с точки зрения вероятности предсказания токена (с контекстом и без него). personalllm: tailoring llms to individual preferences авторы симулировали пользователей, создавая их предпочтения путём усреднения различных reward-моделей, а затем обучили llm на этих синтетических данных. деталей обучения не приводят, но на их бенчмарке модель показывает хорошие результаты. для новых пользователей ищут похожих на основе language space и строят ответы, опираясь на поведение тех, чьи данные были в обучении. beyond content relevance: evaluating instruction following in retrieval models исследователи жалуются, что современные модели ранжирования плохо понимают сложные инструкции вроде: «найди статью на турецком в 5 абзацев, написанную простым языком» — по этому поводу собрали бенчмарк. рассматривали следующие параметры: пользователь (audience), поисковые запросы или темы (keyword), формат отображения (format), длина ответа (length), язык (language), источник информации (source). качество работы моделей оценивали с помощью двух метрик: - strict instruction compliance ratio (sicr): бинарная метрика, которая проверяет, что при явном указании условия (например, «документ только на казахском») скор растёт относительно безусловного режима, а при обратном условии («всё кроме казахского») — падает. - weighted instruction sensitivity evaluation (wise): версия метрики, учитывающая изменения позиций в ранжировании. лучше всех с задачей справился gpt-4o. @recsyschannel интересные работы заметили ❣ маргарита мишустина, эльдар ганбаров, алёна фомина, алексей степанов #yaiclr интересные статьи третьего дня iclr 2025 продолжаем рассказывать о работах на iclr 2025 по теме рекомендательных систем. собрали несколько релевантных постеров и коротко пересказали идеи: от симуляции пользователей для обучения llm до новых бенчмарков на сложные инструкции для ранжирования. language representations can be what recommenders need: findings and potentials авторы берут граф взаимодействий пользователей и айтемов, с помощью llm получают вектора для айтемов и пользователей (усредняя эмбеддинги положительных взаимодействий с айтемами). затем идут «вглубь» до какого-то момента по графу — и получают итоговые вектора. дальше нужно откуда-то семплировать негативы: в исследовании просто взяли случайные строки из датасета, с которыми пользователь не взаимодействовал (автор сказал, так поступили, потому что не хватило explicit-фидбэка). интересный момент про правый нижний угол постера: промпты для movielens генерировали через chatgpt, а потом вручную валидировали (поскольку chatgpt при генерации мог использовать таргетную информацию). при этом скоры получились подозрительно высокие — возможно, результат слегка завышен. ещё автор сказал, что некоторые компании уже видят профит от подхода, но деталей он не раскрыл. bridging jensen gap for max-min group fairness optimization in recommendation авторы делят датасет на группы (в их случае — жанры фильмов), считают внутри каждой группы лосс и на следующей итерации дают больший вес группе с худшим лоссом. cos: enhancing personalization and mitigating bias with context steering статья о том, как добавить контекст к выводу llm без обучения. при этом можно управлять уровнем контекстности (параметром λ). суть метода — измерять влияние контекста с точки зрения вероятности предсказания токена (с контекстом и без него). personalllm: tailoring llms to individual preferences авторы симулировали пользователей, создавая их предпочтения путём усреднения различных reward-моделей, а затем обучили llm на этих синтетических данных. деталей обучения не приводят, но на их бенчмарке модель показывает хорошие результаты. для новых пользователей ищут похожих на основе language space и строят ответы, опираясь на поведение тех, чьи данные были в обучении. beyond content relevance: evaluating instruction following in retrieval models исследователи жалуются, что современные модели ранжирования плохо понимают сложные инструкции вроде: «найди статью на турецком в 5 абзацев, написанную простым языком» — по этому поводу собрали бенчмарк. рассматривали следующие параметры: пользователь (audience), поисковые запросы или темы (keyword), формат отображения (format), длина ответа (length), язык (language), источник информации (source). качество работы моделей оценивали с помощью двух метрик: - strict instruction compliance ratio (sicr): бинарная метрика, которая проверяет, что при явном указании условия (например, «документ только на казахском») скор растёт относительно безусловного режима, а при обратном условии («всё кроме казахского») — падает. - weighted instruction sensitivity evaluation (wise): версия метрики, учитывающая изменения позиций в ранжировании. лучше всех с задачей справился gpt-4o. @recsyschannel интересные работы заметили ❣ маргарита мишустина, эльдар ганбаров, алёна фомина, алексей степанов #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-26T13:31:35+00:00" href="./posts/95.html">2025-04-26 13:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные статьи третьего дня ICLR 2025</strong><br><br>Продолжаем рассказывать о работах на ICLR 2025 по теме рекомендательных систем. Собрали несколько релевантных постеров и коротко пересказали идеи: от симуляции пользователей для обучения LLM до новых бенчмарков на сложные инструкции для ранжирования.<br><br><a href="https://arxiv.org/abs/2407.05441" rel="nofollow noopener noreferrer"><strong>Language Representations Can be What Recommenders Need: Findings and Potentials</strong></a><br><br>Авторы берут граф взаимодействий пользователей и айтемов, с помощью LLM получают вектора для айтемов и пользователей (усредняя эмбеддинги положительных взаимодействий с айтемами). Затем идут «вглубь» до какого-то момента по графу — и получают итоговые вектора. <br><br>Дальше нужно откуда-то семплировать негативы: в исследовании просто взяли случайные строки из датасета, с которыми пользователь не взаимодействовал (автор сказал, так поступили, потому что не хватило explicit-фидбэка). <br><br>Интересный момент про правый нижний угол постера: промпты для Movielens генерировали через ChatGPT, а потом вручную валидировали (поскольку ChatGPT при генерации мог использовать таргетную информацию). <br><br>При этом скоры получились подозрительно высокие — возможно, результат слегка завышен.<br><br>Ещё автор сказал, что некоторые компании уже видят профит от подхода, но деталей он не раскрыл.<br><br><a href="https://arxiv.org/abs/2502.09319" rel="nofollow noopener noreferrer"><strong>Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation</strong></a><br><br>Авторы делят датасет на группы (в их случае — жанры фильмов), считают внутри каждой группы лосс и на следующей итерации дают больший вес группе с худшим лоссом.<br><br><a href="https://arxiv.org/html/2405.01768v1" rel="nofollow noopener noreferrer"><strong>CoS: Enhancing Personalization and Mitigating Bias with Context Steering</strong></a><br><br>Статья о том, как добавить контекст к выводу LLM без обучения. При этом можно управлять уровнем контекстности (параметром λ). Суть метода — измерять влияние контекста с точки зрения вероятности предсказания токена (с контекстом и без него).<br><br><a href="https://arxiv.org/abs/2409.20296" rel="nofollow noopener noreferrer"><strong>PersonalLLM: Tailoring LLMs to Individual Preferences</strong></a><br><br>Авторы симулировали пользователей, создавая их предпочтения путём усреднения различных reward-моделей, а затем обучили LLM на этих синтетических данных. Деталей обучения не приводят, но на их бенчмарке модель показывает хорошие результаты. Для новых пользователей ищут похожих на основе language space и строят ответы, опираясь на поведение тех, чьи данные были в обучении.<br><br><a href="https://arxiv.org/html/2410.23841v1" rel="nofollow noopener noreferrer"><strong>Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models</strong></a><br><br>Исследователи жалуются, что современные модели ранжирования плохо понимают сложные инструкции вроде: «найди статью на турецком в 5 абзацев, написанную простым языком» — по этому поводу собрали бенчмарк.<br><br>Рассматривали следующие параметры: пользователь (Audience), поисковые запросы или темы (Keyword), формат отображения (Format), длина ответа (Length), язык (Language), источник информации (Source).<br><br>Качество работы моделей оценивали с помощью двух метрик:<br><br>- Strict Instruction Compliance Ratio (SICR): бинарная метрика, которая проверяет, что при явном указании условия (например, «документ только на казахском») скор растёт относительно безусловного режима, а при обратном условии («всё кроме казахского») — падает.<br><br>- Weighted Instruction Sensitivity Evaluation (WISE): версия метрики, учитывающая изменения позиций в ранжировании.<br><br>Лучше всех с задачей справился GPT-4o.<br><br>@RecSysChannel<br><br>Интересные работы заметили <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Маргарита Мишустина, Эльдар Ганбаров, Алёна Фомина, Алексей Степанов<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/95_480.webp" srcset="../assets/media/thumbs/95_480.webp 480w, ../assets/media/95.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/96_480.webp" srcset="../assets/media/thumbs/96_480.webp 480w, ../assets/media/96.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/97_480.webp" srcset="../assets/media/thumbs/97_480.webp 480w, ../assets/media/97.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 370 просмотров · 11 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/95" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/95.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="90" data-search="кадры из самой гущи событий. можно оценить масштабы главного холла, где выступают с докладами, своими глазами увидеть очередь к хайповому стенду и убедиться: сингапур хорош как при свете дня, так и под покровом ночи. @recsyschannel #yaiclr кадры из самой гущи событий. можно оценить масштабы главного холла, где выступают с докладами, своими глазами увидеть очередь к хайповому стенду и убедиться: сингапур хорош как при свете дня, так и под покровом ночи. @recsyschannel #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T14:28:22+00:00" href="./posts/90.html">2025-04-25 14:28 UTC</a></div>
      </div>
      <div class="post-body">Кадры из самой гущи событий. Можно оценить масштабы главного холла, где выступают с докладами, своими глазами увидеть очередь к хайповому стенду и убедиться: Сингапур хорош как при свете дня, так и под покровом ночи.<br><br>@RecSysChannel<br><br>#YaICLR<div class="media"><video controls preload="metadata" src="../assets/media/90_____.mp4"></video><video controls preload="metadata" src="../assets/media/91_IMG_8354__1_.mp4"></video><img class="media-img" loading="lazy" src="../assets/media/thumbs/92_480.webp" srcset="../assets/media/thumbs/92_480.webp 480w, ../assets/media/92.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/93_480.webp" srcset="../assets/media/thumbs/93_480.webp 480w, ../assets/media/93.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/94_480.webp" srcset="../assets/media/thumbs/94_480.webp 480w, ../assets/media/94.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 356 просмотров · 10 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/90" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/90.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="86" data-search="интересные статьи двух первых дней iclr 2025 конференция в разгаре — статей по рекомендательным системам становится всё больше! делимся избранным и ждём комментариев: какие идеи показались интересными вам. contextgnn: beyond two-tower recommendation systems в статье описано, как объединить попарный скор и двухбашенный подход в одной модели, избежав недостатков каждого решения и не делая двухстадийное ранжирование. для этого используют разные модели для объектов, с которыми пользователь взаимодействовал, и остальных, прогнозируя пожелания пользователя в данный момент. preference diffusion for recommendation авторы из tiktok-ток развивают идеи диффузионных моделей для рекомендаций. базово решают задачу предсказания следующей покупки или взаимодействия пользователя, пытаясь диффузионками сгенерировать (!) эмбеддинг товара. недостаток — решение обучается и применяется только на пользователях, сделавших хотя бы 10 покупок, и автор признала, что в проде такое не взлетит. in prospect and retrospect: reflective memory management for long-term personalized dialogue agents статья о персонализации в контексте llm. первая идея: точность модели существенно растёт, если использовать весь предыдущий контекст пользователя (диалога). вторая — в целом, пользовательские фичи можно собирать поумнее: сначала суммиризировать, потом дополнять суммаризацию релевантными топиками из базы, дальше использовать rl-подход для отчистки базы. это, кстати, применимо не только к ассистенту, но и в целом к другим проектам персонализации или рекомендаций. slmrec: distilling large language models into small for sequential recommendation авторы хотят прикрутить llm к рекомендациям — посмотрели на существующие алгоритмы и задались разумным вопросом: «откуда зафриженные llm могут узнать об айдшниках в промпте?» и «точно ли все параметры llm так уж нужны?». в итоге взяли часть слоёв llm (13% параметров осталось), предложили дистилляцию — то есть дообучают кусок llm под задачу ранжирования и делают так, чтобы эмбеды совпадали у дистиллируемой части и учителя. автор говорит, что решение применяется в 6–8 раз быстрее, чем llm до выкидывания слоёв. @recsyschannel интересные постеры заметили ❣ василий астахов, александр воронцов, алёна фомина и маргарита мишустина #yaiclr интересные статьи двух первых дней iclr 2025 конференция в разгаре — статей по рекомендательным системам становится всё больше! делимся избранным и ждём комментариев: какие идеи показались интересными вам. contextgnn: beyond two-tower recommendation systems в статье описано, как объединить попарный скор и двухбашенный подход в одной модели, избежав недостатков каждого решения и не делая двухстадийное ранжирование. для этого используют разные модели для объектов, с которыми пользователь взаимодействовал, и остальных, прогнозируя пожелания пользователя в данный момент. preference diffusion for recommendation авторы из tiktok-ток развивают идеи диффузионных моделей для рекомендаций. базово решают задачу предсказания следующей покупки или взаимодействия пользователя, пытаясь диффузионками сгенерировать (!) эмбеддинг товара. недостаток — решение обучается и применяется только на пользователях, сделавших хотя бы 10 покупок, и автор признала, что в проде такое не взлетит. in prospect and retrospect: reflective memory management for long-term personalized dialogue agents статья о персонализации в контексте llm. первая идея: точность модели существенно растёт, если использовать весь предыдущий контекст пользователя (диалога). вторая — в целом, пользовательские фичи можно собирать поумнее: сначала суммиризировать, потом дополнять суммаризацию релевантными топиками из базы, дальше использовать rl-подход для отчистки базы. это, кстати, применимо не только к ассистенту, но и в целом к другим проектам персонализации или рекомендаций. slmrec: distilling large language models into small for sequential recommendation авторы хотят прикрутить llm к рекомендациям — посмотрели на существующие алгоритмы и задались разумным вопросом: «откуда зафриженные llm могут узнать об айдшниках в промпте?» и «точно ли все параметры llm так уж нужны?». в итоге взяли часть слоёв llm (13% параметров осталось), предложили дистилляцию — то есть дообучают кусок llm под задачу ранжирования и делают так, чтобы эмбеды совпадали у дистиллируемой части и учителя. автор говорит, что решение применяется в 6–8 раз быстрее, чем llm до выкидывания слоёв. @recsyschannel интересные постеры заметили ❣ василий астахов, александр воронцов, алёна фомина и маргарита мишустина #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T08:20:58+00:00" href="./posts/86.html">2025-04-25 08:20 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные статьи двух первых дней ICLR 2025</strong><br><strong><br></strong>Конференция в разгаре — статей по рекомендательным системам становится всё больше! Делимся избранным и ждём комментариев: какие идеи показались интересными вам.<br><br><a href="https://arxiv.org/abs/2411.19513" rel="nofollow noopener noreferrer"><strong>ContextGNN: Beyond Two-Tower Recommendation Systems</strong></a><br>В статье описано, как объединить попарный скор и двухбашенный подход в одной модели, избежав недостатков каждого решения и не делая двухстадийное ранжирование. Для этого используют разные модели для объектов, с которыми пользователь взаимодействовал, и остальных, прогнозируя пожелания пользователя в данный момент.<br><br><a href="https://arxiv.org/abs/2410.13117" rel="nofollow noopener noreferrer"><strong>Preference Diffusion for Recommendation</strong></a><br>Авторы из TikTok-ток развивают идеи диффузионных моделей для рекомендаций. Базово решают задачу предсказания следующей покупки или взаимодействия пользователя, пытаясь диффузионками сгенерировать (!) эмбеддинг товара. Недостаток — решение обучается и применяется только на пользователях, сделавших хотя бы 10 покупок, и автор признала, что в проде такое не взлетит.<br><br><a href="https://arxiv.org/abs/2503.08026" rel="nofollow noopener noreferrer"><strong>In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents</strong></a><br>Статья о персонализации в контексте LLM. Первая идея: точность модели существенно растёт, если использовать весь предыдущий контекст пользователя (диалога). Вторая — в целом, пользовательские фичи можно собирать поумнее: сначала суммиризировать, потом дополнять суммаризацию релевантными топиками из базы, дальше использовать RL-подход для отчистки базы. Это, кстати, применимо не только к ассистенту, но и в целом к другим проектам персонализации или рекомендаций.<br><br><a href="https://arxiv.org/abs/2405.17890" rel="nofollow noopener noreferrer"><strong>SLMRec: Distilling Large Language Models into Small for Sequential Recommendation</strong></a><br>Авторы хотят прикрутить LLM к рекомендациям — посмотрели на существующие алгоритмы и задались разумным вопросом: «откуда зафриженные LLM могут узнать об айдшниках в промпте?» и «точно ли все параметры LLM так уж нужны?». В итоге взяли часть слоёв LLM (13% параметров осталось), предложили дистилляцию — то есть дообучают кусок LLM под задачу ранжирования и делают так, чтобы эмбеды совпадали у дистиллируемой части и учителя. Автор говорит, что решение применяется в 6–8 раз быстрее, чем LLM до выкидывания слоёв.<br><br>@RecSysChannel<br><br>Интересные постеры заметили <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Василий Астахов, Александр Воронцов, Алёна Фомина и Маргарита Мишустина<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/86_480.webp" srcset="../assets/media/thumbs/86_480.webp 480w, ../assets/media/86.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="86" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/87_480.webp" srcset="../assets/media/thumbs/87_480.webp 480w, ../assets/media/87.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="86" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/88_480.webp" srcset="../assets/media/thumbs/88_480.webp 480w, ../assets/media/88.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="86" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/89_480.webp" srcset="../assets/media/thumbs/89_480.webp 480w, ../assets/media/89.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="86" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 733 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/86" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/86.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="83" data-search="эти фото сделаны в городе ессентуки сингапуре, где завтра начнётся iclr 2025 — одна из крупнейших конференций в области машинного обучения. ml-инженеры яндекса уже отправились в центр событий, и скоро канал наполнится новостями с мероприятия! эти фото сделаны в городе ессентуки сингапуре, где завтра начнётся iclr 2025 — одна из крупнейших конференций в области машинного обучения. ml-инженеры яндекса уже отправились в центр событий, и скоро канал наполнится новостями с мероприятия!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-23T16:02:56+00:00" href="./posts/83.html">2025-04-23 16:02 UTC</a></div>
      </div>
      <div class="post-body">Эти фото сделаны в городе <del>Ессентуки</del> Сингапуре, где завтра начнётся ICLR 2025 — одна из крупнейших конференций в области машинного обучения. ML-инженеры Яндекса уже отправились в центр событий, и скоро канал наполнится новостями с мероприятия!<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/83_480.webp" srcset="../assets/media/thumbs/83_480.webp 480w, ../assets/media/83.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="83" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/84_480.webp" srcset="../assets/media/thumbs/84_480.webp 480w, ../assets/media/84.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="83" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/85_480.webp" srcset="../assets/media/thumbs/85_480.webp 480w, ../assets/media/85.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="83" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 539 просмотров · 36 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/83" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/83.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="82" data-search="interformer: towards effective heterogeneous interaction learning for ctr prediction сегодня разбираем статью interformer — новую архитектуру для ctr prediction, в которой особое внимание уделено взаимодействию между разными типами признаков. модель создавалась при участии соавторов wukong, и является её идейным продолжением. в новой статье особое внимание уделяется работе с различными последовательностями, которые можно извлечь из пользовательских логов. авторы пытаются исправить два недостатка существующих моделей: 1) последовательности уточняются контекстными признаками, но не наоборот; 2) слишком агрессивная агрегация последовательностей. interformer пытается решить обе проблемы с помощью двух ветвей обработки — «глобальной» и «последовательной», — которые обмениваются информацией в каждом новом слое. в первую ветку попадают всевозможные категориальные и dense-признаки. эта ветка отвечает за построение взаимодействий признаков, что можно делать несколькими способами: используя факторизационные машины, dcnv2 или, например, dhen. вторая ветка предназначена для работы с последовательностями. сначала данные очищаются с помощью masknet’а, а затем подаются в классический attention-слой. ключевая особенность модели — механизм взаимодействия между этими ветками. из первой ветки во вторую перед attention-слоем приходит обучаемая проекция на элементы последовательности. в свою очередь, из второй ветки в первую передаётся агрегация последовательности, в которую входят cls-токен, pma (pooling by multihead attention), а также фиксированное число последних элементов последовательности. важно, что взаимодействия между признаками в первой ветке считаются уже с учётом этой агрегации, а для сохранения размерностей используются обычные mlp. с помощью такой организации перекрёстного обмена авторы решают сразу обе указанные ими проблемы. interformer тестируется на трёх публичных датасетах и одном крупном внутреннем. на всех задачах он показывает sota-результаты, обгоняя как non-sequential-, так и известные sequential-решения. в отдельном эксперименте авторы показывают, что действительно важен взаимный обмен информацией между ветками. при его (частичном) отключении качество значительно проседает. также исследуется масштабируемость interformer’а по числу и размерам последовательностей и самой модели — авторы утверждают, что модель хорошо скейлится по всем направлениям. наконец, авторы проводят небольшое ablation study, по результатам которого делают вывод, что каждая составляющая предложенной в статье агрегации последовательностей очень важна. @recsyschannel обзор подготовил ❣ олег сорокин interformer: towards effective heterogeneous interaction learning for ctr prediction сегодня разбираем статью interformer — новую архитектуру для ctr prediction, в которой особое внимание уделено взаимодействию между разными типами признаков. модель создавалась при участии соавторов wukong , и является её идейным продолжением. в новой статье особое внимание уделяется работе с различными последовательностями, которые можно извлечь из пользовательских логов. авторы пытаются исправить два недостатка существующих моделей: 1) последовательности уточняются контекстными признаками, но не наоборот; 2) слишком агрессивная агрегация последовательностей. interformer пытается решить обе проблемы с помощью двух ветвей обработки — «глобальной» и «последовательной», — которые обмениваются информацией в каждом новом слое. в первую ветку попадают всевозможные категориальные и dense-признаки. эта ветка отвечает за построение взаимодействий признаков, что можно делать несколькими способами: используя факторизационные машины, dcnv2 или, например, dhen . вторая ветка предназначена для работы с последовательностями. сначала данные очищаются с помощью masknet’а , а затем подаются в классический attention-слой. ключевая особенность модели — механизм взаимодействия между этими ветками. из первой ветки во вторую перед attention-слоем приходит обучаемая проекция на элементы последовательности. в свою очередь, из второй ветки в первую передаётся агрегация последовательности, в которую входят cls-токен, pma (pooling by multihead attention), а также фиксированное число последних элементов последовательности. важно, что взаимодействия между признаками в первой ветке считаются уже с учётом этой агрегации, а для сохранения размерностей используются обычные mlp. с помощью такой организации перекрёстного обмена авторы решают сразу обе указанные ими проблемы. interformer тестируется на трёх публичных датасетах и одном крупном внутреннем. на всех задачах он показывает sota-результаты, обгоняя как non-sequential-, так и известные sequential-решения. в отдельном эксперименте авторы показывают, что действительно важен взаимный обмен информацией между ветками. при его (частичном) отключении качество значительно проседает. также исследуется масштабируемость interformer’а по числу и размерам последовательностей и самой модели — авторы утверждают, что модель хорошо скейлится по всем направлениям. наконец, авторы проводят небольшое ablation study, по результатам которого делают вывод, что каждая составляющая предложенной в статье агрегации последовательностей очень важна. @recsyschannel обзор подготовил ❣ олег сорокин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-18T07:32:35+00:00" href="./posts/82.html">2025-04-18 07:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>InterFormer: Towards Effective Heterogeneous Interaction Learning for CTR Prediction</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2411.09852" rel="nofollow noopener noreferrer">статью</a> InterFormer — новую архитектуру для CTR prediction, в которой особое внимание уделено взаимодействию между разными типами признаков.<br><br>Модель создавалась при участии соавторов <a href="https://t.me/RecSysChannel/80" rel="nofollow noopener noreferrer">Wukong</a>, и является её идейным продолжением. В новой статье особое внимание уделяется работе с различными последовательностями, которые можно извлечь из пользовательских логов. Авторы пытаются исправить два недостатка существующих моделей:<br><br>1) последовательности уточняются контекстными признаками, но не наоборот;<br>2) слишком агрессивная агрегация последовательностей.<br><br>InterFormer пытается решить обе проблемы с помощью двух ветвей обработки — «глобальной» и «последовательной», — которые обмениваются информацией в каждом новом слое.<br><br>В первую ветку попадают всевозможные категориальные и dense-признаки. Эта ветка отвечает за построение взаимодействий признаков, что можно делать несколькими способами: используя факторизационные машины, <a href="https://arxiv.org/abs/2008.13535" rel="nofollow noopener noreferrer">DCNv2</a> или, например, <a href="https://arxiv.org/abs/2203.11014" rel="nofollow noopener noreferrer">DHEN</a>.<br><br>Вторая ветка предназначена для работы с последовательностями. Сначала данные очищаются с помощью <a href="https://arxiv.org/abs/2102.07619" rel="nofollow noopener noreferrer">MaskNet’а</a>, а затем подаются в классический attention-слой.<br><br>Ключевая особенность модели — механизм взаимодействия между этими ветками. Из первой ветки во вторую перед attention-слоем приходит обучаемая проекция на элементы последовательности. В свою очередь, из второй ветки в первую передаётся агрегация последовательности, в которую входят CLS-токен, PMA (Pooling by Multihead Attention), а также фиксированное число последних элементов последовательности. Важно, что взаимодействия между признаками в первой ветке считаются уже с учётом этой агрегации, а для сохранения размерностей используются обычные MLP. С помощью такой организации перекрёстного обмена авторы решают сразу обе указанные ими проблемы.<br><br>InterFormer тестируется на трёх публичных датасетах и одном крупном внутреннем. На всех задачах он показывает SOTA-результаты, обгоняя как non-sequential-, так и известные sequential-решения.<br><br>В отдельном эксперименте авторы показывают, что действительно важен взаимный обмен информацией между ветками. При его (частичном) отключении качество значительно проседает.<br><br>Также исследуется масштабируемость InterFormer’а по числу и размерам последовательностей и самой модели — авторы утверждают, что модель хорошо скейлится по всем направлениям.<br><br>Наконец, авторы проводят небольшое ablation study, по результатам которого делают вывод, что каждая составляющая предложенной в статье агрегации последовательностей очень важна.<br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Олег Сорокин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/82_480.webp" srcset="../assets/media/thumbs/82_480.webp 480w, ../assets/media/82.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="82" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 126 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/82" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/82.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="81" data-search="что делают в мире: llm &amp; recsys. часть 1/2 этой весной мы запланировали чаще делиться подборками последних статей в области information retrieval. решили начать с темы влияния llm на recsys. в первой части — общий обзор тренда, а во второй — разбор статей по этой теме. постановка задач, решаемых llm и рекомендательными моделями, очень похожа: есть исторический контекст в виде упорядоченной последовательности, по которой мы хотим подобрать наиболее подходящий следующий объект в этой последовательности — семантический токен или айтем-товар. неудивительно, что если мы посмотрим на sota-подходы последних лет, то увидим концептуально схожие мотивы и методы: авторегрессии, господство трансформеров, gpt- и bert-like архитектуры, rl-методы по типу dpo — всё это и показывает отличные результаты в задачах обработки естественного языка, и с успехом адаптируется в рекомендательных моделях. передний край исследований «recsys as is» как раз состоит в том, чтобы повторить успех llm в скейлинге и качестве. однако сами llm не особо хорошо справляются с рекомендациями «из коробки»: плохо учитывают исторический контекст, могут галлюцинировать. но в силу обширного знания о мире и прокаченных способностей к рассуждениям внедрение llm в recsys-пайплайн кажется заманчивой перспективой. несколько ярких направлений (но далеко не единственных), в рамках которых можно использовать сильные стороны языковых моделей: — end-to-end llm-based рекомендации. концептуально всё просто — конструируем промпт, получаем рекомендацию на выходе. самое наивное решение — запрос «посоветуй фантастический фильм». решение чуть более персонализированное — «я только что посмотрел всю сагу „звездные войны“, посоветуй мне фантастический фильм» (и прочие zero-shot- и few-shot-решения). но если добавить контекста, внести большую историю взаимодействий, затюнить модель на нужный таргет, то уже можно получать хорошие результаты. — llm для извлечения знания. большие языковые модели — носители гигантского объёма информации во всём многообразии тем и идей. из llm можно брать информативные латентные представления, специфически уточняя их для того или иного таргета. — llm для объяснения рекомендаций. recsys-модели хранят в себе много полезного знания, но они, по большей части, — чёрные ящики. хотелось бы иметь возможность влиять на причины и логику того, почему тот или иной товар подходит пользователю или нет, а llm — как раз тот инструмент, который может значительно улучшить explainability сложных моделей. в следующей части — разбираем последние статьи на тему llm &amp; recsys. @recsyschannel обзор подготовил ❣ руслан кулиев что делают в мире: llm &amp;amp; recsys. часть 1/2 этой весной мы запланировали чаще делиться подборками последних статей в области information retrieval. решили начать с темы влияния llm на recsys. в первой части — общий обзор тренда, а во второй — разбор статей по этой теме. постановка задач, решаемых llm и рекомендательными моделями, очень похожа: есть исторический контекст в виде упорядоченной последовательности, по которой мы хотим подобрать наиболее подходящий следующий объект в этой последовательности — семантический токен или айтем-товар. неудивительно, что если мы посмотрим на sota-подходы последних лет, то увидим концептуально схожие мотивы и методы: авторегрессии, господство трансформеров, gpt- и bert-like архитектуры, rl-методы по типу dpo — всё это и показывает отличные результаты в задачах обработки естественного языка, и с успехом адаптируется в рекомендательных моделях. передний край исследований «recsys as is» как раз состоит в том, чтобы повторить успех llm в скейлинге и качестве. однако сами llm не особо хорошо справляются с рекомендациями «из коробки»: плохо учитывают исторический контекст, могут галлюцинировать. но в силу обширного знания о мире и прокаченных способностей к рассуждениям внедрение llm в recsys-пайплайн кажется заманчивой перспективой. несколько ярких направлений (но далеко не единственных), в рамках которых можно использовать сильные стороны языковых моделей: — end-to-end llm-based рекомендации. концептуально всё просто — конструируем промпт, получаем рекомендацию на выходе. самое наивное решение — запрос «посоветуй фантастический фильм». решение чуть более персонализированное — «я только что посмотрел всю сагу „звездные войны“, посоветуй мне фантастический фильм» (и прочие zero-shot- и few-shot-решения). но если добавить контекста, внести большую историю взаимодействий, затюнить модель на нужный таргет, то уже можно получать хорошие результаты. — llm для извлечения знания. большие языковые модели — носители гигантского объёма информации во всём многообразии тем и идей. из llm можно брать информативные латентные представления, специфически уточняя их для того или иного таргета. — llm для объяснения рекомендаций. recsys-модели хранят в себе много полезного знания, но они, по большей части, — чёрные ящики. хотелось бы иметь возможность влиять на причины и логику того, почему тот или иной товар подходит пользователю или нет, а llm — как раз тот инструмент, который может значительно улучшить explainability сложных моделей. в следующей части — разбираем последние статьи на тему llm &amp;amp; recsys. @recsyschannel обзор подготовил ❣ руслан кулиев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-09T08:04:50+00:00" href="./posts/81.html">2025-04-09 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что делают в мире: LLM &amp; RecSys. Часть 1/2<br></strong><br>Этой весной мы запланировали чаще делиться подборками последних статей в области Information Retrieval. Решили начать с темы влияния LLM на RecSys. В первой части — общий обзор тренда, а во второй — разбор статей по этой теме.<br><br>Постановка задач, решаемых LLM и рекомендательными моделями, очень похожа: есть исторический контекст в виде упорядоченной последовательности, по которой мы хотим подобрать наиболее подходящий следующий объект в этой последовательности — семантический токен или айтем-товар. Неудивительно, что если мы посмотрим на SOTA-подходы последних лет, то увидим концептуально схожие мотивы и методы: авторегрессии, господство трансформеров, GPT- и BERT-like архитектуры, RL-методы по типу DPO — всё это и показывает отличные результаты в задачах обработки естественного языка, и с успехом адаптируется в рекомендательных моделях. Передний край исследований «RecSys as is» как раз состоит в том, чтобы повторить успех LLM в скейлинге и качестве.<br><br>Однако сами LLM не особо хорошо справляются с рекомендациями «из коробки»: плохо учитывают исторический контекст, могут галлюцинировать. Но в силу обширного знания о мире и прокаченных способностей к рассуждениям внедрение LLM в RecSys-пайплайн кажется заманчивой перспективой. Несколько ярких направлений (но далеко не единственных), в рамках которых можно использовать сильные стороны языковых моделей:<br><br>— <strong>End-to-end LLM-based рекомендации.</strong> Концептуально всё просто — конструируем промпт, получаем рекомендацию на выходе. Самое наивное решение — запрос «Посоветуй фантастический фильм». Решение чуть более персонализированное — «Я только что посмотрел всю сагу „Звездные войны“, посоветуй мне фантастический фильм» (и прочие zero-shot- и few-shot-решения). Но если добавить контекста, внести большую историю взаимодействий, затюнить модель на нужный таргет, то уже можно получать хорошие результаты.<br><br>— <strong>LLM для извлечения знания.</strong> Большие языковые модели — носители гигантского объёма информации во всём многообразии тем и идей. Из LLM можно брать информативные латентные представления, специфически уточняя их для того или иного таргета.<br><br><strong>— LLM для объяснения рекомендаций.</strong> RecSys-модели хранят в себе много полезного знания, но они, по большей части, — чёрные ящики. Хотелось бы иметь возможность влиять на причины и логику того, почему тот или иной товар подходит пользователю или нет, а LLM — как раз тот инструмент, который может значительно улучшить explainability сложных моделей.<br><br>В следующей части — разбираем последние статьи на тему LLM &amp; RecSys.<br><br>@RecSysChannel<br>Обзор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Руслан Кулиев</div>
      <div class="actions">
        <span>1 981 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/81" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/81.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="80" data-search="wukong: towards a scaling law for large-scale recommendation сегодня разбираем не новую, но важную статью на тему scaling law в recsys. в домене nlp он сводится к идее: чем больше модель, тем выше её качество. но в современных рекомендательных системах нет такой явной зависимости. мотивация авторов в том, чтобы создать архитектуру, которая, прежде всего, хорошо масштабируется по параметрам. исследователи утверждают, что достигли своих целей: 1) создали архитектуру, которая позволяет улавливать сложные взаимодействия признаков высокого порядка; 2) добились плавного масштабирования качества в зависимости от размера датасета, объёма вычислений (gflop) и ограничений по параметрам. в статье по большей части рассматривают подход sparse scaling, суть которого в том, чтобы добавить множество эмбеддингов и за счёт этого масштабироваться по числу параметров. однако это не совсем то, к чему стремятся авторы, по двум причинам. первая: если просто добавить много новых эмбеддингов, не будут улавливаться нетривиальные взаимодействия. вторая: при таком подходе не используется потенциал современных gpu, а только задействуется дополнительная видеопамять. ключевая инновация статьи — использование серии последовательно расположенных факторизационных машин (factorization machines, fmb). это как раз и позволяет учитывать нетривиальные взаимодействия высоких порядков. общую схему архитектуры можно описать так: сначала берутся dense embeddings — это преобразование всех признаков в эмбеддинги. затем следует блок interaction stack, состоящий из нескольких wukong-слоёв, каждый из которых разделён на два простых блока: factorization machine block и linear compress block. interaction modules stack состоит из l одинаковых слоёв (interaction layers), причём каждый слой постепенно захватывает всё более высокие порядки взаимодействия признаков. для слоя i cтека его результаты могут содержать взаимодействия признаков произвольного порядка от 1 до 2i. авторы указывают, что чем важнее фича, тем большая размерность эмбеддинга ей выделяется. затем все эти эмбеддинги объединяют, конкатенируют и через mlp преобразуют в d-dimensional векторы. на внутреннем датасете исследователей насчитывается около семисот фич, среди которых есть не только категориальные, но и dense-фичи. их тоже пропускают через mlp, чтобы привести к одинаковым представлениям. получив эти эмбеддинги, авторы переходят к следующему слою. также авторы пишут, что используют собственную оптимизированную версию факторизационных машин. отмечается, что в большинстве современных датасетов число фичей ощутимо больше размерности эмбеддингов. поэтому они вводят определённые упрощения, которые нацелены на оптимизацию вычислительных затрат, а не на улучшение качества. но в целом fmb можно считать околодефолтными. также в статье рассказывается, как именно можно масштабировать предложенную архитектуру. во-первых, можно увеличить число слоёв в блоке interaction stack. во-вторых, допускается повышение размерности эмбеддингов, которые генерируются каждой внутренней компонентой. наконец, авторы отмечают, что можно настраивать гиперпараметры, чтобы сбалансировать производительность и качество модели. в финале авторы показывают результаты на шести общедоступных датасетах: по метрике auc модель почти везде превосходит другие решения. при этом по logloss на ряде датасетов (особенно там, где высокая вариативность) wukong не всегда занимает первое место. в целом, полученное решение действительно показывает поведение, близкое к scaling law: при увеличении числа параметров и размера датасета качество предсказаний закономерно возрастает. @recsyschannel разбор подготовил ❣ константин ширшов wukong: towards a scaling law for large-scale recommendation сегодня разбираем не новую, но важную статью на тему scaling law в recsys. в домене nlp он сводится к идее: чем больше модель, тем выше её качество. но в современных рекомендательных системах нет такой явной зависимости. мотивация авторов в том, чтобы создать архитектуру, которая, прежде всего, хорошо масштабируется по параметрам. исследователи утверждают, что достигли своих целей: 1) создали архитектуру, которая позволяет улавливать сложные взаимодействия признаков высокого порядка; 2) добились плавного масштабирования качества в зависимости от размера датасета, объёма вычислений (gflop) и ограничений по параметрам. в статье по большей части рассматривают подход sparse scaling, суть которого в том, чтобы добавить множество эмбеддингов и за счёт этого масштабироваться по числу параметров. однако это не совсем то, к чему стремятся авторы, по двум причинам. первая: если просто добавить много новых эмбеддингов, не будут улавливаться нетривиальные взаимодействия. вторая: при таком подходе не используется потенциал современных gpu, а только задействуется дополнительная видеопамять. ключевая инновация статьи — использование серии последовательно расположенных факторизационных машин (factorization machines, fmb). это как раз и позволяет учитывать нетривиальные взаимодействия высоких порядков. общую схему архитектуры можно описать так: сначала берутся dense embeddings — это преобразование всех признаков в эмбеддинги. затем следует блок interaction stack, состоящий из нескольких wukong-слоёв, каждый из которых разделён на два простых блока: factorization machine block и linear compress block. interaction modules stack состоит из l одинаковых слоёв (interaction layers), причём каждый слой постепенно захватывает всё более высокие порядки взаимодействия признаков. для слоя i cтека его результаты могут содержать взаимодействия признаков произвольного порядка от 1 до 2i. авторы указывают, что чем важнее фича, тем большая размерность эмбеддинга ей выделяется. затем все эти эмбеддинги объединяют, конкатенируют и через mlp преобразуют в d-dimensional векторы. на внутреннем датасете исследователей насчитывается около семисот фич, среди которых есть не только категориальные, но и dense-фичи. их тоже пропускают через mlp, чтобы привести к одинаковым представлениям. получив эти эмбеддинги, авторы переходят к следующему слою. также авторы пишут, что используют собственную оптимизированную версию факторизационных машин. отмечается, что в большинстве современных датасетов число фичей ощутимо больше размерности эмбеддингов. поэтому они вводят определённые упрощения, которые нацелены на оптимизацию вычислительных затрат, а не на улучшение качества. но в целом fmb можно считать околодефолтными. также в статье рассказывается, как именно можно масштабировать предложенную архитектуру. во-первых, можно увеличить число слоёв в блоке interaction stack. во-вторых, допускается повышение размерности эмбеддингов, которые генерируются каждой внутренней компонентой. наконец, авторы отмечают, что можно настраивать гиперпараметры, чтобы сбалансировать производительность и качество модели. в финале авторы показывают результаты на шести общедоступных датасетах: по метрике auc модель почти везде превосходит другие решения. при этом по logloss на ряде датасетов (особенно там, где высокая вариативность) wukong не всегда занимает первое место. в целом, полученное решение действительно показывает поведение, близкое к scaling law: при увеличении числа параметров и размера датасета качество предсказаний закономерно возрастает. @recsyschannel разбор подготовил ❣ константин ширшов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-04T07:29:01+00:00" href="./posts/80.html">2025-04-04 07:29 UTC</a></div>
      </div>
      <div class="post-body"><strong>Wukong: Towards a Scaling Law for Large-Scale Recommendation </strong><br><br>Сегодня разбираем не новую, но важную <a href="https://arxiv.org/abs/2403.02545" rel="nofollow noopener noreferrer">статью</a> на тему scaling law в RecSys. В домене NLP он сводится к идее: чем больше модель, тем выше её качество. Но в современных рекомендательных системах нет такой явной зависимости. Мотивация авторов в том, чтобы создать архитектуру, которая, прежде всего, хорошо масштабируется по параметрам. <br><br>Исследователи утверждают, что достигли своих целей: <br><br>1) создали архитектуру, которая позволяет улавливать сложные взаимодействия признаков высокого порядка; <br><br>2) добились плавного масштабирования качества в зависимости от размера датасета, объёма вычислений (GFLOP) и ограничений по параметрам.<br><br>В статье по большей части рассматривают подход sparse scaling, суть которого в том, чтобы добавить множество эмбеддингов и за счёт этого масштабироваться по числу параметров. Однако это не совсем то, к чему стремятся авторы, по двум причинам. Первая: если просто добавить много новых эмбеддингов, не будут улавливаться нетривиальные взаимодействия. Вторая: при таком подходе не используется потенциал современных GPU, а только задействуется дополнительная видеопамять.<br><br>Ключевая инновация статьи — использование серии последовательно расположенных факторизационных машин (Factorization Machines, FMB). Это как раз и позволяет учитывать нетривиальные взаимодействия высоких порядков. <br><br>Общую схему архитектуры можно описать так: сначала берутся Dense embeddings — это преобразование всех признаков в эмбеддинги. Затем следует блок Interaction Stack, состоящий из нескольких Wukong-слоёв, каждый из которых разделён на два простых блока: Factorization Machine Block и Linear Compress Block.<br><br>Interaction Modules Stack состоит из l одинаковых слоёв (interaction layers), причём каждый слой постепенно захватывает всё более высокие порядки взаимодействия признаков. Для слоя i cтека его результаты могут содержать взаимодействия признаков произвольного порядка от 1 до 2i.<br><br>Авторы указывают, что чем важнее фича, тем большая размерность эмбеддинга ей выделяется. Затем все эти эмбеддинги объединяют, конкатенируют и через MLP преобразуют в d-dimensional векторы.<br><br>На внутреннем датасете исследователей насчитывается около семисот фич, среди которых есть не только категориальные, но и Dense-фичи. Их тоже пропускают через MLP, чтобы привести к одинаковым представлениям. Получив эти эмбеддинги, авторы переходят к следующему слою.<br><br>Также авторы пишут, что используют собственную оптимизированную версию факторизационных машин. Отмечается, что в большинстве современных датасетов число фичей ощутимо больше размерности эмбеддингов. Поэтому они вводят определённые упрощения, которые нацелены на оптимизацию вычислительных затрат, а не на улучшение качества. Но в целом FMB можно считать околодефолтными.<br><br>Также в статье рассказывается, как именно можно масштабировать предложенную архитектуру. Во-первых, можно увеличить число слоёв в блоке Interaction Stack. Во-вторых, допускается повышение размерности эмбеддингов, которые генерируются каждой внутренней компонентой. Наконец, авторы отмечают, что можно настраивать гиперпараметры, чтобы сбалансировать производительность и качество модели.<br><br>В финале авторы показывают результаты на шести общедоступных датасетах: по метрике AUC модель почти везде превосходит другие решения. При этом по LogLoss на ряде датасетов (особенно там, где высокая вариативность) Wukong не всегда занимает первое место. <br><br>В целом, полученное решение действительно показывает поведение, близкое к scaling law: при увеличении числа параметров и размера датасета качество предсказаний закономерно возрастает.<br> <br>@RecSysChannel<br>Разбор подготовил <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> Константин Ширшов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/80_480.webp" srcset="../assets/media/thumbs/80_480.webp 480w, ../assets/media/80.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="80" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 934 просмотров · 11 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/80" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/80.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="79" data-search="onerec: unifying retrieve and rank with generative recommender and iterative preference alignment сегодня разбираем хайповую статью от kuaishou — второго по популярности сервиса коротких видео в китае. авторы утверждают, что создали одностадийную рекомендательную систему onerec, где генерация кандидатов и их ранжирование объединены в одной модели. архитектура модели — используют архитектуру «энкодер-декодер»: энкодер обрабатывает историю действий (на вход принимает только позитивные поведения: лайки, подписки, шеры, просмотры), а декодер генерирует всю сессию целиком. — вместо предсказания следующего айтема строит всю траекторию пользователя. — использует технику direct preference optimization (dpo), причём итеративно — модель оценивает реворд каждой сессии и постепенно его увеличивает. энкодер — это обычный трансформер с n слоями. между сессиями используют специальный токен-сепаратор. декодер устроен сложнее: в нём есть каузальный self-attention внутри сессии и cross-attention на энкодер (стандартная схема для декодеров). ffn-слой заменили на смесь экспертов. это важно, чтобы избежать «узкого горлышка» и дать модели запомнить больше деталей о хороших сессиях. такой подход увеличивает число параметров, но не требует лишних вычислений: в каждом проходе активируются только 4 эксперта из 24. за счёт этого растёт ёмкость модели без большого увеличения flops. ещё в декодере есть токены начала и конца сессии, последний предсказывается моделью. подробнее об устройстве многостадийные системы всегда зависят от качества каждой стадии: если ретривал выбирает неудачных кандидатов, даже хорошее ранжирование не исправит ситуацию. авторы предлагают заменить эту схему энкодером-декодером, который сразу генерирует сессии без деления на этапы. подход с прямой генерацией id уже использовали ранее, например, в работе tiger. но там не удалось сделать его end-to-end: модель работала как ретривал, но не заменяла ранжирование — эту стадию пришлось оставить. авторы kuaishou утверждают, что решили эту проблему. для видео они используют собственный мультимодальный эмбеддинг qarm. берётся мультимодальная llm, которая обрабатывает текст, изображения и аудио, её элайнят на рекомендательный сигнал и в конце применяют квантизацию. с квантизацией есть нюанс. обычно, как в той же tiger, применяют rq-vae для получения семантических id и построения семантических токенов. но авторы пишут, что этот метод работает плохо из-за эффекта «песочных часов»: большинство айтемов мапятся в одни и те же id, а остальные остаются невостребованными. в результате код-бук используется неэффективно. авторы предлагают заменить rq-vae на residual k-means. сначала вектор кластеризуется на k групп, затем для каждого объекта вычисляют разницу с центроидом его кластера и повторяют кластеризацию для этих разностей. этот процесс выполняется несколько раз, а в итоге получается код из нескольких id. чтобы коды использовались равномерно, делают балансировку — стараются распределять видео по кластерам примерно поровну. генерировать хотят не просто сессии, а сессии с высокой ценностью. по критериям kuaishou, сессия — это последовательность из 5–10 видео. она считается удачной, если пользователь посмотрел хотя бы 5 роликов, провёл за просмотром больше определённого времени или проявил активность: лайкнул, добавил в коллекцию, поделился. эти правила позволяют отобрать качественные сессии и использовать их в обучении. в самих сессиях каждый айтем представлен набором семантических id. что в итоге авторы пишут, что систему удалось не только разработать, но и внедрить в прод. она заменила многостадийную архитектуру, упростив процесс, и при этом увеличила время просмотра на 1,6%. результаты могли бы быть впечатляющими, однако в статье есть несколько непонятных моментов. например, неясно, есть ли в новой архитектуре учёт длинной истории. кроме того, выглядит так, будто у модели нет реактивности к дизлайкам и негативному фидбеку. @recsyschannel разбор подготовил ❣ виктор януш onerec: unifying retrieve and rank with generative recommender and iterative preference alignment сегодня разбираем хайповую статью от kuaishou — второго по популярности сервиса коротких видео в китае. авторы утверждают, что создали одностадийную рекомендательную систему onerec, где генерация кандидатов и их ранжирование объединены в одной модели. архитектура модели — используют архитектуру «энкодер-декодер»: энкодер обрабатывает историю действий (на вход принимает только позитивные поведения: лайки, подписки, шеры, просмотры), а декодер генерирует всю сессию целиком. — вместо предсказания следующего айтема строит всю траекторию пользователя. — использует технику direct preference optimization (dpo), причём итеративно — модель оценивает реворд каждой сессии и постепенно его увеличивает. энкодер — это обычный трансформер с n слоями. между сессиями используют специальный токен-сепаратор. декодер устроен сложнее: в нём есть каузальный self-attention внутри сессии и cross-attention на энкодер (стандартная схема для декодеров). ffn-слой заменили на смесь экспертов. это важно, чтобы избежать «узкого горлышка» и дать модели запомнить больше деталей о хороших сессиях. такой подход увеличивает число параметров, но не требует лишних вычислений: в каждом проходе активируются только 4 эксперта из 24. за счёт этого растёт ёмкость модели без большого увеличения flops. ещё в декодере есть токены начала и конца сессии, последний предсказывается моделью. подробнее об устройстве многостадийные системы всегда зависят от качества каждой стадии: если ретривал выбирает неудачных кандидатов, даже хорошее ранжирование не исправит ситуацию. авторы предлагают заменить эту схему энкодером-декодером, который сразу генерирует сессии без деления на этапы. подход с прямой генерацией id уже использовали ранее, например, в работе tiger . но там не удалось сделать его end-to-end: модель работала как ретривал, но не заменяла ранжирование — эту стадию пришлось оставить. авторы kuaishou утверждают, что решили эту проблему. для видео они используют собственный мультимодальный эмбеддинг qarm . берётся мультимодальная llm, которая обрабатывает текст, изображения и аудио, её элайнят на рекомендательный сигнал и в конце применяют квантизацию. с квантизацией есть нюанс. обычно, как в той же tiger, применяют rq-vae для получения семантических id и построения семантических токенов. но авторы пишут, что этот метод работает плохо из-за эффекта «песочных часов»: большинство айтемов мапятся в одни и те же id, а остальные остаются невостребованными. в результате код-бук используется неэффективно. авторы предлагают заменить rq-vae на residual k-means. сначала вектор кластеризуется на k групп, затем для каждого объекта вычисляют разницу с центроидом его кластера и повторяют кластеризацию для этих разностей. этот процесс выполняется несколько раз, а в итоге получается код из нескольких id. чтобы коды использовались равномерно, делают балансировку — стараются распределять видео по кластерам примерно поровну. генерировать хотят не просто сессии, а сессии с высокой ценностью. по критериям kuaishou, сессия — это последовательность из 5–10 видео. она считается удачной, если пользователь посмотрел хотя бы 5 роликов, провёл за просмотром больше определённого времени или проявил активность: лайкнул, добавил в коллекцию, поделился. эти правила позволяют отобрать качественные сессии и использовать их в обучении. в самих сессиях каждый айтем представлен набором семантических id. что в итоге авторы пишут, что систему удалось не только разработать, но и внедрить в прод. она заменила многостадийную архитектуру, упростив процесс, и при этом увеличила время просмотра на 1,6%. результаты могли бы быть впечатляющими, однако в статье есть несколько непонятных моментов. например, неясно, есть ли в новой архитектуре учёт длинной истории. кроме того, выглядит так, будто у модели нет реактивности к дизлайкам и негативному фидбеку. @recsyschannel разбор подготовил ❣ виктор януш">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-26T09:04:43+00:00" href="./posts/79.html">2025-03-26 09:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment </strong><br><br>Сегодня разбираем хайповую <a href="https://arxiv.org/abs/2502.18965" rel="nofollow noopener noreferrer">статью</a> от Kuaishou — второго по популярности сервиса коротких видео в Китае. Авторы утверждают, что создали одностадийную рекомендательную систему OneRec, где генерация кандидатов и их ранжирование объединены в одной модели.<br><br><strong>Архитектура модели</strong><br><br>— Используют архитектуру «энкодер-декодер»: энкодер обрабатывает историю действий (на вход принимает только позитивные поведения: лайки, подписки, шеры, просмотры), а декодер генерирует всю сессию целиком.<br>— Вместо предсказания следующего айтема  строит всю траекторию пользователя.<br>— Использует технику Direct Preference Optimization (DPO), причём итеративно — модель оценивает реворд каждой сессии и постепенно его увеличивает.<br><br>Энкодер — это обычный трансформер с N слоями. Между сессиями используют специальный токен-сепаратор. <br><br>Декодер устроен сложнее: в нём есть каузальный self-attention внутри сессии и cross-attention на энкодер (стандартная схема для декодеров). FFN-слой заменили на смесь экспертов. Это важно, чтобы избежать «узкого горлышка» и дать модели запомнить больше деталей о хороших сессиях. Такой подход увеличивает число параметров, но не требует лишних вычислений: в каждом проходе активируются только 4 эксперта из 24. За счёт этого растёт ёмкость модели без большого увеличения FLOPs. Ещё в декодере есть токены начала и конца сессии, последний предсказывается моделью.<br><br><strong>Подробнее об устройстве</strong><br><br>Многостадийные системы всегда зависят от качества каждой стадии: если ретривал выбирает неудачных кандидатов, даже хорошее ранжирование не исправит ситуацию. Авторы предлагают заменить эту схему энкодером-декодером, который сразу генерирует сессии без деления на этапы.<br><br>Подход с прямой генерацией ID уже использовали ранее, например, в работе <a href="https://arxiv.org/abs/2407.02095" rel="nofollow noopener noreferrer">TIGER</a>. Но там не удалось сделать его end-to-end: модель работала как ретривал, но не заменяла ранжирование — эту стадию пришлось оставить. Авторы Kuaishou утверждают, что решили эту проблему.<br><br>Для видео они используют собственный мультимодальный эмбеддинг <a href="https://arxiv.org/abs/2411.11739" rel="nofollow noopener noreferrer">QARM</a>. Берётся мультимодальная LLM, которая обрабатывает текст, изображения и аудио, её элайнят на рекомендательный сигнал и в конце применяют квантизацию.<br><br>С квантизацией есть нюанс. Обычно, как в той же TIGER, применяют RQ-VAE для получения семантических ID и построения семантических токенов. Но авторы пишут, что этот метод работает плохо из-за эффекта «песочных часов»: большинство айтемов мапятся в одни и те же ID, а остальные остаются невостребованными. В результате код-бук используется неэффективно.<br><br>Авторы предлагают заменить RQ-VAE на residual K-Means. Сначала вектор кластеризуется на K групп, затем для каждого объекта вычисляют разницу с центроидом его кластера и повторяют кластеризацию для этих разностей. Этот процесс выполняется несколько раз, а в итоге получается код из нескольких ID. Чтобы коды использовались равномерно, делают балансировку — стараются распределять видео по кластерам примерно поровну.<br><br>Генерировать хотят не просто сессии, а сессии с высокой ценностью. По критериям Kuaishou, сессия — это последовательность из 5–10 видео. Она считается удачной, если пользователь посмотрел хотя бы 5 роликов, провёл за просмотром больше определённого времени или проявил активность: лайкнул, добавил в коллекцию, поделился. Эти правила позволяют отобрать качественные сессии и использовать их в обучении. В самих сессиях каждый айтем представлен набором семантических ID.<br><br><strong>Что в итоге</strong><br><br>Авторы пишут, что систему удалось не только разработать, но и внедрить в прод. Она заменила многостадийную архитектуру, упростив процесс, и при этом увеличила время просмотра на 1,6%.<br><br>Результаты могли бы быть впечатляющими, однако в статье есть несколько непонятных моментов. Например, неясно, есть ли в новой архитектуре учёт длинной истории. Кроме того, выглядит так, будто у модели нет реактивности к дизлайкам и негативному фидбеку.<br><br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Виктор Януш<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/79_480.webp" srcset="../assets/media/thumbs/79_480.webp 480w, ../assets/media/79.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="79" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 336 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/79" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/79.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="78" data-search="beyond item dissimilarities: diversifying by intent in recommender systems сегодня разбираем статью, в которой предлагается новый подход к диверсификации рекомендаций, основанный на понимании пользовательских интентов. авторы хотят решить проблему разнообразия контента, предлагаемого пользователям. для этого они предлагают учитывать при формировании рекомендаций пользовательские интенты, а не ограничиваться только схожестью юзер-айтемов. намерения пользователей могут меняться в зависимости от дня недели, времени суток и контекста — например, они могут проявлять интерес к спорту, учёбе или отдыху. учитывая эти факторы, можно сделать выдачу более разнообразной и релевантной. в работе предложен фреймворк, который накладывается на существующую рекомендательную систему. авторы описывают его лишь концептуально, не уточняя, чем моделировали распределения. идея в том, чтобы перейти от p(item | user) к p(item | user, intent) = p(item | user) * p(intent | user, item) / p(intent | user). от неё берут матожидание по априорной p(intent | user), получают вероятность того, что айтем подходит юзеру с учётом всех возможных интентов. это значение возводится в степень γ (гиперпараметр) и умножается на скор ранжирующей мод ели. таким образом учитывается как user-item-релевантность, так и intent-aware-релевантность, формируя итоговый скор рекомендации. 1) выбираем айтем с наибольшим скором и ставим его на первую позицию. дальше повторяем процесс. k+1) переходя от позиции “k” к “k+1”, считаем, что k-ый айтем не подошёл пользователю. с учётом этого обновляем распределение интентов по теореме байеса, находя апостериорное распределение. после этого пересчитываем скоры, но теперь матожидаем не по априорному, а по апостериорному распределению. этот пересчёт снижает вероятность однотипных рекомендаций и добавляет разнообразие в выдачу. @recsyschannel разбор подготовил ❣ сергей макеев beyond item dissimilarities: diversifying by intent in recommender systems сегодня разбираем статью , в которой предлагается новый подход к диверсификации рекомендаций, основанный на понимании пользовательских интентов. авторы хотят решить проблему разнообразия контента, предлагаемого пользователям. для этого они предлагают учитывать при формировании рекомендаций пользовательские интенты, а не ограничиваться только схожестью юзер-айтемов. намерения пользователей могут меняться в зависимости от дня недели, времени суток и контекста — например, они могут проявлять интерес к спорту, учёбе или отдыху. учитывая эти факторы, можно сделать выдачу более разнообразной и релевантной. в работе предложен фреймворк, который накладывается на существующую рекомендательную систему. авторы описывают его лишь концептуально, не уточняя, чем моделировали распределения. идея в том, чтобы перейти от p(item | user) к p(item | user, intent) = p(item | user) * p(intent | user, item) / p(intent | user) . от неё берут матожидание по априорной p(intent | user) , получают вероятность того, что айтем подходит юзеру с учётом всех возможных интентов. это значение возводится в степень γ (гиперпараметр) и умножается на скор ранжирующей мод ели. таким образом учитывается как user-item-релевантность, так и intent-aware-релевантность, формируя итоговый скор рекомендации. 1) выбираем айтем с наибольшим скором и ставим его на первую позицию. дальше повторяем процесс. k+1) переходя от позиции “k” к “k+1”, считаем, что k-ый айтем не подошёл пользователю. с учётом этого обновляем распределение интентов по теореме байеса, находя апостериорное распределение. после этого пересчитываем скоры, но теперь матожидаем не по априорному, а по апостериорному распределению. этот пересчёт снижает вероятность однотипных рекомендаций и добавляет разнообразие в выдачу. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-19T10:00:01+00:00" href="./posts/78.html">2025-03-19 10:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2405.12327" rel="nofollow noopener noreferrer">статью</a>, в которой предлагается новый подход к диверсификации рекомендаций, основанный на понимании пользовательских интентов.<br><br>Авторы хотят решить проблему разнообразия контента, предлагаемого пользователям. Для этого они предлагают учитывать при формировании рекомендаций пользовательские интенты, а не ограничиваться только схожестью юзер-айтемов. Намерения пользователей могут меняться в зависимости от дня недели, времени суток и контекста — например, они могут проявлять интерес к спорту, учёбе или отдыху. Учитывая эти факторы, можно сделать выдачу более разнообразной и релевантной.<br><br>В работе предложен фреймворк, который накладывается на существующую рекомендательную систему. Авторы описывают его лишь концептуально, не уточняя, чем моделировали распределения.<br><br>Идея в том, чтобы перейти от <em>p(item | user)</em> к <em>p(item | user, intent) = p(item | user) * p(intent | user, item) / p(intent | user)</em>. От неё берут матожидание по априорной <em>p(intent | user)</em>, получают вероятность того, что айтем подходит юзеру с учётом всех возможных интентов.<br><br>Это значение возводится в степень γ (гиперпараметр) и умножается на скор ранжирующей мод ели. Таким образом учитывается как user-item-релевантность, так и intent-aware-релевантность, формируя итоговый скор рекомендации.<br><br>1) Выбираем айтем с наибольшим скором и ставим его на первую позицию. Дальше повторяем процесс.<br><br>k+1) Переходя от позиции “k” к “k+1”, считаем, что k-ый айтем не подошёл пользователю. С учётом этого обновляем распределение интентов по теореме Байеса, находя апостериорное распределение. После этого пересчитываем скоры, но теперь матожидаем не по априорному, а по апостериорному распределению.<br><br>Этот пересчёт снижает вероятность однотипных рекомендаций и добавляет разнообразие в выдачу.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/78_480.webp" srcset="../assets/media/thumbs/78_480.webp 480w, ../assets/media/78.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 287 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/78" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/78.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="77" data-search="fuxi-𝛼: scaling recommendation model with feature interaction enhanced transformer сегодня разбираем статью от huawei noah’s ark lab об их новой рекомендательной модели fuxi-𝛼. hstu — одно из главных достижений индустрии за прошлый год. среди важных архитектурных решений, которые вошли в эту модель, можно выделить отказ от feed-forward network (ffn) блока и модификацию attention, куда, помимо прочего, добавили дополнительную компоненту для более явного учёта позиционной информации. авторы fuxi-𝛼 утверждают, что без ffn-блока очень сложно качественно учитывать неявное (implicit) взаимодействие между признаками. более того, временную и позиционную информацию они считают настолько важной, что для корректной работы с ними недостаточно простой правки attention. основных изменений в модели fuxi-𝛼 два: 1. модификация hstu-attention. его заменили на fuxi-блоки с adaptive multi-channel self-attention (ams). они кодируют семантическую информацию о последовательностях стандартным образом, беря за основу hstu-модификацию attention. а вот позиционная и темпоральная информация в новом блоке обрабатываются отдельно, в специальных обучаемых матрицах. подробнее о том, как это работает, — на центральной схеме. 2. возвращение ffn, от которого отказались в hstu. при этом, добавили гейтинг и residual connections почти в каждый слой архитектуры, чтобы градиенты могли течь по любому маршруту (правая схема). для проверки получившейся модели авторы выбрали публичные датасеты movielens (1m и 20m), kuairand и собственный индустриальный датасет. fuxi-𝛼 показывала лучшие результаты по всем метрикам и хороший скейлинг как на small, так и на large моделях. например, на kuairand ndcg@50 fuxi-𝛼-large выше hstu-large на 9%, а hitrate@50 — на 7%. а согласно онлайн-экспериментам в huawei music, внедрение fuxi-𝛼 позволило вырастить среднее время прослушивания на 5,1%. @recsyschannel разбор подготовил ❣ руслан кулиев fuxi- 𝛼 : scaling recommendation model with feature interaction enhanced transformer сегодня разбираем статью от huawei noah’s ark lab об их новой рекомендательной модели fuxi-𝛼. hstu — одно из главных достижений индустрии за прошлый год. среди важных архитектурных решений, которые вошли в эту модель, можно выделить отказ от feed-forward network (ffn) блока и модификацию attention, куда, помимо прочего, добавили дополнительную компоненту для более явного учёта позиционной информации. авторы fuxi-𝛼 утверждают, что без ffn-блока очень сложно качественно учитывать неявное (implicit) взаимодействие между признаками. более того, временную и позиционную информацию они считают настолько важной, что для корректной работы с ними недостаточно простой правки attention. основных изменений в модели fuxi-𝛼 два: 1. модификация hstu-attention. его заменили на fuxi-блоки с adaptive multi-channel self-attention (ams). они кодируют семантическую информацию о последовательностях стандартным образом, беря за основу hstu-модификацию attention. а вот позиционная и темпоральная информация в новом блоке обрабатываются отдельно, в специальных обучаемых матрицах. подробнее о том, как это работает, — на центральной схеме. 2. возвращение ffn, от которого отказались в hstu. при этом, добавили гейтинг и residual connections почти в каждый слой архитектуры, чтобы градиенты могли течь по любому маршруту (правая схема). для проверки получившейся модели авторы выбрали публичные датасеты movielens (1m и 20m), kuairand и собственный индустриальный датасет. fuxi-𝛼 показывала лучшие результаты по всем метрикам и хороший скейлинг как на small, так и на large моделях. например, на kuairand ndcg@50 fuxi-𝛼-large выше hstu-large на 9%, а hitrate@50 — на 7%. а согласно онлайн-экспериментам в huawei music, внедрение fuxi-𝛼 позволило вырастить среднее время прослушивания на 5,1%. @recsyschannel разбор подготовил ❣ руслан кулиев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-14T08:07:03+00:00" href="./posts/77.html">2025-03-14 08:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>FuXi-</strong>𝛼<strong>: Scaling Recommendation Model with Feature Interaction Enhanced Transformer</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2502.03036" rel="nofollow noopener noreferrer">статью</a> от Huawei Noah’s Ark Lab об их новой рекомендательной модели FuXi-𝛼.<br><br><a href="https://t.me/RecSysChannel/48" rel="nofollow noopener noreferrer">HSTU</a> — одно из главных достижений индустрии за прошлый год. Среди важных архитектурных решений, которые вошли в эту модель, можно выделить отказ от Feed-Forward Network (FFN) блока и модификацию attention, куда, помимо прочего, добавили дополнительную компоненту для более явного учёта позиционной информации. <br><br>Авторы FuXi-𝛼 утверждают, что без FFN-блока очень сложно качественно учитывать неявное (implicit) взаимодействие между признаками. Более того, временную и позиционную информацию они считают настолько важной, что для корректной работы с ними недостаточно простой правки attention.<br><br>Основных изменений в модели FuXi-𝛼 два:<br><br><strong>1. Модификация HSTU-attention. </strong>Его заменили на FuXi-блоки с Adaptive Multi-channel Self-attention (AMS). Они кодируют семантическую информацию о последовательностях стандартным образом, беря за основу HSTU-модификацию attention. А вот позиционная и темпоральная информация в новом блоке обрабатываются отдельно, в специальных обучаемых матрицах. Подробнее о том, как это работает, — на центральной схеме. <br><br><strong>2. Возвращение FFN, от которого отказались в HSTU. </strong>При этом, добавили гейтинг и residual connections почти в каждый слой архитектуры, чтобы градиенты могли течь по любому маршруту (правая схема).<br><br>Для проверки получившейся модели авторы выбрали публичные датасеты MovieLens (1M и 20M), KuaiRand и собственный индустриальный датасет. FuXi-𝛼 показывала лучшие результаты по всем метрикам и хороший скейлинг как на small, так и на large моделях. Например, на KuaiRand NDCG@50 FuXi-𝛼-Large выше HSTU-large на 9%, а HitRate@50 — на 7%. А согласно онлайн-экспериментам в Huawei Music, внедрение FuXi-𝛼 позволило вырастить среднее время прослушивания на 5,1%.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji>  Руслан Кулиев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/77_480.webp" srcset="../assets/media/thumbs/77_480.webp 480w, ../assets/media/77.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 659 просмотров · 12 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/77" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/77.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="76" data-search="from features to transformers: redefining ranking for scalable impact сегодня разбираем статью от linkedin, в которой фёдор борисюк и команда делятся результатами внедрения hstu-подобной модели. публикация интересна тем, что объединяет ключевые тренды 2024 года: идею target-aware из hstu и использование semantic ids из tiger. в работе обозначено несколько целей: 1) избавиться от большого количества hand-crafted фичей авторы хотят показать, что их небольшая модель с базовыми фичами может переиграть сложные подходы и дать высокие результаты в ранжировании. 2) использовать list-wise подходы вместо point-wise исторически ранжирующие модели linkedin использовали point-wise подход, то есть оценивали каждый айтем независимо от прочих. в статье утверждается, что вероятность клика на конкретный айтем зависит не только от него самого, но и от других совместно показанных айтемов. из-за этого авторы отказываются от point-wise и переходят на list-wise ранжирование. ещё эта мера помогла им избавиться от rule-based бизнес-логики в обработке выходов моделей. 3) scaling law в рекомендациях если в nlp и cv scaling law продемонстрирован многократно, то в recsys — лишь в единицах исследований. авторы ссылаются на работы, которые показывают, что при правильном увеличении модели можно увидеть scaling law (hstu, wukong, hllm). им самим удалось добиться этого, используя всего семь ключевых фичей, основанных на разных id. решение, предложенное в статье, — модель под названием linkedin generative recommender (ligr), которая собирает воедино перечисленные доработки. до ligr linkedin использовали в проде для ранжирования две отдельные point-wise модели, которые предсказывали несколько типов взаимодействия: click tower — вероятность клика и длительного просмотра поста; contributions tower — лайки, комментарии и репосты. все фичи проходили через fully-connected слои, выдавая на выходе вероятности событий. в ligr авторы полностью отходят от предыдущего подхода и используют sequential-архитектуру. на вход модели подаётся хронологическая история интеракций пользователя в виде последовательности токенов, где токен — агрегация представлений признаков айтема из истории пользователя. на выходе для каждого айтема модель выдаёт вектор, где каждая компонента отвечает за отдельный тип взаимодействия. вдохновляясь hstu, авторы используют target-aware постановку (в истории пользователя явно представлены не только айтемы, но соответствующие им действия совершенные пользователем) и gated attention — измененный механизм внимания с использованием гейтинга. также авторы предлагают использовать дополнительные attention блоки, работающие только в рамках сессий. идея, заимствованная из sng (session-based neural graphs), заключается в том, что внутри одной сессии все айтемы могут учитывать друг друга для уточнения финальных скоров. эксперименты и эффекты один из главных результатов — исследователям удалось показать, что scaling law действительно работает. они попробовали масштабировать практически все аспекты: флопсы, размеры эмбеддингов, размеры трансформера. в сравнении с hstu их подход показал лучшее качество. ещё одно наблюдение — простое увеличение длины sequence length при фиксировании других параметров дало ощутимый рост метрик. один из самых заметных эффектов от внедрения модели — рост daily active users (dau) на 0,27%. в качестве итога можно сказать, что у авторов получилось полностью отказаться rule-based подхода, основанного на устаревших принципах. а наибольший прирост производительности был получен за счёт скейлинга. @recsyschannel разбор подготовил ❣ владимир байкалов from features to transformers: redefining ranking for scalable impact сегодня разбираем статью от linkedin, в которой фёдор борисюк и команда делятся результатами внедрения hstu-подобной модели. публикация интересна тем, что объединяет ключевые тренды 2024 года: идею target-aware из hstu и использование semantic ids из tiger. в работе обозначено несколько целей: 1) избавиться от большого количества hand-crafted фичей авторы хотят показать, что их небольшая модель с базовыми фичами может переиграть сложные подходы и дать высокие результаты в ранжировании. 2) использовать list-wise подходы вместо point-wise исторически ранжирующие модели linkedin использовали point-wise подход, то есть оценивали каждый айтем независимо от прочих. в статье утверждается, что вероятность клика на конкретный айтем зависит не только от него самого, но и от других совместно показанных айтемов. из-за этого авторы отказываются от point-wise и переходят на list-wise ранжирование. ещё эта мера помогла им избавиться от rule-based бизнес-логики в обработке выходов моделей. 3) scaling law в рекомендациях если в nlp и cv scaling law продемонстрирован многократно, то в recsys — лишь в единицах исследований. авторы ссылаются на работы, которые показывают, что при правильном увеличении модели можно увидеть scaling law ( hstu , wukong , hllm ). им самим удалось добиться этого, используя всего семь ключевых фичей, основанных на разных id. решение, предложенное в статье, — модель под названием linkedin generative recommender (ligr) , которая собирает воедино перечисленные доработки. до ligr linkedin использовали в проде для ранжирования две отдельные point-wise модели, которые предсказывали несколько типов взаимодействия: click tower — вероятность клика и длительного просмотра поста; contributions tower — лайки, комментарии и репосты. все фичи проходили через fully-connected слои, выдавая на выходе вероятности событий. в ligr авторы полностью отходят от предыдущего подхода и используют sequential-архитектуру. на вход модели подаётся хронологическая история интеракций пользователя в виде последовательности токенов, где токен — агрегация представлений признаков айтема из истории пользователя. на выходе для каждого айтема модель выдаёт вектор, где каждая компонента отвечает за отдельный тип взаимодействия. вдохновляясь hstu, авторы используют target-aware постановку (в истории пользователя явно представлены не только айтемы, но соответствующие им действия совершенные пользователем) и gated attention — измененный механизм внимания с использованием гейтинга. также авторы предлагают использовать дополнительные attention блоки, работающие только в рамках сессий. идея, заимствованная из sng ( session-based neural graphs ), заключается в том, что внутри одной сессии все айтемы могут учитывать друг друга для уточнения финальных скоров. эксперименты и эффекты один из главных результатов — исследователям удалось показать, что scaling law действительно работает. они попробовали масштабировать практически все аспекты: флопсы, размеры эмбеддингов, размеры трансформера. в сравнении с hstu их подход показал лучшее качество. ещё одно наблюдение — простое увеличение длины sequence length при фиксировании других параметров дало ощутимый рост метрик. один из самых заметных эффектов от внедрения модели — рост daily active users (dau) на 0,27%. в качестве итога можно сказать, что у авторов получилось полностью отказаться rule-based подхода, основанного на устаревших принципах. а наибольший прирост производительности был получен за счёт скейлинга. @recsyschannel разбор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-07T08:00:04+00:00" href="./posts/76.html">2025-03-07 08:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>From Features to Transformers: Redefining Ranking for Scalable Impact</strong> <br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2502.03417" rel="nofollow noopener noreferrer">статью</a> от Linkedin, в которой Фёдор Борисюк и команда делятся результатами внедрения HSTU-подобной модели. Публикация интересна тем, что объединяет ключевые тренды 2024 года: идею target-aware из HSTU и использование semantic IDs из TIGER.<br><br>В работе обозначено несколько целей:<br><br><strong>1) Избавиться от большого количества hand-crafted фичей</strong><br>Авторы хотят показать, что их небольшая модель с базовыми фичами может переиграть сложные подходы и дать высокие результаты в ранжировании.<br><br><strong>2) Использовать list-wise подходы вместо point-wise</strong><br>Исторически ранжирующие модели Linkedin использовали point-wise подход, то есть оценивали каждый айтем независимо от прочих. В статье утверждается, что вероятность клика на конкретный айтем зависит не только от него самого, но и от других совместно показанных айтемов. Из-за этого авторы отказываются от point-wise и переходят на list-wise ранжирование. Ещё эта мера помогла им избавиться от rule-based бизнес-логики в обработке выходов моделей.<br><br><strong>3) Scaling law в рекомендациях</strong><br>Если в NLP и CV scaling law продемонстрирован многократно, то в RecSys — лишь в единицах исследований. Авторы ссылаются на работы, которые показывают, что при правильном увеличении модели можно увидеть scaling law (<a href="https://arxiv.org/abs/2402.17152v1" rel="nofollow noopener noreferrer">HSTU</a>, <a href="https://arxiv.org/abs/2403.02545" rel="nofollow noopener noreferrer">Wukong</a>, <a href="https://arxiv.org/abs/2409.12740" rel="nofollow noopener noreferrer">HLLM</a>). Им самим удалось добиться этого, используя всего семь ключевых фичей, основанных на разных ID.<br><br>Решение, предложенное в статье, — модель под названием <strong>LinkedIn Generative Recommender (LiGR)</strong>, которая собирает воедино перечисленные доработки. <br><br>До LiGR LinkedIn использовали в проде для ранжирования две отдельные point-wise модели, которые предсказывали несколько типов взаимодействия: Click Tower — вероятность клика и длительного просмотра поста; Contributions Tower — лайки, комментарии и репосты. Все фичи проходили через fully-connected слои, выдавая на выходе вероятности событий.<br><br>В LiGR авторы полностью отходят от предыдущего подхода и используют sequential-архитектуру. На вход модели подаётся хронологическая история интеракций пользователя в виде последовательности токенов, где токен — агрегация представлений признаков айтема из истории пользователя. На выходе для каждого айтема модель выдаёт вектор, где каждая компонента отвечает за отдельный тип взаимодействия.<br><br>Вдохновляясь HSTU, авторы используют target-aware постановку (в истории пользователя явно представлены не только айтемы, но соответствующие им действия совершенные пользователем) и gated attention — измененный механизм внимания с использованием гейтинга.<br><br>Также авторы предлагают использовать дополнительные attention блоки, работающие только в рамках сессий. Идея, заимствованная из SNG (<a href="https://arxiv.org/abs/1811.00855" rel="nofollow noopener noreferrer">Session-based Neural Graphs</a>), заключается в том, что внутри одной сессии все айтемы могут учитывать друг друга для уточнения финальных скоров.<br><br><strong>Эксперименты и эффекты </strong><br><br>Один из главных результатов — исследователям удалось показать, что scaling law действительно работает. Они попробовали масштабировать практически все аспекты: флопсы, размеры эмбеддингов, размеры трансформера. В сравнении с HSTU их подход показал лучшее качество.<br><br>Ещё одно наблюдение — простое увеличение длины sequence length при фиксировании других параметров дало ощутимый рост метрик. <br><br>Один из самых заметных эффектов от внедрения модели — рост Daily Active Users (DAU) на 0,27%.<br><br>В качестве итога можно сказать, что у авторов получилось полностью отказаться rule-based подхода, основанного на устаревших принципах. А наибольший прирост производительности был получен за счёт скейлинга. <br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/76_480.webp" srcset="../assets/media/thumbs/76_480.webp 480w, ../assets/media/76.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="76" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 699 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/76" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/76.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 141, "media": [{"kind": "photo", "path": "../assets/media/141.jpg", "thumb": "../assets/media/thumbs/141_480.webp", "size": 86246, "mime": "image/jpeg", "name": null}]}, {"id": 138, "media": [{"kind": "photo", "path": "../assets/media/138.jpg", "thumb": "../assets/media/thumbs/138_480.webp", "size": 191970, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/139.jpg", "thumb": "../assets/media/thumbs/139_480.webp", "size": 265644, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/140.jpg", "thumb": "../assets/media/thumbs/140_480.webp", "size": 244044, "mime": "image/jpeg", "name": null}]}, {"id": 137, "media": [{"kind": "photo", "path": "../assets/media/137.jpg", "thumb": "../assets/media/thumbs/137_480.webp", "size": 59789, "mime": "image/jpeg", "name": null}]}, {"id": 133, "media": [{"kind": "photo", "path": "../assets/media/133.jpg", "thumb": "../assets/media/thumbs/133_480.webp", "size": 32591, "mime": "image/jpeg", "name": null}]}, {"id": 123, "media": [{"kind": "photo", "path": "../assets/media/123.jpg", "thumb": "../assets/media/thumbs/123_480.webp", "size": 129142, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/124.jpg", "thumb": "../assets/media/thumbs/124_480.webp", "size": 164854, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/125.jpg", "thumb": "../assets/media/thumbs/125_480.webp", "size": 159231, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/126.jpg", "thumb": "../assets/media/thumbs/126_480.webp", "size": 118577, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/127.jpg", "thumb": "../assets/media/thumbs/127_480.webp", "size": 97703, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/128.jpg", "thumb": "../assets/media/thumbs/128_480.webp", "size": 122080, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/129.jpg", "thumb": "../assets/media/thumbs/129_480.webp", "size": 116180, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/130.jpg", "thumb": "../assets/media/thumbs/130_480.webp", "size": 129100, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/131.jpg", "thumb": "../assets/media/thumbs/131_480.webp", "size": 88288, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/132.jpg", "thumb": "../assets/media/thumbs/132_480.webp", "size": 127619, "mime": "image/jpeg", "name": null}]}, {"id": 122, "media": [{"kind": "photo", "path": "../assets/media/122.jpg", "thumb": "../assets/media/thumbs/122_480.webp", "size": 98435, "mime": "image/jpeg", "name": null}]}, {"id": 121, "media": []}, {"id": 120, "media": [{"kind": "video", "path": "../assets/media/120_video.mp4", "thumb": null, "size": 2748838, "mime": "video/mp4", "name": "video.mp4"}]}, {"id": 119, "media": [{"kind": "photo", "path": "../assets/media/119.jpg", "thumb": "../assets/media/thumbs/119_480.webp", "size": 78858, "mime": "image/jpeg", "name": null}]}, {"id": 117, "media": [{"kind": "photo", "path": "../assets/media/117.jpg", "thumb": "../assets/media/thumbs/117_480.webp", "size": 185994, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/118.jpg", "thumb": "../assets/media/thumbs/118_480.webp", "size": 185047, "mime": "image/jpeg", "name": null}]}, {"id": 107, "media": [{"kind": "photo", "path": "../assets/media/107.jpg", "thumb": "../assets/media/thumbs/107_480.webp", "size": 83744, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/108.jpg", "thumb": "../assets/media/thumbs/108_480.webp", "size": 143161, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/109.jpg", "thumb": "../assets/media/thumbs/109_480.webp", "size": 137497, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/110.jpg", "thumb": "../assets/media/thumbs/110_480.webp", "size": 141609, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/111.jpg", "thumb": "../assets/media/thumbs/111_480.webp", "size": 155477, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/112.jpg", "thumb": "../assets/media/thumbs/112_480.webp", "size": 117699, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/113.jpg", "thumb": "../assets/media/thumbs/113_480.webp", "size": 158463, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/114.jpg", "thumb": "../assets/media/thumbs/114_480.webp", "size": 135012, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/115.jpg", "thumb": "../assets/media/thumbs/115_480.webp", "size": 128214, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/116.jpg", "thumb": "../assets/media/thumbs/116_480.webp", "size": 72779, "mime": "image/jpeg", "name": null}]}, {"id": 105, "media": [{"kind": "photo", "path": "../assets/media/105.jpg", "thumb": "../assets/media/thumbs/105_480.webp", "size": 124573, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/106.jpg", "thumb": "../assets/media/thumbs/106_480.webp", "size": 148144, "mime": "image/jpeg", "name": null}]}, {"id": 104, "media": [{"kind": "photo", "path": "../assets/media/104.jpg", "thumb": "../assets/media/thumbs/104_480.webp", "size": 199731, "mime": "image/jpeg", "name": null}]}, {"id": 103, "media": [{"kind": "photo", "path": "../assets/media/103.jpg", "thumb": "../assets/media/thumbs/103_480.webp", "size": 39506, "mime": "image/jpeg", "name": null}]}, {"id": 102, "media": []}, {"id": 101, "media": [{"kind": "photo", "path": "../assets/media/101.jpg", "thumb": "../assets/media/thumbs/101_480.webp", "size": 22457, "mime": "image/jpeg", "name": null}]}, {"id": 100, "media": [{"kind": "photo", "path": "../assets/media/100.jpg", "thumb": "../assets/media/thumbs/100_480.webp", "size": 82316, "mime": "image/jpeg", "name": null}]}, {"id": 99, "media": [{"kind": "photo", "path": "../assets/media/99.jpg", "thumb": "../assets/media/thumbs/99_480.webp", "size": 95209, "mime": "image/jpeg", "name": null}]}, {"id": 98, "media": [{"kind": "video", "path": "../assets/media/98_IMG_6498.MOV.mov", "thumb": null, "size": 2315980, "mime": "video/quicktime", "name": "IMG_6498.MOV"}]}, {"id": 95, "media": [{"kind": "photo", "path": "../assets/media/95.jpg", "thumb": "../assets/media/thumbs/95_480.webp", "size": 183473, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/96.jpg", "thumb": "../assets/media/thumbs/96_480.webp", "size": 197661, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/97.jpg", "thumb": "../assets/media/thumbs/97_480.webp", "size": 155633, "mime": "image/jpeg", "name": null}]}, {"id": 90, "media": [{"kind": "video", "path": "../assets/media/90_____.mp4", "thumb": null, "size": 3092231, "mime": "video/mp4", "name": "____.mp4"}, {"kind": "video", "path": "../assets/media/91_IMG_8354__1_.mp4", "thumb": null, "size": 3704672, "mime": "video/mp4", "name": "IMG_8354__1_.mp4"}, {"kind": "photo", "path": "../assets/media/92.jpg", "thumb": "../assets/media/thumbs/92_480.webp", "size": 219530, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/93.jpg", "thumb": "../assets/media/thumbs/93_480.webp", "size": 93502, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/94.jpg", "thumb": "../assets/media/thumbs/94_480.webp", "size": 102958, "mime": "image/jpeg", "name": null}]}, {"id": 86, "media": [{"kind": "photo", "path": "../assets/media/86.jpg", "thumb": "../assets/media/thumbs/86_480.webp", "size": 181722, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/87.jpg", "thumb": "../assets/media/thumbs/87_480.webp", "size": 207106, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/88.jpg", "thumb": "../assets/media/thumbs/88_480.webp", "size": 124422, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/89.jpg", "thumb": "../assets/media/thumbs/89_480.webp", "size": 113987, "mime": "image/jpeg", "name": null}]}, {"id": 83, "media": [{"kind": "photo", "path": "../assets/media/83.jpg", "thumb": "../assets/media/thumbs/83_480.webp", "size": 253792, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/84.jpg", "thumb": "../assets/media/thumbs/84_480.webp", "size": 170072, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/85.jpg", "thumb": "../assets/media/thumbs/85_480.webp", "size": 249183, "mime": "image/jpeg", "name": null}]}, {"id": 82, "media": [{"kind": "photo", "path": "../assets/media/82.jpg", "thumb": "../assets/media/thumbs/82_480.webp", "size": 125878, "mime": "image/jpeg", "name": null}]}, {"id": 81, "media": []}, {"id": 80, "media": [{"kind": "photo", "path": "../assets/media/80.jpg", "thumb": "../assets/media/thumbs/80_480.webp", "size": 77809, "mime": "image/jpeg", "name": null}]}, {"id": 79, "media": [{"kind": "photo", "path": "../assets/media/79.jpg", "thumb": "../assets/media/thumbs/79_480.webp", "size": 142648, "mime": "image/jpeg", "name": null}]}, {"id": 78, "media": [{"kind": "photo", "path": "../assets/media/78.jpg", "thumb": "../assets/media/thumbs/78_480.webp", "size": 31834, "mime": "image/jpeg", "name": null}]}, {"id": 77, "media": [{"kind": "photo", "path": "../assets/media/77.jpg", "thumb": "../assets/media/thumbs/77_480.webp", "size": 75612, "mime": "image/jpeg", "name": null}]}, {"id": 76, "media": [{"kind": "photo", "path": "../assets/media/76.jpg", "thumb": "../assets/media/thumbs/76_480.webp", "size": 73090, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "Рекомендательная [RecSys Channel]", "username": "RecSysChannel", "channel": "RecSysChannel", "last_sync_utc": "2026-02-03T16:34:48Z", "posts_count": 104, "last_seen_message_id": 217, "stats": {"new": 104, "updated": 4, "media_downloaded": 104}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
