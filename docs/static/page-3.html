<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Рекомендательная [RecSys Channel] — статическая версия (стр. 3/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-04T08%3A06%3A57Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-04T08%3A06%3A57Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-04T08%3A06%3A57Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Рекомендательная [RecSys Channel]</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+3NrSk0BmQ-QzZTMy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="66" data-search="какие рексис-тренды будут развивать в яндексе в 2025 году трендов, которые могут повлиять на рексис в этом году, — довольно много. мы решили разузнать, на какие из них точно планируют сделать упор в яндексе. для этого поговорили с группой исследования перспективных рекомендательных технологий. а на карточках собрали самые горячие направления, по мнению команды исследователей. @recsyschannel какие рексис-тренды будут развивать в яндексе в 2025 году трендов, которые могут повлиять на рексис в этом году, — довольно много. мы решили разузнать, на какие из них точно планируют сделать упор в яндексе. для этого поговорили с группой исследования перспективных рекомендательных технологий. а на карточках собрали самые горячие направления, по мнению команды исследователей. @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-03T07:37:00+00:00" href="./posts/66.html">2025-03-03 07:37 UTC</a></div>
      </div>
      <div class="post-body"><strong>Какие рексис-тренды будут развивать в Яндексе в 2025 году</strong><br><br>Трендов, которые могут повлиять на рексис в этом году, — довольно много. Мы решили разузнать, на какие из них точно планируют сделать упор в Яндексе. Для этого поговорили с Группой исследования перспективных рекомендательных технологий. А на карточках собрали самые горячие направления, по мнению команды исследователей.<br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/66_480.webp" srcset="../assets/media/thumbs/66_480.webp 480w, ../assets/media/66.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/67_480.webp" srcset="../assets/media/thumbs/67_480.webp 480w, ../assets/media/67.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/68_480.webp" srcset="../assets/media/thumbs/68_480.webp 480w, ../assets/media/68.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/69_480.webp" srcset="../assets/media/thumbs/69_480.webp 480w, ../assets/media/69.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/70_480.webp" srcset="../assets/media/thumbs/70_480.webp 480w, ../assets/media/70.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/71_480.webp" srcset="../assets/media/thumbs/71_480.webp 480w, ../assets/media/71.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/72_480.webp" srcset="../assets/media/thumbs/72_480.webp 480w, ../assets/media/72.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/73_480.webp" srcset="../assets/media/thumbs/73_480.webp 480w, ../assets/media/73.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/74_480.webp" srcset="../assets/media/thumbs/74_480.webp 480w, ../assets/media/74.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/75_480.webp" srcset="../assets/media/thumbs/75_480.webp 480w, ../assets/media/75.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="9" /></div></div>
      <div class="actions">
        <span>5 933 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/66" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/66.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="65" data-search="twin v2: scaling ultra-long user behavior sequence modeling for enhanced ctr prediction at kuaishou сегодня разбираем статью от команды китайской платформы коротких видео kuaishou. за последние 3 года у 2% пользователей платформы накопилось от 100 тысяч до миллиона событий в истории. несмотря на небольшую долю таких пользователей, они генерируют 60% трафика. в целом 95% трафика исходит от пользователей с более чем 10 тысячами событий, поэтому масштабирование моделей под длинные последовательности в kuaishou критично. система обработки пользовательской истории в kuaishou состоит из двух этапов. general search unit (gsu) отбирает наиболее релевантные события из всей истории. затем exact search unit (esu) обрабатывает этот отфильтрованный список. первая версия twin могла работать только с 10 тысячами событий в истории — примерно 3–4 месяца активности пользователей. этого оказалось недостаточно, и новая версия расширяет этот лимит. как происходит обработка истории при офлайн-обработке размер истории пользователя уменьшают примерно в 10 раз. к каждому событию из истории привязан его completion ratio (=playing time / video duration) — на их основе события разбиваются на 5 групп, чтобы в каждой группе у видео были примерно одинаковые значения. затем группы иерархически кластеризуются методом k-means, пока мощность кластеров не достигнет определенного значения. кластеризацию делают на основе эмбеддингов от внутренней рексистемы kuaishou. при онлайн-обработке для каждого кандидата выбирают топ-100 релевантных ему кластеров из истории. чтобы оценить релевантность, кластеры кодируют: numerical-фичи кластера — это усредненные numerical-фичи его айтемов; категориальные фичи берут от элемента, который ближе всего к центроиду. между кандидатом и каждым кластером считают нечто похожее на attention, но без софтмакса: просто скалярные произведения, к которым добавляют логарифм размера кластера, чтобы усилить значимость кластеров, более интересных пользователю. дальше отбирают топ-100 кластеров по полученным скорам, после чего в esu они проходят через трансформер с обычным attention’ом. исследователи проводили эксперименты на собственных логах, сравнивая разные методы отбора наиболее релевантных событий из истории. a/b-тесты показали значимый прирост метрик watch time и diversity. @recsyschannel разбор подготовил ❣ сергей макеев twin v2: scaling ultra-long user behavior sequence modeling for enhanced ctr prediction at kuaishou сегодня разбираем статью от команды китайской платформы коротких видео kuaishou. за последние 3 года у 2% пользователей платформы накопилось от 100 тысяч до миллиона событий в истории. несмотря на небольшую долю таких пользователей, они генерируют 60% трафика. в целом 95% трафика исходит от пользователей с более чем 10 тысячами событий, поэтому масштабирование моделей под длинные последовательности в kuaishou критично. система обработки пользовательской истории в kuaishou состоит из двух этапов. general search unit (gsu) отбирает наиболее релевантные события из всей истории. затем exact search unit (esu) обрабатывает этот отфильтрованный список. первая версия twin могла работать только с 10 тысячами событий в истории — примерно 3–4 месяца активности пользователей. этого оказалось недостаточно, и новая версия расширяет этот лимит. как происходит обработка истории при офлайн-обработке размер истории пользователя уменьшают примерно в 10 раз. к каждому событию из истории привязан его completion ratio (=playing time / video duration) — на их основе события разбиваются на 5 групп, чтобы в каждой группе у видео были примерно одинаковые значения. затем группы иерархически кластеризуются методом k-means, пока мощность кластеров не достигнет определенного значения. кластеризацию делают на основе эмбеддингов от внутренней рексистемы kuaishou. при онлайн-обработке для каждого кандидата выбирают топ-100 релевантных ему кластеров из истории. чтобы оценить релевантность, кластеры кодируют: numerical-фичи кластера — это усредненные numerical-фичи его айтемов; категориальные фичи берут от элемента, который ближе всего к центроиду. между кандидатом и каждым кластером считают нечто похожее на attention, но без софтмакса: просто скалярные произведения, к которым добавляют логарифм размера кластера, чтобы усилить значимость кластеров, более интересных пользователю. дальше отбирают топ-100 кластеров по полученным скорам, после чего в esu они проходят через трансформер с обычным attention’ом. исследователи проводили эксперименты на собственных логах, сравнивая разные методы отбора наиболее релевантных событий из истории. a/b-тесты показали значимый прирост метрик watch time и diversity. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-28T07:27:22+00:00" href="./posts/65.html">2025-02-28 07:27 UTC</a></div>
      </div>
      <div class="post-body"><strong>TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2407.16357" rel="nofollow noopener noreferrer">статью</a> от команды китайской платформы коротких видео Kuaishou. <br><br>За последние 3 года у 2% пользователей платформы накопилось от 100 тысяч до миллиона событий в истории. Несмотря на небольшую долю таких пользователей, они генерируют 60% трафика. В целом 95% трафика исходит от пользователей с более чем 10 тысячами событий, поэтому масштабирование моделей под длинные последовательности в Kuaishou критично.<br><br>Система обработки пользовательской истории в Kuaishou состоит из двух этапов. General Search Unit (GSU) отбирает наиболее релевантные события из всей истории. Затем Exact Search Unit (ESU) обрабатывает этот отфильтрованный список.<br><br>Первая версия TWIN могла работать только с 10 тысячами событий в истории — примерно 3–4 месяца активности пользователей. Этого оказалось недостаточно, и новая версия расширяет этот лимит.<br><br><strong>Как происходит обработка истории</strong><br><br>При офлайн-обработке размер истории пользователя уменьшают примерно в 10 раз. К каждому событию из истории привязан его Completion Ratio (=playing time / video duration) — на их основе события разбиваются на 5 групп, чтобы в каждой группе у видео были примерно одинаковые значения. Затем группы иерархически кластеризуются методом k-means, пока мощность кластеров не достигнет определенного значения. Кластеризацию делают на основе эмбеддингов от внутренней рексистемы Kuaishou.<br><br>При онлайн-обработке для каждого кандидата выбирают топ-100 релевантных ему кластеров из истории. Чтобы оценить релевантность, кластеры кодируют: numerical-фичи кластера — это усредненные numerical-фичи его айтемов; категориальные фичи берут от элемента, который ближе всего к центроиду.<br><br>Между кандидатом и каждым кластером считают нечто похожее на attention, но без софтмакса: просто скалярные произведения, к которым добавляют логарифм размера кластера, чтобы усилить значимость кластеров, более интересных пользователю. Дальше отбирают топ-100 кластеров по полученным скорам, после чего в ESU они проходят через трансформер с обычным attention’ом.<br><br>Исследователи проводили эксперименты на собственных логах, сравнивая разные методы отбора наиболее релевантных событий из истории. A/B-тесты показали значимый прирост метрик Watch Time и Diversity.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/65_480.webp" srcset="../assets/media/thumbs/65_480.webp 480w, ../assets/media/65.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 044 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/65" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/65.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="64" data-search="real-time indexing for large-scale recommendation by streaming vector quantization retriever сегодня разберём статью об обучаемом в реалтайме индексе кандидатов от bytedance (tiktok). retrieval — ключевой этап в рекомендательных системах. он asap отбирает тысячи потенциально релевантных кандидатов в рекомендации из огромного пула документов. ограничения по времени заставляют системы полагаться на индексы, но традиционные подходы имеют свои недостатки. один из самых популярных подходов — hnsw в связке с two-tower-моделью. однако у него хватает минусов: — индекс нужно регулярно перестраивать, что занимает много времени. — при построении индекса не учитывается таргет. — two-tower хуже моделирует user-item взаимодействия. авторы статьи предлагают новый подход к построению индекса — streaming vector quantization, который обеспечивает: — index immediacy — быструю адаптацию к действиям пользователей, так как индекс дообучается в реальном времени. это особенно важно для tiktok. — index reparability — устойчивость к деградации. в отличие от hnsw two-tower, steaming vq не перестраивается, поэтому нужно убедиться, что качество индекса не начнёт деградировать со временем. — index balancing: отсутствие bias в пользу популярного контента. — multi-task learning: при обучении индекса можно одновременно учитывать разные рекомендательные таргеты. на картинке — схема двух хронологических этапов, из которых состоит streaming vq: — retrieval indexing. с помощью vq-vae из всего корпуса документов отбираются несколько тысяч кандидатов, близких к эмбеддингу пользователя. — retrieval ranking. более сложная модель с кросс-аттеншном сортирует результаты предыдущего этапа, отправляя лучшие на финальное ранжирование. по словам авторов, внедрение streaming vq в tiktok значительно улучшило ключевые продуктовые метрики, полностью заменив другие подходы к генерации кандидатов. @recsyschannel разбор подготовил ❣ владислав тыцкий real-time indexing for large-scale recommendation by streaming vector quantization retriever сегодня разберём статью об обучаемом в реалтайме индексе кандидатов от bytedance (tiktok). retrieval — ключевой этап в рекомендательных системах. он asap отбирает тысячи потенциально релевантных кандидатов в рекомендации из огромного пула документов. ограничения по времени заставляют системы полагаться на индексы, но традиционные подходы имеют свои недостатки. один из самых популярных подходов — hnsw в связке с two-tower-моделью. однако у него хватает минусов: — индекс нужно регулярно перестраивать, что занимает много времени. — при построении индекса не учитывается таргет. — two-tower хуже моделирует user-item взаимодействия. авторы статьи предлагают новый подход к построению индекса — streaming vector quantization, который обеспечивает: — index immediacy — быструю адаптацию к действиям пользователей, так как индекс дообучается в реальном времени. это особенно важно для tiktok. — index reparability — устойчивость к деградации. в отличие от hnsw two-tower, steaming vq не перестраивается, поэтому нужно убедиться, что качество индекса не начнёт деградировать со временем. — index balancing : отсутствие bias в пользу популярного контента. — multi-task learning : при обучении индекса можно одновременно учитывать разные рекомендательные таргеты. на картинке — схема двух хронологических этапов, из которых состоит streaming vq: — retrieval indexing . с помощью vq-vae из всего корпуса документов отбираются несколько тысяч кандидатов, близких к эмбеддингу пользователя. — retrieval ranking . более сложная модель с кросс-аттеншном сортирует результаты предыдущего этапа, отправляя лучшие на финальное ранжирование. по словам авторов, внедрение streaming vq в tiktok значительно улучшило ключевые продуктовые метрики, полностью заменив другие подходы к генерации кандидатов. @recsyschannel разбор подготовил ❣ владислав тыцкий">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-18T08:05:23+00:00" href="./posts/64.html">2025-02-18 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Real-time Indexing for Large-scale Recommendation by Streaming Vector Quantization Retriever</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2501.08695" rel="nofollow noopener noreferrer">статью</a> об обучаемом в реалтайме индексе кандидатов от ByteDance (TikTok).<br><br>Retrieval — ключевой этап в рекомендательных системах. Он ASAP отбирает тысячи потенциально релевантных кандидатов в рекомендации из огромного пула документов. Ограничения по времени заставляют системы полагаться на индексы, но традиционные подходы имеют свои недостатки.  <br><br>Один из самых популярных подходов — <a href="https://arxiv.org/pdf/1603.09320" rel="nofollow noopener noreferrer">HNSW</a> в связке с Two-Tower-моделью. Однако у него хватает минусов:  <br><br>— Индекс нужно регулярно перестраивать, что занимает много времени.  <br>— При построении индекса не учитывается таргет.  <br>— Two-Tower хуже моделирует user-item взаимодействия.<br><br>Авторы статьи предлагают новый подход к построению индекса — streaming vector quantization, который обеспечивает:<br><br>— <strong>Index Immediacy</strong> — быструю адаптацию к действиям пользователей, так как индекс дообучается в реальном времени. Это особенно важно для TikTok.<br>— <strong>Index Reparability</strong> — устойчивость к деградации. В отличие от HNSW Two-Tower, steaming VQ не перестраивается, поэтому нужно убедиться, что качество индекса не начнёт деградировать со временем.<br>— <strong>Index Balancing</strong>: отсутствие bias в пользу популярного контента.  <br>— <strong>Multi-task Learning</strong>: при обучении индекса можно одновременно учитывать разные рекомендательные таргеты.<br><br>На картинке — схема двух хронологических этапов, из которых состоит streaming VQ: <br><br>— <strong>Retrieval Indexing</strong>. С помощью <a href="https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf" rel="nofollow noopener noreferrer">VQ-VAE</a> из всего корпуса документов отбираются несколько тысяч кандидатов, близких к эмбеддингу пользователя.  <br>— <strong>Retrieval Ranking</strong>. Более сложная модель с кросс-аттеншном сортирует результаты предыдущего этапа, отправляя лучшие на финальное ранжирование.  <br><br>По словам авторов, внедрение streaming VQ в TikTok значительно улучшило ключевые продуктовые метрики, полностью заменив другие подходы к генерации кандидатов.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владислав Тыцкий<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/64_480.webp" srcset="../assets/media/thumbs/64_480.webp 480w, ../assets/media/64.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="64" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 176 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/64" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/64.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="63" data-search="diffusion-based contrastive learning for sequential recommendation сегодня разбираем статью о подходе contrastive learning в sequential recommendation (sr). авторы ставят под вопрос существующие методы аугментации цепочек последовательных заказов с целью генерации новых цепочек: — маскирование и переупорядочивание истории пользователя, представленной разреженными товарами, может исказить предпочтения. — подмена айтема похожим (coserec) не учитывает контекст, так как использует лишь информацию о взаимной встречаемости товаров. — двойной forward pass c разными dropout-масками (duorec) тоже теряет смысловую последовательность. вместо перечисленного предлагается использовать guided-диффузию для оценки условного распределения айтема, обусловленного на контекст прошлых и будущих заказов. чтобы сблизить латентное пространство диффузионки и sr-модели (в этом случае — sasrec), их обучают вместе, end-to-end, деля между ними эмбеддинги айтемов. кодирование последовательности и sr-модель: history += positional embeddings ➡️ transformer ➡️последний токен последовательности. следующий айтем предсказывается через скалярное произведение векторного представления истории пользователя и эмбеддингов айтемов. цель обучения — минимизировать bce со случайными негативами. аугментация из последовательности s извлекается подмножество s&#x27; с фиксированным соотношением |s&#x27;| / |s|. после чего элементы s&#x27; заменяются на семантически близкие айтемы, предложенные диффузионкой. контрастивное сближение аугментированных последовательностей берётся батч последовательностей, каждая из которых аугментируется дважды с помощью случайных подмножеств, после чего аугментации кодируются трансформером. две аугментированные версии одной последовательности считаются позитивными и противопоставляются оставшимся 2*(batch_size — 1) аугментированным последовательностям, которые выступают как негативы. лосс рассчитывается с помощью кросс-энтропии. диффузия для аугментации последовательность прогоняется через трансформер, после чего на полученных эмбеддингах начинается диффузионный процесс. зашумляются эмбеддинги только тех элементов, которые будут заменены. остальные элементы обуславливают обратный процесс, они — тот самый контекст. обратный процесс моделируется двунаправленным трансформером. итоговый лосс рассчитывается в end-to-end сценарии, где суммируются три компоненты: bce для sr-модели, контрастивное сближение и vlb для диффузионки. @recsyschannel разбор подготовил ❣ сергей макеев diffusion-based contrastive learning for sequential recommendation сегодня разбираем статью о подходе contrastive learning в sequential recommendation (sr). авторы ставят под вопрос существующие методы аугментации цепочек последовательных заказов с целью генерации новых цепочек: — маскирование и переупорядочивание истории пользователя, представленной разреженными товарами, может исказить предпочтения. — подмена айтема похожим (coserec) не учитывает контекст, так как использует лишь информацию о взаимной встречаемости товаров. — двойной forward pass c разными dropout-масками (duorec) тоже теряет смысловую последовательность. вместо перечисленного предлагается использовать guided-диффузию для оценки условного распределения айтема, обусловленного на контекст прошлых и будущих заказов. чтобы сблизить латентное пространство диффузионки и sr-модели (в этом случае — sasrec), их обучают вместе, end-to-end, деля между ними эмбеддинги айтемов. кодирование последовательности и sr-модель: history += positional embeddings ➡️ transformer ➡️последний токен последовательности. следующий айтем предсказывается через скалярное произведение векторного представления истории пользователя и эмбеддингов айтемов. цель обучения — минимизировать bce со случайными негативами. аугментация из последовательности s извлекается подмножество s&amp;#x27; с фиксированным соотношением |s&amp;#x27;| / |s|. после чего элементы s&amp;#x27; заменяются на семантически близкие айтемы, предложенные диффузионкой. контрастивное сближение аугментированных последовательностей берётся батч последовательностей, каждая из которых аугментируется дважды с помощью случайных подмножеств, после чего аугментации кодируются трансформером. две аугментированные версии одной последовательности считаются позитивными и противопоставляются оставшимся 2*(batch_size — 1) аугментированным последовательностям, которые выступают как негативы. лосс рассчитывается с помощью кросс-энтропии. диффузия для аугментации последовательность прогоняется через трансформер, после чего на полученных эмбеддингах начинается диффузионный процесс. зашумляются эмбеддинги только тех элементов, которые будут заменены. остальные элементы обуславливают обратный процесс, они — тот самый контекст. обратный процесс моделируется двунаправленным трансформером. итоговый лосс рассчитывается в end-to-end сценарии, где суммируются три компоненты: bce для sr-модели, контрастивное сближение и vlb для диффузионки. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-12T08:05:49+00:00" href="./posts/63.html">2025-02-12 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Diffusion-based Contrastive Learning for Sequential Recommendation </strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2405.09369" rel="nofollow noopener noreferrer">статью</a> о подходе Contrastive Learning в Sequential Recommendation (SR). <br><br>Авторы ставят под вопрос существующие методы аугментации цепочек последовательных заказов с целью генерации новых цепочек:<br><br>— Маскирование и переупорядочивание истории пользователя, представленной разреженными товарами, может исказить предпочтения. <br>— Подмена айтема похожим (CoSeRec) не учитывает контекст, так как использует лишь информацию о взаимной встречаемости товаров. <br>— Двойной forward pass c разными dropout-масками (DuoRec) тоже теряет смысловую последовательность.<br><br>Вместо перечисленного предлагается использовать guided-диффузию для оценки условного распределения айтема, обусловленного на контекст прошлых и будущих заказов. Чтобы сблизить латентное пространство диффузионки и SR-модели (в этом случае — SASRec), их обучают вместе, end-to-end, деля между ними эмбеддинги айтемов.<br><br>Кодирование последовательности и SR-модель: history += positional embeddings ➡️ transformer ➡️последний токен последовательности. Следующий айтем предсказывается через скалярное произведение векторного представления истории пользователя и эмбеддингов айтемов. <br><br>Цель обучения — минимизировать BCE со случайными негативами.<br><br><strong>Аугментация</strong><br><br>Из последовательности S извлекается подмножество S&#x27; с фиксированным соотношением |S&#x27;| / |S|. После чего элементы S&#x27; заменяются на семантически близкие айтемы, предложенные диффузионкой.<br><br><strong>Контрастивное сближение аугментированных последовательностей</strong><br><br>Берётся батч последовательностей, каждая из которых аугментируется дважды с помощью случайных подмножеств, после чего аугментации кодируются трансформером. Две аугментированные версии одной последовательности считаются позитивными и противопоставляются оставшимся 2*(batch_size — 1) аугментированным последовательностям, которые выступают как негативы. Лосс рассчитывается с помощью кросс-энтропии.<br><br><strong>Диффузия для аугментации</strong><br><br>Последовательность прогоняется через трансформер, после чего на полученных эмбеддингах начинается диффузионный процесс. Зашумляются эмбеддинги только тех элементов, которые будут заменены. Остальные элементы обуславливают обратный процесс, они — тот самый контекст. Обратный процесс моделируется двунаправленным трансформером.<br><br>Итоговый лосс рассчитывается в end-to-end сценарии, где суммируются три компоненты: BCE для SR-модели, контрастивное сближение и VLB для диффузионки.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев</div>
      <div class="actions">
        <span>1 972 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/63" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/63.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="62" data-search="towards understanding the overfitting phenomenon of deep click-through rate prediction models сегодня делимся статьёй о резком переобучении ctr-моделей в начале второй эпохи, a.k.a. one-epoch phenomenon. этой проблеме подвержены модели со структурой вида categorical features with large sparsity ➡️ embedding ➡️ mlp. чтобы понять, можно ли справиться с феноменом, авторы всесторонне его анализируют. так, на качество обучения ctr-моделей не влияют: — число параметров модели; — вид функций активации и батч сайз; — weight decay и dropout (без dropout, кстати, лучше). из-за чего же тогда могут переобучаться модели? по результатам экспериментов, есть несколько причин: — оптимизаторы. чем быстрее сходимость, тем сильнее отрицательное влияние на обучение. наиболее подвержены эффекту adam и rmsprop. — высокие lr: эффект наблюдается только если они больше 10⁻⁷. — кардинальность фичей. чем меньше уникальных эмбеддингов, тем слабее эффект. авторы рассматривали filter (использовали m% эмбеддингов наиболее частых id, остальные — в один эмбеддинг), hash (создавали табличку с m% строк от общего числа id, далее — хэшировали id в эмбеддинги). обязательное условие one-epoch феномена — сдвиг между распределениями p(x_{trained}, y) и p(x_{untrained}, y), вызванный обучением на фичах высокой кардинальности, например, id товаров. если этот сдвиг есть, модель поверх эмбеддинг-слоёв моментально переобучается под p(x_{trained}, y). на картинках показаны нормы изменений параметров слоёв. легко заметить, что в начале второй эпохи резко меняются параметры слоёв поверх эмбеддингов. это доказывает разницу в распределениях p(x_{trained}, y) и p(x_{untrained}, y). можно, конечно, просто не использовать эмбеддинги товаров, чтобы избежать переобучения, но это сильно просадит пиковое качество. вывод авторов — обучать ctr-модели лучше в одну эпоху. @recsyschannel разбор подготовил ❣ сергей макеев towards understanding the overfitting phenomenon of deep click-through rate prediction models сегодня делимся статьёй о резком переобучении ctr-моделей в начале второй эпохи, a.k.a. one-epoch phenomenon. этой проблеме подвержены модели со структурой вида categorical features with large sparsity ➡️ embedding ➡️ mlp. чтобы понять, можно ли справиться с феноменом, авторы всесторонне его анализируют. так, на качество обучения ctr-моделей не влияют: — число параметров модели; — вид функций активации и батч сайз; — weight decay и dropout (без dropout, кстати, лучше). из-за чего же тогда могут переобучаться модели? по результатам экспериментов, есть несколько причин: — оптимизаторы. чем быстрее сходимость, тем сильнее отрицательное влияние на обучение. наиболее подвержены эффекту adam и rmsprop. — высокие lr: эффект наблюдается только если они больше 10⁻⁷. — кардинальность фичей. чем меньше уникальных эмбеддингов, тем слабее эффект. авторы рассматривали filter (использовали m% эмбеддингов наиболее частых id, остальные — в один эмбеддинг), hash (создавали табличку с m% строк от общего числа id, далее — хэшировали id в эмбеддинги). обязательное условие one-epoch феномена — сдвиг между распределениями p(x_{trained}, y) и p(x_{untrained}, y), вызванный обучением на фичах высокой кардинальности, например, id товаров. если этот сдвиг есть, модель поверх эмбеддинг-слоёв моментально переобучается под p(x_{trained}, y). на картинках показаны нормы изменений параметров слоёв. легко заметить, что в начале второй эпохи резко меняются параметры слоёв поверх эмбеддингов. это доказывает разницу в распределениях p(x_{trained}, y) и p(x_{untrained}, y). можно, конечно, просто не использовать эмбеддинги товаров, чтобы избежать переобучения, но это сильно просадит пиковое качество. вывод авторов — обучать ctr-модели лучше в одну эпоху. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-05T08:03:13+00:00" href="./posts/62.html">2025-02-05 08:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Prediction Models</strong><br><br>Сегодня делимся <a href="https://arxiv.org/abs/2209.06053" rel="nofollow noopener noreferrer">статьёй</a> о резком переобучении CTR-моделей в начале второй эпохи, a.k.a. one-epoch phenomenon.<br><br>Этой проблеме подвержены модели со структурой вида categorical features with large sparsity ➡️ Embedding ➡️ MLP.<br><br>Чтобы понять, можно ли справиться с феноменом, авторы всесторонне его анализируют. Так, на качество обучения CTR-моделей не влияют:<br><br>— число параметров модели;<br>— вид функций активации и батч сайз;<br>— weight decay и dropout (без dropout, кстати, лучше).<br><br>Из-за чего же тогда могут переобучаться модели? По результатам экспериментов, есть несколько причин:<br><br>— Оптимизаторы. Чем быстрее сходимость, тем сильнее отрицательное влияние на обучение.  Наиболее подвержены эффекту Adam и RMSPROP.<br>— Высокие LR: эффект наблюдается только если они больше 10⁻⁷.<br>— Кардинальность фичей. Чем меньше уникальных эмбеддингов, тем слабее эффект. Авторы рассматривали FILTER (использовали m% эмбеддингов наиболее частых ID, остальные — в один эмбеддинг), Hash (создавали табличку с m% строк от общего числа ID, далее — хэшировали ID в эмбеддинги). <br><br>Обязательное условие one-epoch феномена — сдвиг между распределениями p(x_{trained}, y) и p(x_{untrained}, y), вызванный обучением на фичах высокой кардинальности, например, ID товаров. Если этот сдвиг есть, модель поверх эмбеддинг-слоёв моментально переобучается под p(x_{trained}, y). <br><br>На картинках показаны нормы изменений параметров слоёв. Легко заметить, что в начале второй эпохи резко меняются параметры слоёв поверх эмбеддингов. Это доказывает разницу в распределениях p(x_{trained}, y) и p(x_{untrained}, y). <br><br>Можно, конечно, просто не использовать эмбеддинги товаров, чтобы избежать переобучения, но это сильно просадит пиковое качество. Вывод авторов — обучать CTR-модели лучше в одну эпоху. <br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/62_480.webp" srcset="../assets/media/thumbs/62_480.webp 480w, ../assets/media/62.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 057 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/62" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/62.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="61" data-search="user-creator feature polarization in recommender systems with dual influence сегодня разбираем необычную статью, содержащую много математики. авторы изучают две проблемы link prediction: filter bubbles — сегрегация в графах, когда модель обособляет кластеры друг от друга, вместо того чтобы предсказывать что-то принципиально новое. в терминах рекомендательных систем — insufficient recommendation diversity, проблема на стороне нейросети. polarization — пользователи разбиваются на кластеры и взаимодействуют только внутри них, не видя альтернативных мнений. в терминах рексистем — insufficient creation diversity, проблема на стороне поставщика контента. для описания рекомендательных систем авторы предлагают использовать упрощённую модель из двух матриц: пользователей и создателей контента. каждый пользователь и каждый создатель в момент времени t описываются своим вектором. в каждый момент времени вектора спроецированы на единичную сферу. пользователю i рекомендуют создателя j, после чего эмбеддинг пользователя i обновляется в зависимости от влияния на него j-го создателя. обновляется и эмбеддинг создателя: на основе эмбеддингов тех, кому его рекомендовали. авторы сформулировали гипотезу: если каждый создатель может быть рекомендован любому пользователю с неотрицательной вероятностью, то поляризация неизбежна. доказывали так: эмбеддинги пользователей и создателей меняются довольно плавно. вероятность того, что каждого создателя порекомендуют каждому пользователю, больше 0. тогда система схлопнется либо в 1 кластер (consensus), либо в 2 (bi-polarization). при этом, если рекомендовать только top-k создателей, зануляя для остальных вероятности или relevance-скоры (скалярные произведения user на creator), можно избежать поляризации и забустить diversity, так как появятся нулевые вероятности. с другой стороны, если модель оптимизирует какой-то лосс не только по релевантности, но и по разнообразию выдачи, избежать биполяризации не получится. @recsyschannel разбор подготовил ❣ сергей макеев user-creator feature polarization in recommender systems with dual influence сегодня разбираем необычную статью , содержащую много математики. авторы изучают две проблемы link prediction: filter bubbles — сегрегация в графах, когда модель обособляет кластеры друг от друга, вместо того чтобы предсказывать что-то принципиально новое. в терминах рекомендательных систем — insufficient recommendation diversity, проблема на стороне нейросети. polarization — пользователи разбиваются на кластеры и взаимодействуют только внутри них, не видя альтернативных мнений. в терминах рексистем — insufficient creation diversity, проблема на стороне поставщика контента. для описания рекомендательных систем авторы предлагают использовать упрощённую модель из двух матриц: пользователей и создателей контента. каждый пользователь и каждый создатель в момент времени t описываются своим вектором. в каждый момент времени вектора спроецированы на единичную сферу. пользователю i рекомендуют создателя j, после чего эмбеддинг пользователя i обновляется в зависимости от влияния на него j-го создателя. обновляется и эмбеддинг создателя: на основе эмбеддингов тех, кому его рекомендовали. авторы сформулировали гипотезу: если каждый создатель может быть рекомендован любому пользователю с неотрицательной вероятностью, то поляризация неизбежна. доказывали так: эмбеддинги пользователей и создателей меняются довольно плавно. вероятность того, что каждого создателя порекомендуют каждому пользователю, больше 0. тогда система схлопнется либо в 1 кластер (consensus), либо в 2 (bi-polarization). при этом, если рекомендовать только top-k создателей, зануляя для остальных вероятности или relevance-скоры (скалярные произведения user на creator), можно избежать поляризации и забустить diversity, так как появятся нулевые вероятности. с другой стороны, если модель оптимизирует какой-то лосс не только по релевантности, но и по разнообразию выдачи, избежать биполяризации не получится. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-28T08:07:36+00:00" href="./posts/61.html">2025-01-28 08:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>User-Creator Feature Polarization in Recommender Systems with Dual Influence</strong><br><br>Сегодня разбираем необычную <a href="https://arxiv.org/abs/2407.14094" rel="nofollow noopener noreferrer">статью</a>, содержащую много математики. <br><br>Авторы изучают две проблемы link prediction: <br><br>Filter bubbles — сегрегация в графах, когда модель обособляет кластеры друг от друга, вместо того чтобы предсказывать что-то принципиально новое. В терминах рекомендательных систем — insufficient recommendation diversity, проблема на стороне нейросети.<br><br>Polarization — пользователи разбиваются на кластеры и взаимодействуют только внутри них, не видя альтернативных мнений. В терминах рексистем — insufficient creation diversity, проблема на стороне поставщика контента.  <br><br>Для описания рекомендательных систем авторы предлагают использовать упрощённую модель из двух матриц: пользователей и создателей контента. Каждый пользователь и каждый создатель в момент времени t описываются своим вектором. В каждый момент времени вектора спроецированы на единичную сферу.<br><br>Пользователю i рекомендуют создателя j, после чего эмбеддинг пользователя i обновляется в зависимости от влияния на него j-го создателя. Обновляется и эмбеддинг создателя: на основе эмбеддингов тех, кому его рекомендовали.<br><br>Авторы сформулировали гипотезу: если каждый создатель может быть рекомендован любому пользователю с неотрицательной вероятностью, то поляризация неизбежна.<br><br>Доказывали так: эмбеддинги пользователей и создателей меняются довольно плавно. Вероятность того, что каждого создателя порекомендуют каждому пользователю, больше 0. Тогда система схлопнется либо в 1 кластер (consensus), либо в 2 (bi-polarization).<br><br>При этом, если рекомендовать только top-k создателей, зануляя для остальных вероятности или relevance-скоры (скалярные произведения user на creator), можно избежать поляризации и забустить diversity, так как появятся нулевые вероятности.<br><br>С другой стороны, если модель оптимизирует какой-то лосс не только по релевантности, но и по разнообразию выдачи, избежать биполяризации не получится. <br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев</div>
      <div class="actions">
        <span>2 066 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/61" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/61.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="60" data-search="fedud: exploiting unaligned data for cross-platform federated click-through rate prediction, alibaba эта статья о том, как с помощью знаний о пользователе с разных доменов бустить качество ctr-модели на домене основном. просто собрать данные с разных доменов в одном месте и централизованно обучить модель не получится — это не конфиденциально. для того чтобы безопасно обрабатывать чувствительные данные, существует подход vertical federated learning (vfl): обучение происходит на каждом домене по отдельности, а полученные верхнеуровневые представления собираются в общую модель. но есть нюанс: собрать таким образом можно только выровненные представления (aligned data). авторы статьи предлагают, как утилизировать на основном домене unaligned data — данные, к которым почему-то не получилось присоединить полезную информацию с других доменов. для aligned data обучение будет состоять из двух фаз (как на схеме): 1. на каждом домене своя сетка получит эмбеддинг пользователя, после чего передаст его сетке главного домена (вместо сырых данных — высокоуровневые представления). эмбеддинг главного домена копируется на две головы. 2. первая голова пытается выучить эмбеддинг другого домена. происходит дистилляция: эмбеддинг главного домена проходит через mlp и через mse сближается с эмбеддингом второстепенного домена (или доменов). во второй голове эмбеддинг из главного и второстепенного доменов конкатятся, прогоняются через mlp и идут в bce loss. когда готова голова, которая предсказывает эмбеддинг с другого домена, можно обучаться и на unaligned data: только во второй голове вместо эмбеддинга с другого домена используется эмбеддинг, предсказанный дистилляционной головой. результаты подтверждаются офлайн-экспериментами на открытом и приватном датасетах. ориентируясь на auc и logloss, авторы сравнивают предложенный подход: — с другими vfl-моделями (которые используют как aligned, так и aligned + unaligned данные); — с wide&amp;deep (без кросс-домена). @recsyschannel разбор подготовил ❣ сергей макеев fedud: exploiting unaligned data for cross-platform federated click-through rate prediction, alibaba эта статья о том, как с помощью знаний о пользователе с разных доменов бустить качество ctr-модели на домене основном. просто собрать данные с разных доменов в одном месте и централизованно обучить модель не получится — это не конфиденциально. для того чтобы безопасно обрабатывать чувствительные данные, существует подход vertical federated learning (vfl): обучение происходит на каждом домене по отдельности, а полученные верхнеуровневые представления собираются в общую модель. но есть нюанс: собрать таким образом можно только выровненные представления (aligned data). авторы статьи предлагают, как утилизировать на основном домене unaligned data — данные, к которым почему-то не получилось присоединить полезную информацию с других доменов. для aligned data обучение будет состоять из двух фаз (как на схеме): 1. на каждом домене своя сетка получит эмбеддинг пользователя, после чего передаст его сетке главного домена (вместо сырых данных — высокоуровневые представления). эмбеддинг главного домена копируется на две головы. 2. первая голова пытается выучить эмбеддинг другого домена. происходит дистилляция: эмбеддинг главного домена проходит через mlp и через mse сближается с эмбеддингом второстепенного домена (или доменов). во второй голове эмбеддинг из главного и второстепенного доменов конкатятся, прогоняются через mlp и идут в bce loss. когда готова голова, которая предсказывает эмбеддинг с другого домена, можно обучаться и на unaligned data: только во второй голове вместо эмбеддинга с другого домена используется эмбеддинг, предсказанный дистилляционной головой. результаты подтверждаются офлайн-экспериментами на открытом и приватном датасетах. ориентируясь на auc и logloss, авторы сравнивают предложенный подход: — с другими vfl-моделями (которые используют как aligned, так и aligned + unaligned данные); — с wide&amp;amp;deep (без кросс-домена). @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-21T08:05:04+00:00" href="./posts/60.html">2025-01-21 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction, Alibaba</strong><br><br>Эта <a href="https://arxiv.org/abs/2407.18472" rel="nofollow noopener noreferrer">статья</a> о том, как с помощью знаний о пользователе с разных доменов бустить качество CTR-модели на домене основном.<br><br>Просто собрать данные с разных доменов в одном месте и централизованно обучить модель не получится — это не конфиденциально. Для того чтобы безопасно обрабатывать чувствительные данные, существует подход Vertical Federated Learning (VFL): обучение происходит на каждом домене по отдельности, а полученные верхнеуровневые представления собираются в общую модель.<br><br>Но есть нюанс: собрать таким образом можно только выровненные представления (aligned data). Авторы статьи предлагают, как утилизировать на основном домене unaligned data — данные, к которым почему-то не получилось присоединить полезную информацию с других доменов. <br><br>Для aligned data обучение будет состоять из двух фаз (как на схеме): <br><br>1. На каждом домене своя сетка получит эмбеддинг пользователя, после чего передаст его сетке главного домена (вместо сырых данных — высокоуровневые представления). Эмбеддинг главного домена копируется на две головы. <br>2. Первая голова пытается выучить эмбеддинг другого домена. Происходит дистилляция: эмбеддинг главного домена проходит через MLP и через MSE сближается с эмбеддингом второстепенного домена (или доменов). Во второй голове эмбеддинг из главного и второстепенного доменов конкатятся, прогоняются через MLP и идут в BCE loss.<br><br>Когда готова голова, которая предсказывает эмбеддинг с другого домена, можно обучаться и на unaligned data: только во второй голове вместо эмбеддинга с другого домена используется эмбеддинг, предсказанный дистилляционной головой.<br><br>Результаты подтверждаются офлайн-экспериментами на открытом и приватном датасетах. Ориентируясь на AUC и LogLoss, авторы сравнивают предложенный подход:  <br><br>— с другими VFL-моделями (которые используют как aligned, так и aligned + unaligned данные); <br>— с Wide&amp;Deep (без кросс-домена).<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/60_480.webp" srcset="../assets/media/thumbs/60_480.webp 480w, ../assets/media/60.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="60" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 216 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/60" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/60.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="59" data-search="разбор тренда: графовые нейросети в индустрии начинаем год с взгляда в будущее! некоторое время назад кирилл хрыльченко написал на хабре пост о главных трендах в рексис. практически все эти тренды будут актуальны и в 2025-м — в науке и индустрии ещё много чего можно сделать в этих направлениях. сегодня разберём подробнее один из трендов, который точно не потеряет актуальности в мире рекомендательных систем, — графовые нейросети. посмотрим, какие подходы существуют и с какими интересными статьями стоит ознакомиться, чтобы лучше разобраться в теме. классификация подходов: 1. end-to-end — обучаем графовую нейросеть вместе с последующей моделью. 2. frozen — переиспользуем уже выученные графовые представления в замороженном виде: ◦ трансдуктивные — не обобщаемся на новых юзеров или айтемы; ◦ индуктивные — можем получать представления для новых юзеров или айтемов; ◦ промежуточные подходы — для юзеров получить представления можем, а для айтемов — нет. end-to-end etsy — (2023) unified embedding based personalized retrieval in etsy search. особенности статьи: retrieval для поиска, графовое представление как дополнительная фича документа. searchquery-product граф, семплирование соседей и усреднение в качестве агрегации. taobao — (2023) graph contrastive learning with multi-objective for personalized product retrieval in taobao search. аналогичное предыдущей статье применение подхода, item-item граф, attention в качестве агрегации. amazon — (2021) graph-based multilingual product retrieval in e-commerce search. идея, похожая на работу от etsy. авторы делают multilingual модель. alibaba group — (2022) multi-level contrastive learning framework for sequential recommendation. на обучении графовое представление пользователя сближается с тем, что получено из sequential модели. на инференсе графовая часть откидывается. трансдуктивные x/twitter — (2022) twhin. embedding the twitter heterogeneous information network for personalized recommendation. обучаемые id для каждой сущности, transe на bce (link-prediction). промежуточные amazon — (2023) multi-task knowledge enhancement for zero-shot and multi-domain recommendation in an ai assistant application. авторы создают кросс-доменный граф: музыка, видео, книги. юзеры кодируются индуктивно, а айтемы = обучаемые id. индуктивные pinterest — (2022) multibisage: a web-scale recommendation system using multiple bipartite graphs at pinterest. исследователи получают графовые представления для пинов, которые потом переиспользуют везде. используют контент. spotify — (2021) multi-task learning of graph-based inductive representations of music content. мультитаск, bce — пара вершин принадлежит одному плейлисту (link-prediction), либо пара вершин имеет один и тот же жанр, регрессия основана на близости по контенту. spotify — (2020) podcast recommendations and search query using gnns at spotify. graph learning workshop 2022. рассказ о собственных графовых сетках spotify. kuaishou — (2023) a unified model for video understanding and knowledge embedding with heterogeneous knowledge graph dataset. авторы получают индуктивные представления для видео и тегов к ним. основные выводы 1. используя графовые нейросети, многие авторы статей наблюдают улучшение метрик на long-tail айтемах. 2. такие сетки удобно использовать для данных из разных доменов. 3. также графовые нейросети используются как один из источников генерации кандидатов или фич в ранжировании. @recsyschannel обзор подготовил ❣ артем матвеев разбор тренда: графовые нейросети в индустрии начинаем год с взгляда в будущее! некоторое время назад кирилл хрыльченко написал на хабре пост о главных трендах в рексис . практически все эти тренды будут актуальны и в 2025-м — в науке и индустрии ещё много чего можно сделать в этих направлениях. сегодня разберём подробнее один из трендов, который точно не потеряет актуальности в мире рекомендательных систем, — графовые нейросети. посмотрим, какие подходы существуют и с какими интересными статьями стоит ознакомиться, чтобы лучше разобраться в теме. классификация подходов: 1. end-to-end — обучаем графовую нейросеть вместе с последующей моделью. 2. frozen — переиспользуем уже выученные графовые представления в замороженном виде: ◦ трансдуктивные — не обобщаемся на новых юзеров или айтемы; ◦ индуктивные — можем получать представления для новых юзеров или айтемов; ◦ промежуточные подходы — для юзеров получить представления можем, а для айтемов — нет. end-to-end etsy — (2023) unified embedding based personalized retrieval in etsy search . особенности статьи: retrieval для поиска, графовое представление как дополнительная фича документа. searchquery-product граф, семплирование соседей и усреднение в качестве агрегации. taobao — (2023) graph contrastive learning with multi-objective for personalized product retrieval in taobao search . аналогичное предыдущей статье применение подхода, item-item граф, attention в качестве агрегации. amazon — (2021) graph-based multilingual product retrieval in e-commerce search . идея, похожая на работу от etsy. авторы делают multilingual модель. alibaba group — (2022) multi-level contrastive learning framework for sequential recommendation . на обучении графовое представление пользователя сближается с тем, что получено из sequential модели. на инференсе графовая часть откидывается. трансдуктивные x/twitter — (2022) twhin. embedding the twitter heterogeneous information network for personalized recommendation . обучаемые id для каждой сущности, transe на bce (link-prediction). промежуточные amazon — (2023) multi-task knowledge enhancement for zero-shot and multi-domain recommendation in an ai assistant application . авторы создают кросс-доменный граф: музыка, видео, книги. юзеры кодируются индуктивно, а айтемы = обучаемые id. индуктивные pinterest — (2022) multibisage: a web-scale recommendation system using multiple bipartite graphs at pinterest . исследователи получают графовые представления для пинов, которые потом переиспользуют везде. используют контент. spotify — (2021) multi-task learning of graph-based inductive representations of music content . мультитаск, bce — пара вершин принадлежит одному плейлисту (link-prediction), либо пара вершин имеет один и тот же жанр, регрессия основана на близости по контенту. spotify — (2020) podcast recommendations and search query using gnns at spotify. graph learning workshop 2022 . рассказ о собственных графовых сетках spotify. kuaishou — (2023) a unified model for video understanding and knowledge embedding with heterogeneous knowledge graph dataset . авторы получают индуктивные представления для видео и тегов к ним. основные выводы 1. используя графовые нейросети, многие авторы статей наблюдают улучшение метрик на long-tail айтемах. 2. такие сетки удобно использовать для данных из разных доменов. 3. также графовые нейросети используются как один из источников генерации кандидатов или фич в ранжировании. @recsyschannel обзор подготовил ❣ артем матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-16T08:02:40+00:00" href="./posts/59.html">2025-01-16 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Разбор тренда: графовые нейросети в индустрии</strong><br><br>Начинаем год с взгляда в будущее! Некоторое время назад Кирилл Хрыльченко написал на Хабре <a href="https://habr.com/ru/companies/yandex/articles/857068/" rel="nofollow noopener noreferrer">пост о главных трендах в рексис</a>. Практически все эти тренды будут актуальны и в 2025-м — в науке и индустрии ещё много чего можно сделать в этих направлениях. Сегодня разберём подробнее один из трендов, который точно не потеряет актуальности в мире рекомендательных систем, — графовые нейросети. Посмотрим, какие подходы существуют и с какими интересными статьями стоит ознакомиться, чтобы лучше разобраться в теме.<br><br><strong>Классификация подходов:</strong><br><br>1. End-to-end — обучаем графовую нейросеть вместе с последующей моделью. <br>2. Frozen — переиспользуем уже выученные графовые представления в замороженном виде:<br>    ◦ трансдуктивные — не обобщаемся на новых юзеров или айтемы;<br>    ◦ индуктивные — можем получать представления для новых юзеров или айтемов; <br>    ◦ промежуточные подходы — для юзеров получить представления можем, а для айтемов — нет.<br><br><strong>End-to-end</strong><br><br>Etsy — (2023) <a href="https://arxiv.org/abs/2306.04833" rel="nofollow noopener noreferrer">Unified Embedding Based Personalized Retrieval in Etsy Search</a>. Особенности статьи: retrieval для поиска, графовое представление как дополнительная фича документа. SearchQuery-Product граф, семплирование соседей и усреднение в качестве агрегации.<br><br>Taobao — (2023) <a href="https://arxiv.org/abs/2307.04322" rel="nofollow noopener noreferrer">Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval in Taobao Search</a>. Аналогичное предыдущей статье применение подхода, item-item граф, attention в качестве агрегации.<br><br>Amazon — (2021) <a href="https://arxiv.org/abs/2105.02978" rel="nofollow noopener noreferrer">Graph-based Multilingual Product Retrieval in E-Commerce Search</a>. Идея, похожая на работу от Etsy. Авторы делают multilingual модель.<br><br>Alibaba Group — (2022) <a href="https://arxiv.org/abs/2208.13007" rel="nofollow noopener noreferrer">Multi-level Contrastive Learning Framework for Sequential Recommendation</a>. На обучении графовое представление пользователя сближается с тем, что получено из sequential модели. На инференсе графовая часть откидывается.<br><br><strong>Трансдуктивные</strong><br><br>X/Twitter — (2022) TwHIN. <a href="https://arxiv.org/abs/2202.05387" rel="nofollow noopener noreferrer">Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation</a>. Обучаемые ID для каждой сущности, TransE на BCE (link-prediction). <br><br><strong>Промежуточные</strong><br><br>Amazon — (2023) <a href="https://arxiv.org/abs/2306.06302" rel="nofollow noopener noreferrer">Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain Recommendation in an AI Assistant Application</a>. Авторы создают кросс-доменный граф: музыка, видео, книги. Юзеры кодируются индуктивно, а айтемы = обучаемые ID.<br><br><strong>Индуктивные</strong><br><br>Pinterest — (2022) <a href="https://arxiv.org/abs/2205.10666" rel="nofollow noopener noreferrer">MultiBiSage: A Web-Scale Recommendation System Using Multiple Bipartite Graphs at Pinterest</a>. Исследователи получают графовые представления для пинов, которые потом переиспользуют везде. Используют контент.<br><br>Spotify — (2021) <a href="https://research.atspotify.com/publications/multi-task-learning-of-graph-based-inductive-representations-of-music-content/" rel="nofollow noopener noreferrer">Multi-Task Learning Of Graph-Based Inductive Representations Of Music Content</a>. Мультитаск, BCE — пара вершин принадлежит одному плейлисту (link-prediction), либо пара вершин имеет один и тот же жанр, регрессия основана на близости по контенту.<br><br>Spotify — (2020) <a href="https://youtu.be/79MRwEB5AhA" rel="nofollow noopener noreferrer">Podcast Recommendations and Search Query using GNNs at Spotify. Graph Learning Workshop 2022</a>. Рассказ о собственных графовых сетках Spotify.<br><br>KuaiShou — (2023) <a href="https://arxiv.org/abs/2211.10624" rel="nofollow noopener noreferrer">A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset</a>. Авторы получают индуктивные представления для видео и тегов к ним.<br><br><strong>Основные выводы</strong><br><br>1. Используя графовые нейросети, многие авторы статей наблюдают улучшение метрик на long-tail айтемах. <br>2. Такие сетки удобно использовать для данных из разных доменов.<br>3. Также графовые нейросети используются как один из источников генерации кандидатов или фич в ранжировании.<br><br>@RecSysChannel<br>Обзор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Артем Матвеев</div>
      <div class="actions">
        <span>2 433 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/59" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/59.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="58" data-search="🎄 чтение на каникулы: бонусная подборка статей от экспертов «рекомендательной» мы уже делились лучшими постами за год в канале, но интересных статей в мире рексис гораздо больше, чем мы успели разобрать в 2024-м. планы на 2025-й — масштабные, замедляться не думаем и вам не советуем, поэтому ловите бонусную подборку интересных статей для чтения и профессионального развития на новогодних праздниках. кстати, полные разборы этих материалов обязательно появятся здесь в будущем — не пропустите то, что заинтересует именно вас. и с наступающими! towards understanding the overfitting phenomenon of deep click-through rate prediction models в статье рассматривается проблема резкого переобучения ctr-моделей в начале второй эпохи, a.k.a. one-epoch phenomenon. его можно преодолеть с помощью нескольких трюков: снизить количество уникальных эмбеддингов или вовсе отказаться от эмбеддингов товаров, поэкспериментировать с оптимизаторами (adam и rmsprop оказались более склонны к эффекту), снизить lr, — однако всё это приводит к снижению пикового качества. diffusion-based contrastive learning for sequential recommendation авторы поставили под вопрос существующие методы аугментации цепочек последовательных заказов с целью генерации новых цепочек. вместо маскирования и переупорядочивания истории пользователя предлагают использовать guided-диффузию для оценки условного распределения айтема, обусловленного на контекст. чтобы сблизить латентное пространство диффузионки и sr-модели (в статье это sasrec), их обучают вместе end-to-end, шаря эмбеддинги айтемов. beyond item dissimilarities: diversifying by intent in recommender systems авторы хотят разнообразить предлагаемый пользователю контент, так как человек может иметь разные намерения, например, в зависимости от дня недели или времени: это может быть спорт, учеба, отдых и т. д. было бы здорово учитывать намерение (intent) пользователя при генерации выдачи, а не только user-item схожесть. для решения проблемы авторы предлагают новый фреймворк, который используется поверх рексистемы, но описывают его на идейном уровне, не уточняя, чем моделируют распределения. learned ranking function: from short-term behavior predictions to long-term user satisfaction авторы статьи формулируют ранжирование как multitask slate optimization на языке траекторий пользователя. при ранжировании на вход получают m multitask model scoring предиктов для каждого кандидата. раньше их комбинировали с весами-гиперпараметрами, которые оптимизировали через байесовскую оптимизацию. цель — ранжировать так, чтобы повысить long term user satisfaction, то есть каждый слейт оценивается в том числе и по тому, что происходит после того, как пользователь его покинул. autoregressive generation strategies for top-k sequential recommendations авторегрессионные рекомендации от лабы сбера. хотят генерировать k следующих айтемов для пользователя. решают эту задачу через генерацию s различных пользовательских траекторий, которые потом агрегируют вместе. ссылаются на пиннерформер и используют у себя decoder-only архитектуру. в самой статье предлагают два решения для агрегации: reciprocal rank aggregation и relevance aggregation. в качестве декодера используют gpt-2. в разделе о генерации траекторий упоминают разные алгоритмы (жадный, beam search и temperature sampling), но используют только последний. your causal self-attentive recommender hosts a lonely neighborhood статья от джулиана маколи пытается ответить на вопрос: что все-таки лучше, авторегрессионные подходы или автоэнкодерные и почему. исследователи ссылаются на множество работ и хотят разобраться, насколько результаты робастны по итогу. сравнивают bert4rec и sasrec в основном с их различными модификациями. все эксперименты проведены на открытых датасетах: beauty, sports, video, yelp, movielens. @recsyschannel подборку подготовили ❣ сергей макеев и владимир байкалов 🎄 чтение на каникулы: бонусная подборка статей от экспертов «рекомендательной» мы уже делились лучшими постами за год в канале, но интересных статей в мире рексис гораздо больше, чем мы успели разобрать в 2024-м. планы на 2025-й — масштабные, замедляться не думаем и вам не советуем, поэтому ловите бонусную подборку интересных статей для чтения и профессионального развития на новогодних праздниках. кстати, полные разборы этих материалов обязательно появятся здесь в будущем — не пропустите то, что заинтересует именно вас. и с наступающими! towards understanding the overfitting phenomenon of deep click-through rate prediction models в статье рассматривается проблема резкого переобучения ctr-моделей в начале второй эпохи, a.k.a. one-epoch phenomenon. его можно преодолеть с помощью нескольких трюков: снизить количество уникальных эмбеддингов или вовсе отказаться от эмбеддингов товаров, поэкспериментировать с оптимизаторами (adam и rmsprop оказались более склонны к эффекту), снизить lr, — однако всё это приводит к снижению пикового качества. diffusion-based contrastive learning for sequential recommendation авторы поставили под вопрос существующие методы аугментации цепочек последовательных заказов с целью генерации новых цепочек. вместо маскирования и переупорядочивания истории пользователя предлагают использовать guided-диффузию для оценки условного распределения айтема, обусловленного на контекст. чтобы сблизить латентное пространство диффузионки и sr-модели (в статье это sasrec), их обучают вместе end-to-end, шаря эмбеддинги айтемов. beyond item dissimilarities: diversifying by intent in recommender systems авторы хотят разнообразить предлагаемый пользователю контент, так как человек может иметь разные намерения, например, в зависимости от дня недели или времени: это может быть спорт, учеба, отдых и т. д. было бы здорово учитывать намерение (intent) пользователя при генерации выдачи, а не только user-item схожесть. для решения проблемы авторы предлагают новый фреймворк, который используется поверх рексистемы, но описывают его на идейном уровне, не уточняя, чем моделируют распределения. learned ranking function: from short-term behavior predictions to long-term user satisfaction авторы статьи формулируют ранжирование как multitask slate optimization на языке траекторий пользователя. при ранжировании на вход получают m multitask model scoring предиктов для каждого кандидата. раньше их комбинировали с весами-гиперпараметрами, которые оптимизировали через байесовскую оптимизацию. цель — ранжировать так, чтобы повысить long term user satisfaction, то есть каждый слейт оценивается в том числе и по тому, что происходит после того, как пользователь его покинул. autoregressive generation strategies for top-k sequential recommendations авторегрессионные рекомендации от лабы сбера. хотят генерировать k следующих айтемов для пользователя. решают эту задачу через генерацию s различных пользовательских траекторий, которые потом агрегируют вместе. ссылаются на пиннерформер и используют у себя decoder-only архитектуру. в самой статье предлагают два решения для агрегации: reciprocal rank aggregation и relevance aggregation. в качестве декодера используют gpt-2. в разделе о генерации траекторий упоминают разные алгоритмы (жадный, beam search и temperature sampling), но используют только последний. your causal self-attentive recommender hosts a lonely neighborhood статья от джулиана маколи пытается ответить на вопрос: что все-таки лучше, авторегрессионные подходы или автоэнкодерные и почему. исследователи ссылаются на множество работ и хотят разобраться, насколько результаты робастны по итогу. сравнивают bert4rec и sasrec в основном с их различными модификациями. все эксперименты проведены на открытых датасетах: beauty, sports, video, yelp, movielens. @recsyschannel подборку подготовили ❣ сергей макеев и владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-27T09:15:28+00:00" href="./posts/58.html">2024-12-27 09:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>🎄 Чтение на каникулы: бонусная подборка статей от экспертов «Рекомендательной»</strong><br><br>Мы уже делились лучшими постами за год в канале, но интересных статей в мире рексис гораздо больше, чем мы успели разобрать в 2024-м. Планы на 2025-й — масштабные, замедляться не думаем и вам не советуем, поэтому ловите бонусную подборку интересных статей для чтения и профессионального развития на новогодних праздниках. Кстати, полные разборы этих материалов обязательно появятся здесь в будущем — не пропустите то, что заинтересует именно вас. И с наступающими!<br><br><a href="https://arxiv.org/abs/2209.06053" rel="nofollow noopener noreferrer"><strong>Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Prediction Models</strong></a><br>В статье рассматривается проблема резкого переобучения CTR-моделей в начале второй эпохи, a.k.a. one-epoch phenomenon. Его можно преодолеть с помощью нескольких трюков: снизить количество уникальных эмбеддингов или вовсе отказаться от эмбеддингов товаров, поэкспериментировать с оптимизаторами (Adam и RMSPROP оказались более склонны к эффекту), снизить LR, — однако всё это приводит к снижению пикового качества.<br><br><a href="https://arxiv.org/abs/2405.09369" rel="nofollow noopener noreferrer"><strong>Diffusion-based Contrastive Learning for Sequential Recommendation</strong>  </a><br>Авторы поставили под вопрос существующие методы аугментации цепочек последовательных заказов с целью генерации новых цепочек. Вместо маскирования и переупорядочивания истории пользователя предлагают использовать guided-диффузию для оценки условного распределения айтема, обусловленного на контекст. Чтобы сблизить латентное пространство диффузионки и SR-модели (в статье это SASRec), их обучают вместе end-to-end, шаря эмбеддинги айтемов.<br><br><a href="https://arxiv.org/abs/2405.12327" rel="nofollow noopener noreferrer"><strong>Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems </strong></a><br>Авторы хотят разнообразить предлагаемый пользователю контент, так как человек может иметь разные намерения, например, в зависимости от дня недели или времени: это может быть спорт, учеба, отдых и т. д. Было бы здорово учитывать намерение (intent) пользователя при генерации выдачи, а не только user-item схожесть. Для решения проблемы авторы предлагают новый фреймворк, который используется поверх рексистемы, но описывают его на идейном уровне, не уточняя, чем моделируют распределения.<br><br><a href="https://arxiv.org/abs/2408.06512" rel="nofollow noopener noreferrer"><strong>Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction</strong></a><br>Авторы статьи формулируют ранжирование как multitask slate optimization на языке траекторий пользователя. При ранжировании на вход получают m Multitask Model Scoring предиктов для каждого кандидата. Раньше их комбинировали с весами-гиперпараметрами, которые оптимизировали через Байесовскую оптимизацию. Цель — ранжировать так, чтобы повысить long term user satisfaction, то есть каждый слейт оценивается в том числе и по тому, что происходит после того, как пользователь его покинул.<br><br><a href="https://arxiv.org/abs/2409.17730" rel="nofollow noopener noreferrer"><strong>Autoregressive Generation Strategies for Top-K Sequential Recommendations</strong></a><br>Авторегрессионные рекомендации от лабы Сбера. Хотят генерировать k следующих айтемов для пользователя. Решают эту задачу через генерацию S различных пользовательских траекторий, которые потом агрегируют вместе. Ссылаются на пиннерформер и используют у себя decoder-only архитектуру. В самой статье предлагают два решения для агрегации: Reciprocal Rank Aggregation и Relevance Aggregation. В качестве декодера используют GPT-2. В разделе о генерации траекторий упоминают разные алгоритмы (жадный, beam search и temperature sampling), но используют только последний.<br><br><a href="https://www.zhuanzhi.ai/paper/c467c5c5890a597a5a799c2bb1656252" rel="nofollow noopener noreferrer"><strong>Your Causal Self-Attentive Recommender Hosts a Lonely Neighborhood</strong></a><br>Статья от Джулиана МакОли пытается ответить на вопрос: что все-таки лучше, авторегрессионные подходы или автоэнкодерные и почему. Исследователи ссылаются на множество работ и хотят разобраться, насколько результаты робастны по итогу. Сравнивают BERT4Rec и SASRec в основном с их различными модификациями. Все эксперименты проведены на открытых датасетах: Beauty, Sports, Video, Yelp, MovieLens.<br><br>@RecSysChannel<br>Подборку подготовили <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев и Владимир Байкалов</div>
      <div class="actions">
        <span>2 444 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/58" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/58.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="57" data-search="🏆лучшее за год в рекомендательной год был щедрым на интересные recsys-статьи, а мы не ленились разбирать их. предлагаем освежить в памяти посты, которые вы читали чаще всего. если что-то прошло мимо, самое время наверстать! icml 2024 — как это было даниил лещёв и андрей мищенко рассказали, что запомнилось на icml 2024. среди интересного — новая архитектура ml-моделей в рекомендациях и методы из мира llm для улучшения lstm-моделей. пожалуй, самый необычный хайлайт от ребят (хотя и не из нашего домена, просто им очень понравилось) — статья об обучении роборуки осязанию и возможности различать текстуры поверхностей. даёшь тактильные ощущения роботам! законы масштабирования в больших моделях последовательных рекомендаций scaling law добрался до рекомендаций. артём матвеев разобрал статью от wechat и tencent, где авторы проверили, как увеличение параметров моделей улучшает качество рекомендаций. (спойлер: выяснили, что большие модели справляются лучше на сложных задачах). детали эксперимента — в обзоре. actions speak louder than words: trillion-parameter sequential transducers for generative recommendations кирилл хрыльченко разобрал hstu — новую архитектуру для рекомендаций, которая показывает отличные результаты в онлайн-эксперименте и обрабатывает бо́льшие истории в сравнении с прошлыми подходами. авторы предложили отдельные модели для генерации кандидатов и ранжирования, сделали последовательности событий target-aware и отказались от софтмакса в трансформере, чтобы точнее работать с пользовательской историей. кластерная якорная регуляризация в рекомендательных системах сергей макеев объяснил, как ресёрчеры из deepmind решали проблему popularity bias в рекомендациях. их метод cluster anchor regularization делает так, чтобы популярные айтемы «тянули» за собой непопулярные и помогали справляться с перекосами. новый подход протестировали на youtube shorts — похоже, рекомендации могут стать качественнее. lignn: graph neural networks at linkedin владимир байкалов рассказал, как linkedin использует графы для своих рекомендаций. эти графы связывают пользователей с вакансиями, группами и компаниями. результат — обучение стало быстрее, а метрики рекомендаций заметно выросли. multi-objective learning to rank by model distillation airbnb придумали, как объединить дистилляцию и мультитаск-обучение, чтобы алгоритмы ранжирования стали умнее. подход учитывает важные факторы, вроде возвратов или обращений в поддержку. как всё устроено, разбирался сергей макеев. интересное с acm recsys 2024, часть 3 а ещё мы сделали серию постов о лучших статьях с конференции acm recsys. самым популярным стал разбор модели text2tracks, которая умеет подбирать музыку по текстовому запросу. пётр зайдель описал, как эта модель выбирает треки, и сравнил разные способы кодирования. первая и вторая части тоже заслуживают г̶л̶у̶б̶о̶к̶о̶г̶о лайка. @recsyschannel ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф 🏆 лучшее за год в рекомендательной год был щедрым на интересные recsys-статьи, а мы не ленились разбирать их. предлагаем освежить в памяти посты, которые вы читали чаще всего. если что-то прошло мимо, самое время наверстать! icml 2024 — как это было даниил лещёв и андрей мищенко рассказали, что запомнилось на icml 2024. среди интересного — новая архитектура ml-моделей в рекомендациях и методы из мира llm для улучшения lstm-моделей. пожалуй, самый необычный хайлайт от ребят (хотя и не из нашего домена, просто им очень понравилось) — статья об обучении роборуки осязанию и возможности различать текстуры поверхностей. даёшь тактильные ощущения роботам! законы масштабирования в больших моделях последовательных рекомендаций scaling law добрался до рекомендаций. артём матвеев разобрал статью от wechat и tencent, где авторы проверили, как увеличение параметров моделей улучшает качество рекомендаций. (спойлер: выяснили, что большие модели справляются лучше на сложных задачах). детали эксперимента — в обзоре. actions speak louder than words: trillion-parameter sequential transducers for generative recommendations кирилл хрыльченко разобрал hstu — новую архитектуру для рекомендаций, которая показывает отличные результаты в онлайн-эксперименте и обрабатывает бо́льшие истории в сравнении с прошлыми подходами. авторы предложили отдельные модели для генерации кандидатов и ранжирования, сделали последовательности событий target-aware и отказались от софтмакса в трансформере, чтобы точнее работать с пользовательской историей. кластерная якорная регуляризация в рекомендательных системах сергей макеев объяснил, как ресёрчеры из deepmind решали проблему popularity bias в рекомендациях. их метод cluster anchor regularization делает так, чтобы популярные айтемы «тянули» за собой непопулярные и помогали справляться с перекосами. новый подход протестировали на youtube shorts — похоже, рекомендации могут стать качественнее. lignn: graph neural networks at linkedin владимир байкалов рассказал, как linkedin использует графы для своих рекомендаций. эти графы связывают пользователей с вакансиями, группами и компаниями. результат — обучение стало быстрее, а метрики рекомендаций заметно выросли. multi-objective learning to rank by model distillation airbnb придумали, как объединить дистилляцию и мультитаск-обучение, чтобы алгоритмы ранжирования стали умнее. подход учитывает важные факторы, вроде возвратов или обращений в поддержку. как всё устроено, разбирался сергей макеев. интересное с acm recsys 2024, часть 3 а ещё мы сделали серию постов о лучших статьях с конференции acm recsys. самым популярным стал разбор модели text2tracks, которая умеет подбирать музыку по текстовому запросу. пётр зайдель описал, как эта модель выбирает треки, и сравнил разные способы кодирования. первая и вторая части тоже заслуживают г̶л̶у̶б̶о̶к̶о̶г̶о лайка. @recsyschannel ___ meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-20T11:01:47+00:00" href="./posts/57.html">2024-12-20 11:01 UTC</a></div>
      </div>
      <div class="post-body"><strong><tg-emoji emoji-id="5341761825170017804">🏆</tg-emoji></strong><strong>Лучшее за год в Рекомендательной</strong><br><br>Год был щедрым на интересные recsys-статьи, а мы не ленились разбирать их. Предлагаем освежить в памяти посты, которые вы читали чаще всего. Если что-то прошло мимо, самое время наверстать!<br><br><a href="https://t.me/RecSysChannel/19" rel="nofollow noopener noreferrer"><strong>ICML 2024 — как это было</strong></a><br>Даниил Лещёв и Андрей Мищенко рассказали, что запомнилось на ICML 2024. Среди интересного — новая архитектура ML-моделей в рекомендациях и методы из мира LLM для улучшения LSTM-моделей. Пожалуй, самый необычный хайлайт от ребят (хотя и не из нашего домена, просто им очень понравилось) — статья об обучении роборуки осязанию и возможности различать текстуры поверхностей. Даёшь тактильные ощущения роботам!<br><br><a href="https://t.me/RecSysChannel/22" rel="nofollow noopener noreferrer"><strong>Законы масштабирования в больших моделях последовательных рекомендаций</strong></a><strong> </strong><br>Scaling law добрался до рекомендаций. Артём Матвеев разобрал статью от WeChat и Tencent, где авторы проверили, как увеличение параметров моделей улучшает качество рекомендаций. (Спойлер: выяснили, что большие модели справляются лучше на сложных задачах). Детали эксперимента — в обзоре.<br><br><a href="https://t.me/RecSysChannel/48" rel="nofollow noopener noreferrer"><strong>Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations</strong></a><br>Кирилл Хрыльченко разобрал HSTU — новую архитектуру для рекомендаций, которая показывает отличные результаты в онлайн-эксперименте и обрабатывает бо́льшие истории в сравнении с прошлыми подходами. Авторы предложили отдельные модели для генерации кандидатов и ранжирования, сделали последовательности событий target-aware и отказались от софтмакса в трансформере, чтобы точнее работать с пользовательской историей.<br><br><a href="https://t.me/RecSysChannel/18" rel="nofollow noopener noreferrer"><strong>Кластерная якорная регуляризация в рекомендательных системах</strong></a><br>Сергей Макеев объяснил, как ресёрчеры из DeepMind решали проблему popularity bias в рекомендациях. Их метод Cluster Anchor Regularization делает так, чтобы популярные айтемы «тянули» за собой непопулярные и помогали справляться с перекосами. Новый подход протестировали на YouTube Shorts — похоже, рекомендации могут стать качественнее. <br><br><a href="https://t.me/RecSysChannel/49" rel="nofollow noopener noreferrer"><strong>LiGNN: Graph Neural Networks at LinkedIn</strong></a><br>Владимир Байкалов рассказал, как LinkedIn использует графы для своих рекомендаций. Эти графы связывают пользователей с вакансиями, группами и компаниями. Результат — обучение стало быстрее, а метрики рекомендаций заметно выросли.<br><br><a href="https://t.me/RecSysChannel/24" rel="nofollow noopener noreferrer"><strong>Multi-objective Learning to Rank by Model Distillation</strong></a><br>Airbnb придумали, как объединить дистилляцию и мультитаск-обучение, чтобы алгоритмы ранжирования стали умнее. Подход учитывает важные факторы, вроде возвратов или обращений в поддержку. Как всё устроено, разбирался Сергей Макеев.<br><br><a href="https://t.me/RecSysChannel/38" rel="nofollow noopener noreferrer"><strong>Интересное с ACM RecSys 2024, часть 3</strong></a><br>А ещё мы сделали серию постов о лучших статьях с конференции ACM RecSys. Самым популярным стал разбор модели Text2Tracks, которая умеет подбирать музыку по текстовому запросу. Пётр Зайдель описал, как эта модель выбирает треки, и сравнил разные способы кодирования. <a href="https://t.me/RecSysChannel/28" rel="nofollow noopener noreferrer">Первая</a> и <a href="https://t.me/RecSysChannel/33" rel="nofollow noopener noreferrer">вторая</a> части тоже заслуживают г̶л̶у̶б̶о̶к̶о̶г̶о лайка.<br><br>@RecSysChannel<br>___<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em></div>
      <div class="actions">
        <span>2 481 просмотров · 40 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/57" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/57.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="56" data-search="break the id-language barrier: an adaption framework for sequential recommendation один из главных трендов recsys 2024 — внедрение llm в рекомендательные системы. большинство работ по теме объединяет излишняя академичность (слишком сложные для реализации подходы), поэтому в индустрии это направление широкого признания пока не получило. однако сегодняшняя статья вполне практическая: о способе использовать предобученные эмбеддинги рекомендательной модели в llm. идея применять большие языковые модели для генерации рекомендаций популярна, но не нова: обе технологии отлично компенсируют слабые стороны друг друга. рекомендательные модели слабы в познании мира, а llm лишены богатого коллаборативного сигнала, который хранится в эмбеддингах и весах рекомендательных нейросетей. чтобы объединить лучшее, что есть в каждом из подходов, можно интегрировать в llm предобученные эмбеддинги рекомендательной модели. адаптер, предложенный в статье, — фреймворк, который встраивает представления пользователя в attention-слои llm. как это устроено, показано на схеме. эмбеддинги пользователей берутся из рекомендательной модели, предобученной на next item prediction. hard prompt construction сопоставляет пользователя с его текстовым описание (промптом) и формулирует явное указание, что должна сделать модель, чтобы получить предсказание. а адаптер выравнивает размерность эмбеддингов пользователя (линейными слоями повышает её до внутренней размерности llm) и уточняет эмбеддинги пользователей, смешивая их с промпт-токенами. из статьи вы узнаете, как можно решить проблему distribution shift между рекомендательной моделью и llm, с учётом того, что у каждого слоя языковой модели — свой уровень абстракции и он нуждается в собственной предобработке внешних данных (эмбеддингов пользователей из рекомендательной модели). @recsyschannel разбор подготовил ❣ сергей макеев break the id-language barrier: an adaption framework for sequential recommendation один из главных трендов recsys 2024 — внедрение llm в рекомендательные системы. большинство работ по теме объединяет излишняя академичность (слишком сложные для реализации подходы), поэтому в индустрии это направление широкого признания пока не получило. однако сегодняшняя статья вполне практическая: о способе использовать предобученные эмбеддинги рекомендательной модели в llm. идея применять большие языковые модели для генерации рекомендаций популярна, но не нова: обе технологии отлично компенсируют слабые стороны друг друга. рекомендательные модели слабы в познании мира, а llm лишены богатого коллаборативного сигнала, который хранится в эмбеддингах и весах рекомендательных нейросетей. чтобы объединить лучшее, что есть в каждом из подходов, можно интегрировать в llm предобученные эмбеддинги рекомендательной модели. адаптер, предложенный в статье, — фреймворк, который встраивает представления пользователя в attention-слои llm. как это устроено, показано на схеме. эмбеддинги пользователей берутся из рекомендательной модели, предобученной на next item prediction. hard prompt construction сопоставляет пользователя с его текстовым описание (промптом) и формулирует явное указание, что должна сделать модель, чтобы получить предсказание. а адаптер выравнивает размерность эмбеддингов пользователя (линейными слоями повышает её до внутренней размерности llm) и уточняет эмбеддинги пользователей, смешивая их с промпт-токенами. из статьи вы узнаете, как можно решить проблему distribution shift между рекомендательной моделью и llm, с учётом того, что у каждого слоя языковой модели — свой уровень абстракции и он нуждается в собственной предобработке внешних данных (эмбеддингов пользователей из рекомендательной модели). @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-13T12:38:44+00:00" href="./posts/56.html">2024-12-13 12:38 UTC</a></div>
      </div>
      <div class="post-body"><strong>Break the ID-Language Barrier: An Adaption Framework for Sequential Recommendation</strong><br><br>Один из главных трендов RecSys 2024 — внедрение LLM в рекомендательные системы. Большинство работ по теме объединяет излишняя академичность (слишком сложные для реализации подходы), поэтому в индустрии это направление широкого признания пока не получило. Однако <a href="https://arxiv.org/abs/2411.18262" rel="nofollow noopener noreferrer">сегодняшняя статья</a> вполне практическая: о способе использовать предобученные эмбеддинги рекомендательной модели в LLM.<br><br>Идея применять большие языковые модели для генерации рекомендаций популярна, но не нова: обе технологии отлично компенсируют слабые стороны друг друга. Рекомендательные модели слабы в познании мира, а LLM лишены богатого коллаборативного сигнала, который хранится в эмбеддингах и весах рекомендательных нейросетей. Чтобы объединить лучшее, что есть в каждом из подходов, можно интегрировать в LLM предобученные эмбеддинги рекомендательной модели. Адаптер, предложенный в статье, — фреймворк, который встраивает представления пользователя в attention-слои LLM. <br><br>Как это устроено, показано на схеме. Эмбеддинги пользователей берутся из рекомендательной модели, предобученной на Next Item Prediction. Hard Prompt Construction сопоставляет пользователя с его текстовым описание (промптом) и формулирует явное указание, что должна сделать модель, чтобы получить предсказание. А адаптер выравнивает размерность эмбеддингов пользователя (линейными слоями повышает её до внутренней размерности LLM) и уточняет эмбеддинги пользователей, смешивая их с промпт-токенами.<br><br>Из статьи вы узнаете, как можно решить проблему distribution shift между рекомендательной моделью и LLM, с учётом того, что у каждого слоя языковой модели — свой уровень абстракции и он нуждается в собственной предобработке внешних данных (эмбеддингов пользователей из рекомендательной модели).<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/56_480.webp" srcset="../assets/media/thumbs/56_480.webp 480w, ../assets/media/56.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="56" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 439 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/56" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/56.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="55" data-search="bridging the gap: unpacking the hidden challenges in knowledge distillation for online ranking systems часть 2 далее в статье описываются нюансы реализации. авторы рассматривают: 1. два возможных подхода к дистилляции: 🔹 direct distillation — дистилляционный и основной лосс применяются к одному логиту в модели-ученике. 🔹 auxiliary distillation — в модели-ученике есть два раздельных логита: для основного и для дистилляционного лосса. схема показана на иллюстрации. второй вариант хорошо себя показал для задач предсказания ltv: в офлайн-замере rmse он на 0,4% лучше direct-подхода. это объясняется тем, что ltv — очень шумный и плохо откалиброванный таргет: большая модель выучивает биасы в данных и остаётся плохо откалиброванной. а потом передаёт свои биасы ученикам и приводит к зашумлению таргета. поэтому лучше использовать два отдельных логита. 2. какие таргеты стоит использовать для дистилляции. все таргеты можно поделить на 3 группы: engagement (например, клики), satisfaction (лайки или досмотры) и остальные. авторы отмечают, что лучше использовать только engagement и satisfaction — это даёт прирост +1,13% satisfaction +0,39% engagement относительно модели без дистилляции. добавление дополнительных таргетов влияет на общие слои и ухудшает итоговые результаты. 3. как комбинировать ученика и учителя. архитектуры ученика и учителя похожи, главное отличие — глубина и ширина внутренних слоёв. авторы провели онлайн-эксперименты для комбинаций, когда учитель больше ученика в 2 и в 4 раза: в 2 раза больший учитель позволил добиться прироста +0,42% engagement и +0,34% satisfaction относительно модели без дистилляции, в 4 раза больший учитель — +0,85% и +0,80% соответственно. но эффект масштабирования не будет продолжаться бесконечно, а увеличивать учителя ещё сильнее сложно: во-первых, его нужно обучать на больших объёмах данных, за несколько месяцев. во-вторых – поддерживать онлайн. @recsyschannel разбор подготовил ❣ петр зайдель bridging the gap: unpacking the hidden challenges in knowledge distillation for online ranking systems часть 2 далее в статье описываются нюансы реализации. авторы рассматривают: 1. два возможных подхода к дистилляции : 🔹 direct distillation — дистилляционный и основной лосс применяются к одному логиту в модели-ученике. 🔹 auxiliary distillation — в модели-ученике есть два раздельных логита: для основного и для дистилляционного лосса. схема показана на иллюстрации. второй вариант хорошо себя показал для задач предсказания ltv: в офлайн-замере rmse он на 0,4% лучше direct-подхода. это объясняется тем, что ltv — очень шумный и плохо откалиброванный таргет: большая модель выучивает биасы в данных и остаётся плохо откалиброванной. а потом передаёт свои биасы ученикам и приводит к зашумлению таргета. поэтому лучше использовать два отдельных логита. 2. какие таргеты стоит использовать для дистилляции . все таргеты можно поделить на 3 группы: engagement (например, клики), satisfaction (лайки или досмотры) и остальные. авторы отмечают, что лучше использовать только engagement и satisfaction — это даёт прирост +1,13% satisfaction +0,39% engagement относительно модели без дистилляции. добавление дополнительных таргетов влияет на общие слои и ухудшает итоговые результаты. 3. как комбинировать ученика и учителя . архитектуры ученика и учителя похожи, главное отличие — глубина и ширина внутренних слоёв. авторы провели онлайн-эксперименты для комбинаций, когда учитель больше ученика в 2 и в 4 раза: в 2 раза больший учитель позволил добиться прироста +0,42% engagement и +0,34% satisfaction относительно модели без дистилляции, в 4 раза больший учитель — +0,85% и +0,80% соответственно. но эффект масштабирования не будет продолжаться бесконечно, а увеличивать учителя ещё сильнее сложно: во-первых, его нужно обучать на больших объёмах данных, за несколько месяцев. во-вторых – поддерживать онлайн. @recsyschannel разбор подготовил ❣ петр зайдель">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-06T14:02:01+00:00" href="./posts/55.html">2024-12-06 14:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</strong><br><em>часть 2</em><br><br>Далее в <a href="https://arxiv.org/abs/2408.14678" rel="nofollow noopener noreferrer">статье</a> описываются нюансы реализации. Авторы рассматривают: <br> <br>1. <strong>Два возможных подхода к дистилляции</strong>: <br><br>🔹 Direct distillation — дистилляционный и основной лосс применяются к одному логиту в модели-ученике. <br>🔹 Auxiliary distillation — в модели-ученике есть два раздельных логита: для основного и для дистилляционного лосса. Схема показана на иллюстрации. <br><br>Второй вариант хорошо себя показал для задач предсказания LTV: в офлайн-замере RMSE он на 0,4% лучше direct-подхода. Это объясняется тем, что LTV — очень шумный и плохо откалиброванный таргет: большая модель выучивает биасы в данных и остаётся плохо откалиброванной. А потом передаёт свои биасы ученикам и приводит к зашумлению таргета. Поэтому лучше использовать два отдельных логита.<br><br>2. <strong>Какие таргеты стоит использовать для дистилляции</strong>. Все таргеты можно поделить на 3 группы: Engagement (например, клики), Satisfaction (лайки или досмотры) и остальные. Авторы отмечают, что лучше использовать только Engagement и Satisfaction — это даёт прирост +1,13% Satisfaction +0,39% Engagement относительно модели без дистилляции. Добавление дополнительных таргетов влияет на общие слои и ухудшает итоговые результаты. <br><br>3. <strong>Как комбинировать ученика и учителя</strong>. Архитектуры ученика и учителя похожи, главное отличие — глубина и ширина внутренних слоёв. Авторы провели онлайн-эксперименты для комбинаций, когда учитель больше ученика в 2 и в 4 раза: в 2 раза больший учитель позволил добиться прироста +0,42% Engagement и +0,34% Satisfaction относительно модели без дистилляции, в 4 раза больший учитель — +0,85% и +0,80% соответственно. Но эффект масштабирования не будет продолжаться бесконечно, а увеличивать учителя ещё сильнее сложно: во-первых, его нужно обучать на больших объёмах данных, за несколько месяцев. Во-вторых – поддерживать онлайн.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Петр Зайдель<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/55_480.webp" srcset="../assets/media/thumbs/55_480.webp 480w, ../assets/media/55.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="55" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 240 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/55" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/55.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="54" data-search="bridging the gap: unpacking the hidden challenges in knowledge distillation for online ranking systems часть 1 сегодняшнюю статью подготовила для recsys 2024 команда google. в ней они рассказали, как используют дистилляцию для ранжирования видео на главной youtube: не шортсов, а именно роликов на главной странице. говоря о дистилляции в cv или nlp, обычно подразумевают классический пайплайн: 🔹 обучение большой модели на некотором объёме данных; 🔹 подготовка датасета из предсказаний большой модели; 🔹 обучение маленьких моделей с использованием предсказаний большой нейросети. применять такой подход напрямую для рекомендаций не получится: поведение пользователей, набор рекомендуемых айтемов меняются со временем, иногда даже в течение дня. это значит, что один раз обучить большую модель на длинном промежутке времени и использовать её как учителя не получится, она быстро устареет. для точных рекомендаций youtube учитывает в дистилляции distribution shift: постоянно дообучает модели нейросетевого ранжирования на свежих данных. как это устроено — показано на первой схеме. большая модель-учитель непрерывно обучается на данных за период порядка месяцев. каждая порция таких предсказаний записывается в таблицу, и маленькие модели-ученики используют их в процессе дообучения. для большей эффективности используется только одна большая модель-учитель, заточенная на несколько задач сразу. маленькие же модели готовятся для более узких целей, каждая для своей. такой подход, ко всему прочему, позволяет быстрее и дешевле запускать эксперименты, поскольку для обучения учеников требуются недели, а не месяцы. @recsyschannel разбор подготовил ❣ петр зайдель bridging the gap: unpacking the hidden challenges in knowledge distillation for online ranking systems часть 1 сегодняшнюю статью подготовила для recsys 2024 команда google. в ней они рассказали, как используют дистилляцию для ранжирования видео на главной youtube: не шортсов, а именно роликов на главной странице. говоря о дистилляции в cv или nlp, обычно подразумевают классический пайплайн: 🔹 обучение большой модели на некотором объёме данных; 🔹 подготовка датасета из предсказаний большой модели; 🔹 обучение маленьких моделей с использованием предсказаний большой нейросети. применять такой подход напрямую для рекомендаций не получится: поведение пользователей, набор рекомендуемых айтемов меняются со временем, иногда даже в течение дня. это значит, что один раз обучить большую модель на длинном промежутке времени и использовать её как учителя не получится, она быстро устареет. для точных рекомендаций youtube учитывает в дистилляции distribution shift: постоянно дообучает модели нейросетевого ранжирования на свежих данных. как это устроено — показано на первой схеме. большая модель-учитель непрерывно обучается на данных за период порядка месяцев. каждая порция таких предсказаний записывается в таблицу, и маленькие модели-ученики используют их в процессе дообучения. для большей эффективности используется только одна большая модель-учитель, заточенная на несколько задач сразу. маленькие же модели готовятся для более узких целей, каждая для своей. такой подход, ко всему прочему, позволяет быстрее и дешевле запускать эксперименты, поскольку для обучения учеников требуются недели, а не месяцы. @recsyschannel разбор подготовил ❣ петр зайдель">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-06T14:01:01+00:00" href="./posts/54.html">2024-12-06 14:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</strong><br><em>часть 1</em><br><br><a href="https://arxiv.org/abs/2408.14678" rel="nofollow noopener noreferrer">Сегодняшнюю статью</a> подготовила для RecSys 2024 команда Google. В ней они рассказали, как используют дистилляцию для ранжирования видео на главной YouTube: не шортсов, а именно роликов на главной странице.  <br><br>Говоря о дистилляции в CV или NLP, обычно подразумевают классический пайплайн: <br><br>🔹 обучение большой модели на некотором объёме данных; <br>🔹 подготовка датасета из предсказаний большой модели;<br>🔹 обучение маленьких моделей с использованием предсказаний большой нейросети.<br><br>Применять такой подход напрямую для рекомендаций не получится: поведение пользователей, набор рекомендуемых айтемов меняются со временем, иногда даже в течение дня. Это значит, что один раз обучить большую модель на длинном промежутке времени и использовать её как учителя не получится, она быстро устареет. Для точных рекомендаций YouTube учитывает в дистилляции distribution shift: постоянно дообучает модели нейросетевого ранжирования на свежих данных.<br><br>Как это устроено — показано на первой схеме. Большая модель-учитель непрерывно обучается на данных за период порядка месяцев. Каждая порция таких предсказаний записывается в таблицу, и маленькие модели-ученики используют их в процессе дообучения. <br><br>Для большей эффективности используется только одна большая модель-учитель, заточенная на несколько задач сразу. Маленькие же модели готовятся для более узких целей, каждая для своей. Такой подход, ко всему прочему, позволяет быстрее и дешевле запускать эксперименты, поскольку для обучения учеников требуются недели, а не месяцы. <br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Петр Зайдель<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/54_480.webp" srcset="../assets/media/thumbs/54_480.webp 480w, ../assets/media/54.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 030 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/54" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/54.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="53" data-search="kuaiformer: transformer-based retrieval at kuaishou сегодня разбираем свежую работу от kuaishou о том, как они используют в реалтайме трансформеры для кандидатогенерации. kuaishou — суперпопулярный в китае аналог tiktok: 400 млн активных пользователей, 600+ тыс rps. в среднем один пользователь просматривает сотни видео в день. это первое внедрение в kuaishou трансформера для кандидатогенерации. по их словам, — самое успешное внедрение за последние полгода. новая модель получила название kuaiformer. опираясь на историю взаимодействия пользователя с продуктом, она помогает предсказывать следующие положительные взаимодействия (в случае kuaishou — это, например, лайк, полный просмотр видео и т. д.). pinnerformer, sasrec, bert4rec и другие похожие работы плохо улавливают разнообразные интересы пользователей, поскольку представляют их в виде одного вектора. подходы mind и comirec решают эту проблему: они умеют выделять целые кластеры интересов — так рекомендации получаются более разнообразными. kuaiformer объединяет в себе оба подхода — она умеет справляться с главными проблемами реальных рекомендательных систем: 1. за счёт применения logq-коррекции эффективно работает с большим каталогом айтемов при обучении. 2. порождает разнообразные рекомендации, поскольку выделяет не один вектор интересов, а несколько. во время обучения вероятность целевого айтема моделируется через вектор интересов, наиболее близкий к нему. 3. работает в реалтайме, но не требует большого объёма вычислительных ресурсов, несмотря на огромные rps. добиться этого помогает сворачивание последовательных кусков истории пользователя в один вектор с помощью bidirectional-трансформера: самые старые айтемы, которые были актуальны достаточно давно, сворачиваются в один вектор, а самые свежие — остаются нетронутыми. схема того, как 256 токенов превращаются в 64, показана на рисунке. @recsyschannel разбор подготовил ❣ артем матвеев kuaiformer: transformer-based retrieval at kuaishou сегодня разбираем свежую работу от kuaishou о том, как они используют в реалтайме трансформеры для кандидатогенерации. kuaishou — суперпопулярный в китае аналог tiktok: 400 млн активных пользователей, 600+ тыс rps. в среднем один пользователь просматривает сотни видео в день. это первое внедрение в kuaishou трансформера для кандидатогенерации. по их словам, — самое успешное внедрение за последние полгода. новая модель получила название kuaiformer. опираясь на историю взаимодействия пользователя с продуктом, она помогает предсказывать следующие положительные взаимодействия (в случае kuaishou — это, например, лайк, полный просмотр видео и т. д.). pinnerformer, sasrec, bert4rec и другие похожие работы плохо улавливают разнообразные интересы пользователей, поскольку представляют их в виде одного вектора. подходы mind и comirec решают эту проблему: они умеют выделять целые кластеры интересов — так рекомендации получаются более разнообразными. kuaiformer объединяет в себе оба подхода — она умеет справляться с главными проблемами реальных рекомендательных систем: 1. за счёт применения logq-коррекции эффективно работает с большим каталогом айтемов при обучении. 2. порождает разнообразные рекомендации, поскольку выделяет не один вектор интересов, а несколько. во время обучения вероятность целевого айтема моделируется через вектор интересов, наиболее близкий к нему. 3. работает в реалтайме, но не требует большого объёма вычислительных ресурсов, несмотря на огромные rps. добиться этого помогает сворачивание последовательных кусков истории пользователя в один вектор с помощью bidirectional-трансформера: самые старые айтемы, которые были актуальны достаточно давно, сворачиваются в один вектор, а самые свежие — остаются нетронутыми. схема того, как 256 токенов превращаются в 64, показана на рисунке. @recsyschannel разбор подготовил ❣ артем матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-29T11:28:15+00:00" href="./posts/53.html">2024-11-29 11:28 UTC</a></div>
      </div>
      <div class="post-body"><strong>KuaiFormer: Transformer-Based Retrieval at Kuaishou</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/html/2411.10057v1" rel="nofollow noopener noreferrer">свежую работу от Kuaishou</a> о том, как они используют в реалтайме трансформеры для кандидатогенерации.<br><br>Kuaishou — суперпопулярный в Китае аналог TikTok: 400 млн активных пользователей, 600+ тыс RPS. В среднем один пользователь просматривает сотни видео в день. <br><br>Это первое внедрение в Kuaishou трансформера для кандидатогенерации. По их словам, — самое успешное внедрение за последние полгода.<br><br>Новая модель получила название KuaiFormer. Опираясь на историю взаимодействия пользователя с продуктом, она помогает предсказывать следующие положительные взаимодействия (в случае Kuaishou — это, например, лайк, полный просмотр видео и т. д.). <br><br>PinnerFormer, SASRec, Bert4Rec и другие похожие работы плохо улавливают разнообразные интересы пользователей, поскольку представляют их в виде одного вектора. Подходы MIND и ComiRec решают эту проблему: они умеют выделять целые кластеры интересов — так рекомендации получаются более разнообразными. KuaiFormer объединяет в себе оба подхода — она умеет справляться с главными проблемами реальных рекомендательных систем: <br><br>1. За счёт применения logQ-коррекции эффективно работает с большим каталогом айтемов при обучении. <br><br>2. Порождает разнообразные рекомендации, поскольку выделяет не один вектор интересов, а несколько. Во время обучения вероятность целевого айтема моделируется через вектор интересов, наиболее близкий к нему. <br><br>3. Работает в реалтайме, но не требует большого объёма вычислительных ресурсов, несмотря на огромные RPS. Добиться этого помогает сворачивание последовательных кусков истории пользователя в один вектор с помощью bidirectional-трансформера: самые старые айтемы, которые были актуальны достаточно давно, сворачиваются в один вектор, а самые свежие — остаются нетронутыми. Схема того, как 256 токенов превращаются в 64, показана на рисунке.<br><br><em>@RecSysChannel</em><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Артем Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/53_480.webp" srcset="../assets/media/thumbs/53_480.webp 480w, ../assets/media/53.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 823 просмотров · 36 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/53" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/53.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="52" data-search="recommender systems with generative retrieval современные модели для генерации кандидатов обычно строят так: обучают энкодеры (матричные разложения, трансформеры, модели dssm-like) для получения ембеддингов запроса (пользователя) и кандидата в одном пространстве. далее по кандидатам строится ann-индекс, в котором по ембеддингу запроса ищутся ближайшие по выбранной метрике кандидаты. авторы предлагают отойти от такой схемы и научиться генерировать id айтемов напрямую моделью, которую они обучают. для этого предлагают использовать энкодер-декодер трансформенную модель на основе фреймворка t5x. остается вопрос, как закодировать айтемы для использования в трансформерной модели и как научиться напрямую предсказывать id в декодере? для этого предлагается использовать наработки из прошлой работы — semantic ids. такие id для описания айтемов обладают следующими свойствами: — иерархичность — id в начале отвечают за общие характеристики, а в конце — за более детальные; — они позволяют описывать новые айтемы, решая проблему cold-start; — при генерации можно использовать сэмплинг с температурой, что позволяет контролировать разнообразие. в статье проводят эксперимент на датасете amazon product reviews, состоящий из отзывов пользователей и описания товаров. авторы используют три категории: beauty, sports and outdoors и toys and games. для валидации и тестирования используют схему leave-one-out, когда последний товар в истории каждого пользователя используется для тестирования, а предпоследний — для валидации. такой подход много критиковали за возможные лики, но авторы используют его для сравнения с уже существующими результатами бейзлайнов. semantic ids строили следующим образом: каждый товар описывался строкой из названия, цены, бренда и категории. полученное предложение кодировали предобученной моделью sentence-t5, получая эмбеддинг размерности 768. на этих ембеддингах обучали rq-vae с размерностями слоев 512, 256, 128, активацией relu и внутренним ембеддингом 32. использовали три кодовые книги (codebooks) размером 256 ембеддингов. для стабильности обучения их инициализировали центроидами кластеров k-means на первом батче. в результате каждый айтем описывает три id, каждый из словаря размера 256. для предотвращения коллизий добавляли еще один id с порядковым номером. энкодер и декодер — трансформеры из четырёх слоев каждый с шестиголовым аттеншеном размерности 64, relu активацией, mlp на 1024 и размерностью входа 128. в словарь токенов добавили 1024 (256 × 4) токенов для кодбуков и 2000 токенов для пользователей. в итоге получилась модель на 13 миллионов параметров. каждый пример в датасете выглядит так: hash(user_id) % 2000, &lt;semantic_ids_1&gt;, … &lt;semantic_ids_n&gt; -&gt; &lt;semantic_ids_n+1&gt;. во время инференса метод показывает значительный прирост качества (recall@5, ndcg) по сравнению с бейзлайнами (sasrec, s3-rec etc). при этом нужно учитывать, что у предложенной модели намного больше параметров, чем у остальных. авторы проводят ablation study для семантических id — рассматривают варианты их замены на lsh и случайные id. в обоих случаях semantic id дает большой прирост и является важным компонентом подхода. также проводится анализ возможности модели обобщаться на новые айтемы. для этого из датасета выкидываются 5% товаров, а на инференсе задают отдельным гиперпараметром долю новых кандидатов в top-k (с совпадающими первыми тремя id) и сравнивают свою модель с knn. статья получилась во многом академичной, но она обращает внимание на важное направление, которое сейчас активно развивается. похожий подход можно использовать для кодирования айтемов для llm, чем, судя по разговорам на конференции, уже активно занимаются. также можно отметить, что в статье не раскрывается часть важных вопросов: как добавлять новые айтемы и как переобучать rq-vae (в реальных сервисах часто меняется распределение контента), а также хотелось бы увидеть сравнение на более приближенных к реальным датасетах. @recsyschannel разбор подготовил ❣ петр зайдель recommender systems with generative retrieval современные модели для генерации кандидатов обычно строят так: обучают энкодеры (матричные разложения, трансформеры, модели dssm-like) для получения ембеддингов запроса (пользователя) и кандидата в одном пространстве. далее по кандидатам строится ann-индекс, в котором по ембеддингу запроса ищутся ближайшие по выбранной метрике кандидаты. авторы предлагают отойти от такой схемы и научиться генерировать id айтемов напрямую моделью, которую они обучают. для этого предлагают использовать энкодер-декодер трансформенную модель на основе фреймворка t5x. остается вопрос, как закодировать айтемы для использования в трансформерной модели и как научиться напрямую предсказывать id в декодере? для этого предлагается использовать наработки из прошлой работы — semantic ids . такие id для описания айтемов обладают следующими свойствами: — иерархичность — id в начале отвечают за общие характеристики, а в конце — за более детальные; — они позволяют описывать новые айтемы, решая проблему cold-start; — при генерации можно использовать сэмплинг с температурой, что позволяет контролировать разнообразие. в статье проводят эксперимент на датасете amazon product reviews , состоящий из отзывов пользователей и описания товаров. авторы используют три категории: beauty , sports and outdoors и toys and games . для валидации и тестирования используют схему leave-one-out, когда последний товар в истории каждого пользователя используется для тестирования, а предпоследний — для валидации. такой подход много критиковали за возможные лики, но авторы используют его для сравнения с уже существующими результатами бейзлайнов. semantic ids строили следующим образом: каждый товар описывался строкой из названия, цены, бренда и категории. полученное предложение кодировали предобученной моделью sentence-t5, получая эмбеддинг размерности 768. на этих ембеддингах обучали rq-vae с размерностями слоев 512, 256, 128, активацией relu и внутренним ембеддингом 32. использовали три кодовые книги (codebooks) размером 256 ембеддингов. для стабильности обучения их инициализировали центроидами кластеров k-means на первом батче. в результате каждый айтем описывает три id, каждый из словаря размера 256. для предотвращения коллизий добавляли еще один id с порядковым номером. энкодер и декодер — трансформеры из четырёх слоев каждый с шестиголовым аттеншеном размерности 64, relu активацией, mlp на 1024 и размерностью входа 128. в словарь токенов добавили 1024 (256 × 4) токенов для кодбуков и 2000 токенов для пользователей. в итоге получилась модель на 13 миллионов параметров. каждый пример в датасете выглядит так: hash(user_id) % 2000, &amp;lt;semantic_ids_1&amp;gt;, … &amp;lt;semantic_ids_n&amp;gt; -&amp;gt; &amp;lt;semantic_ids_n+1&amp;gt;. во время инференса метод показывает значительный прирост качества (recall@5, ndcg) по сравнению с бейзлайнами (sasrec, s3-rec etc). при этом нужно учитывать, что у предложенной модели намного больше параметров, чем у остальных. авторы проводят ablation study для семантических id — рассматривают варианты их замены на lsh и случайные id. в обоих случаях semantic id дает большой прирост и является важным компонентом подхода. также проводится анализ возможности модели обобщаться на новые айтемы. для этого из датасета выкидываются 5% товаров, а на инференсе задают отдельным гиперпараметром долю новых кандидатов в top-k (с совпадающими первыми тремя id) и сравнивают свою модель с knn. статья получилась во многом академичной, но она обращает внимание на важное направление, которое сейчас активно развивается. похожий подход можно использовать для кодирования айтемов для llm, чем, судя по разговорам на конференции, уже активно занимаются. также можно отметить, что в статье не раскрывается часть важных вопросов: как добавлять новые айтемы и как переобучать rq-vae (в реальных сервисах часто меняется распределение контента), а также хотелось бы увидеть сравнение на более приближенных к реальным датасетах. @recsyschannel разбор подготовил ❣ петр зайдель">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-22T11:35:58+00:00" href="./posts/52.html">2024-11-22 11:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>Recommender Systems with Generative Retrieval </strong><br><br>Современные модели для генерации кандидатов обычно строят так: обучают энкодеры (матричные разложения, трансформеры, модели dssm-like) для получения ембеддингов запроса (пользователя) и кандидата в одном пространстве. Далее по кандидатам строится ANN-индекс, в котором по ембеддингу запроса ищутся ближайшие по выбранной метрике кандидаты. Авторы <a href="https://arxiv.org/abs/2305.05065" rel="nofollow noopener noreferrer">предлагают отойти</a> от такой схемы и научиться генерировать ID айтемов напрямую моделью, которую они обучают. Для этого предлагают использовать энкодер-декодер трансформенную модель на основе фреймворка T5X. <br><br>Остается вопрос, как закодировать айтемы для использования в трансформерной модели и как научиться напрямую предсказывать ID в декодере? Для этого предлагается использовать наработки из прошлой работы — <a href="https://arxiv.org/abs/2306.08121" rel="nofollow noopener noreferrer">Semantic IDs</a>. Такие ID для описания айтемов обладают следующими свойствами: <br><br>— иерархичность — ID в начале отвечают за общие характеристики, а в конце — за более детальные; <br>— они позволяют описывать новые айтемы, решая проблему cold-start;<br>— при генерации можно использовать сэмплинг с температурой, что позволяет контролировать разнообразие.<br><br>В статье проводят эксперимент на датасете <em>Amazon Product Reviews</em>, состоящий из отзывов пользователей и описания товаров. Авторы используют три категории: <em>Beauty</em>, <em>Sports and Outdoors</em> и <em>Toys and Games</em>. Для валидации и тестирования используют схему leave-one-out, когда последний товар в истории каждого пользователя используется для тестирования, а предпоследний — для валидации. Такой подход много критиковали за возможные лики, но авторы используют его для сравнения с уже существующими результатами бейзлайнов.<br><br>Semantic IDs строили следующим образом: каждый товар описывался строкой из названия, цены, бренда и категории. Полученное предложение кодировали предобученной моделью Sentence-T5, получая эмбеддинг размерности 768. На этих ембеддингах обучали RQ-VAE с размерностями слоев 512, 256, 128, активацией ReLU и внутренним ембеддингом 32. Использовали три кодовые книги (codebooks) размером 256 ембеддингов. Для стабильности обучения их инициализировали центроидами кластеров k-means на первом батче. В результате каждый айтем описывает три ID, каждый из словаря размера 256. Для предотвращения коллизий добавляли еще один ID с порядковым номером.<br><br>Энкодер и декодер — трансформеры из четырёх слоев каждый с шестиголовым аттеншеном размерности 64, ReLU активацией, MLP на 1024 и размерностью входа 128. В словарь токенов добавили 1024 (256 × 4) токенов для кодбуков и 2000 токенов для пользователей. В итоге получилась модель на 13 миллионов параметров. Каждый пример в датасете выглядит так: hash(user_id) % 2000, &lt;semantic_ids_1&gt;, … &lt;semantic_ids_n&gt; -&gt; &lt;semantic_ids_n+1&gt;. Во время инференса метод показывает значительный прирост качества (Recall@5, NDCG) по сравнению с бейзлайнами (SASRec, S3-Rec etc). При этом нужно учитывать, что у предложенной модели намного больше параметров, чем у остальных. <br><br>Авторы проводят ablation study для семантических ID — рассматривают варианты их замены на LSH и случайные ID. В обоих случаях semantic ID дает большой прирост и является важным компонентом подхода. Также проводится анализ возможности модели обобщаться на новые айтемы. Для этого из датасета выкидываются 5% товаров, а на инференсе задают отдельным гиперпараметром долю новых кандидатов в top-k (с совпадающими первыми тремя ID) и сравнивают свою модель с KNN.<br><br>Статья получилась во многом академичной, но она обращает внимание на важное направление, которое сейчас активно развивается. Похожий подход можно использовать для кодирования айтемов для LLM, чем, судя по разговорам на конференции, уже активно занимаются. Также можно отметить, что в статье не раскрывается часть важных вопросов: как добавлять новые айтемы и как переобучать RQ-VAE (в реальных сервисах часто меняется распределение контента), а также хотелось бы увидеть сравнение на более приближенных к реальным датасетах.<br><br><em>@RecSysChannel</em><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Петр Зайдель<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/52_480.webp" srcset="../assets/media/thumbs/52_480.webp 480w, ../assets/media/52.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="52" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 649 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/52" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/52.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="51" data-search="joint modeling of search and recommendations via an unified contextual recommender (unicorn) в ещё одном интересном докладе с acm recsys разработчики из netflix делятся опытом объединения моделей для персонализированного поиска и рекомендаций. в статье есть несколько предпосылок. во-первых, обслуживать одну модель в продакшене проще, чем несколько. во-вторых, качество объединённых моделей может быть выше. представленная архитектура обучается на трёх задачах: персональные рекомендации, персонализированный поиск и рекомендации к текущему видео. для этого в нейросетевой ранкер подаётся поисковой запрос, id текущей сущности (видео), id пользователя, страна и id задачи, которая решается (поиск или одно из ранжирований). также в ранкер подаётся эмбеддинг истории действий пользователя, полученный так называемой &quot;user foundation model&quot;, детали которой не раскрываются ни в тезисах с конференции, ни в ответе на прямой вопрос после устного доклада. чтобы заполнить эмбеддинги сущностей, которые отсутствуют (например, поисковые запросы в задаче рекомендаций), авторы провели серию экспериментов, по итогам которых решили, что в задаче поиска лучше вместо контекста подставлять отдельное нулевое значение, а в задаче рекомендаций — использовать название текущего видео вместо строки запроса. авторы отметили, что до внедрения этого подхода на этапе, когда пользователь вводил несколько первых букв в поисковом запросе, показывались результаты, которые не соответствовали интересам пользователя, так как поиск не был полностью персонализированным. сейчас проблему удалось решить. также в докладе подтверждают, что логика отбора кандидатов для поиска и рекомендаций оказалась ожидаемо разной. результаты — рост на 7% в офлайн-качестве в поиске и на 10% — в рекомендациях. это, по всей видимости, достигается за счёт регуляризации, возникающей при обучении на несколько задач и за счёт перехода к полной персонализации в поиске. @recsyschannel разбор подготовил ❣ владимир цепулин joint modeling of search and recommendations via an unified contextual recommender (unicorn) в ещё одном интересном докладе с acm recsys разработчики из netflix делятся опытом объединения моделей для персонализированного поиска и рекомендаций. в статье есть несколько предпосылок. во-первых, обслуживать одну модель в продакшене проще, чем несколько. во-вторых, качество объединённых моделей может быть выше. представленная архитектура обучается на трёх задачах: персональные рекомендации, персонализированный поиск и рекомендации к текущему видео. для этого в нейросетевой ранкер подаётся поисковой запрос, id текущей сущности (видео), id пользователя, страна и id задачи, которая решается (поиск или одно из ранжирований). также в ранкер подаётся эмбеддинг истории действий пользователя, полученный так называемой &amp;quot;user foundation model&amp;quot;, детали которой не раскрываются ни в тезисах с конференции, ни в ответе на прямой вопрос после устного доклада. чтобы заполнить эмбеддинги сущностей, которые отсутствуют (например, поисковые запросы в задаче рекомендаций), авторы провели серию экспериментов, по итогам которых решили, что в задаче поиска лучше вместо контекста подставлять отдельное нулевое значение, а в задаче рекомендаций — использовать название текущего видео вместо строки запроса. авторы отметили, что до внедрения этого подхода на этапе, когда пользователь вводил несколько первых букв в поисковом запросе, показывались результаты, которые не соответствовали интересам пользователя, так как поиск не был полностью персонализированным. сейчас проблему удалось решить. также в докладе подтверждают, что логика отбора кандидатов для поиска и рекомендаций оказалась ожидаемо разной. результаты — рост на 7% в офлайн-качестве в поиске и на 10% — в рекомендациях. это, по всей видимости, достигается за счёт регуляризации, возникающей при обучении на несколько задач и за счёт перехода к полной персонализации в поиске. @recsyschannel разбор подготовил ❣ владимир цепулин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-15T13:02:21+00:00" href="./posts/51.html">2024-11-15 13:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Joint Modeling of Search and Recommendations Via an Unified Contextual Recommender (UniCoRn)</strong><br><br>В ещё одном интересном <a href="https://www.arxiv.org/abs/2408.10394" rel="nofollow noopener noreferrer">докладе</a> с ACM RecSys разработчики из Netflix делятся опытом объединения моделей для персонализированного поиска и рекомендаций. В статье есть несколько предпосылок. Во-первых, обслуживать одну модель в продакшене проще, чем несколько. Во-вторых, качество объединённых моделей может быть выше. <br><br>Представленная архитектура обучается на трёх задачах: персональные рекомендации, персонализированный поиск и рекомендации к текущему видео. Для этого в нейросетевой ранкер подаётся поисковой запрос, ID текущей сущности (видео), ID пользователя, страна и ID задачи, которая решается (поиск или одно из ранжирований). Также в ранкер подаётся эмбеддинг истории действий пользователя, полученный так называемой &quot;User Foundation Model&quot;, детали которой не раскрываются ни в тезисах с конференции, ни в ответе на прямой вопрос после устного доклада.<br><br>Чтобы заполнить эмбеддинги сущностей, которые отсутствуют (например, поисковые запросы в задаче рекомендаций), авторы провели серию экспериментов, по итогам которых решили, что в задаче поиска лучше вместо контекста подставлять отдельное нулевое значение, а в задаче рекомендаций — использовать название текущего видео вместо строки запроса. <br><br>Авторы отметили, что до внедрения этого подхода на этапе, когда пользователь вводил несколько первых букв в поисковом запросе, показывались результаты, которые не соответствовали интересам пользователя, так как поиск не был полностью персонализированным. Сейчас проблему удалось решить. Также в докладе подтверждают, что логика отбора кандидатов для поиска и рекомендаций оказалась ожидаемо разной.<br><br>Результаты — рост на 7% в офлайн-качестве в поиске и на 10% — в рекомендациях. Это, по всей видимости, достигается за счёт регуляризации, возникающей при обучении на несколько задач и за счёт перехода к полной персонализации в поиске.<br><br><em>@RecSysChannel</em><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владимир Цепулин<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/51_480.webp" srcset="../assets/media/thumbs/51_480.webp 480w, ../assets/media/51.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="51" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 483 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/51" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/51.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="49" data-search="lignn: graph neural networks at linkedin один из интуитивных подходов к представлению данных в рекомендательных системах — графы. например, двудольный гетерогенный граф, где вершины — пользователи и айтемы, а рёбра — факты их взаимодействий. в теории, использование графовой структуры вводит некий inductive bias и может помочь ml-модели в выучивании закономерностей, однако на практике очень сложно внедрить графы в продакшен из-за ряда проблем: distribution shift, cold start, dynamic vocabulary. в сегодняшней статье ребята из linkedin рассказывают, как внедряли графы в свою инфраструктуру, с какими сложностями столкнулись и что усвоили. на первом рисунке — схема их гетерогенного графа для семплирования подграфов. он включает в себя несколько разнородных сущностей: в качестве вершин — пользователи, сообщения, вакансии, группы, компании. а рёбра — три типа взаимодействий: вовлечённость, родство и наличие атрибута. архитектура ml-модели представлена на втором рисунке и состоит из трёх частей: - graph engine — алгоритм для семплирования подграфов на базе открытой библиотеки deepgnn от microsoft. для семплирования используют personalized page rank (ppr). - encoder — помогает получить агрегированные представления вершин графа, опирается на graphsage; - decoder — обычный mlp, вычисляет финальную релевантность между двумя вершинами (source и target). за время работы над lignn команда смогла в 7 раз ускорить обучение графовых нейросетей, частично побороть cold start и запуститься в near-realtime сеттинге. внедрение такой архитектуры как в ранжирование, так и в кандидатогенерацию повысило продуктовые метрики: +1% откликов на вакансии, +2% ctr объявлений, +0,5% еженедельно активных пользователей, +0,2% продолжительности взаимодействия с платформой и +0,1% еженедельно активных пользователей благодаря рекомендациям. посмотреть, как работает lignn, можно в приложениях linkedin: сейчас он развёрнут в доменах feed, jobs, people recommendation и ads. @recsyschannel разбор подготовил ❣ владимир байкалов lignn: graph neural networks at linkedin один из интуитивных подходов к представлению данных в рекомендательных системах — графы. например, двудольный гетерогенный граф, где вершины — пользователи и айтемы, а рёбра — факты их взаимодействий. в теории, использование графовой структуры вводит некий inductive bias и может помочь ml-модели в выучивании закономерностей, однако на практике очень сложно внедрить графы в продакшен из-за ряда проблем: distribution shift, cold start, dynamic vocabulary. в сегодняшней статье ребята из linkedin рассказывают, как внедряли графы в свою инфраструктуру, с какими сложностями столкнулись и что усвоили. на первом рисунке — схема их гетерогенного графа для семплирования подграфов. он включает в себя несколько разнородных сущностей: в качестве вершин — пользователи, сообщения, вакансии, группы, компании. а рёбра — три типа взаимодействий: вовлечённость, родство и наличие атрибута. архитектура ml-модели представлена на втором рисунке и состоит из трёх частей: - graph engine — алгоритм для семплирования подграфов на базе открытой библиотеки deepgnn от microsoft. для семплирования используют personalized page rank (ppr). - encoder — помогает получить агрегированные представления вершин графа, опирается на graphsage ; - decoder — обычный mlp, вычисляет финальную релевантность между двумя вершинами (source и target). за время работы над lignn команда смогла в 7 раз ускорить обучение графовых нейросетей, частично побороть cold start и запуститься в near-realtime сеттинге. внедрение такой архитектуры как в ранжирование, так и в кандидатогенерацию повысило продуктовые метрики: +1% откликов на вакансии, +2% ctr объявлений, +0,5% еженедельно активных пользователей, +0,2% продолжительности взаимодействия с платформой и +0,1% еженедельно активных пользователей благодаря рекомендациям. посмотреть, как работает lignn, можно в приложениях linkedin: сейчас он развёрнут в доменах feed, jobs, people recommendation и ads. @recsyschannel разбор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-08T11:32:37+00:00" href="./posts/49.html">2024-11-08 11:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>LiGNN: Graph Neural Networks at LinkedIn</strong><br><br>Один из интуитивных подходов к представлению данных в рекомендательных системах — графы. Например, двудольный гетерогенный граф, где вершины — пользователи и айтемы, а рёбра — факты их взаимодействий. <br><br>В теории, использование графовой структуры вводит некий inductive bias и может помочь ML-модели в выучивании закономерностей, однако на практике очень сложно внедрить графы в продакшен из-за ряда проблем: distribution shift, cold start, dynamic vocabulary. В сегодняшней <a href="https://arxiv.org/abs/2402.11139" rel="nofollow noopener noreferrer">статье</a> ребята из LinkedIn рассказывают, как внедряли графы в свою инфраструктуру, с какими сложностями столкнулись и что усвоили.<br><br>На первом рисунке — схема их гетерогенного графа для семплирования подграфов. Он включает в себя несколько разнородных сущностей: в качестве вершин — пользователи, сообщения, вакансии, группы, компании. А рёбра — три типа взаимодействий: вовлечённость, родство и наличие атрибута. <br><br>Архитектура ML-модели представлена на втором рисунке и состоит из трёх частей: <br><br>- Graph Engine — алгоритм для семплирования подграфов на базе открытой библиотеки DeepGNN от Microsoft. Для семплирования используют Personalized Page Rank (PPR).<br>- Encoder — помогает получить агрегированные представления вершин графа, опирается на <a href="https://snap.stanford.edu/graphsage/" rel="nofollow noopener noreferrer">GraphSage</a>;<br>- Decoder — обычный MLP, вычисляет финальную релевантность между двумя вершинами (source и target).<br><br>За время работы над LiGNN команда смогла в 7 раз ускорить обучение графовых нейросетей, частично побороть cold start и запуститься в near-realtime сеттинге. Внедрение такой архитектуры как в ранжирование, так и в кандидатогенерацию повысило продуктовые метрики: +1% откликов на вакансии, +2% CTR объявлений, +0,5% еженедельно активных пользователей, +0,2% продолжительности взаимодействия с платформой и +0,1% еженедельно активных пользователей благодаря рекомендациям.<br><br>Посмотреть, как работает LiGNN, можно в приложениях LinkedIn: сейчас он развёрнут в доменах Feed, Jobs, People Recommendation и Ads. <br><br><em>@RecSysChannel</em><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/49_480.webp" srcset="../assets/media/thumbs/49_480.webp 480w, ../assets/media/49.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="49" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/50_480.webp" srcset="../assets/media/thumbs/50_480.webp 480w, ../assets/media/50.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="49" data-image-index="1" /></div></div>
      <div class="actions">
        <span>3 639 просмотров · 33 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/49" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/49.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="48" data-search="actions speak louder than words: trillion-parameter sequential transducers for generative recommendations у нейросетевых рекомендательных систем есть одна большая проблема — они плохо масштабируются, в то время как в nlp и cv скейлинг по размеру нейросетевых энкодеров очень хороший. выделяют несколько причин этого явления: гигантский нестационарный словарь айтемов, гетерогенная природа признаков, а также очень большой объем данных. в сегодняшней статье авторы предлагают переформулировать задачу рекомендации в генеративной постановке. для начала, они представляют данные в виде последовательности событий. вещественные фичи (счетчики и проч.) выкидываются, из взаимодействий с айтемами формируется единая последовательность, и затем в нее добавляются события изменения статической информации, такие как смена локации или изменение любого другого контекста. архитектура для генерации кандидатов выглядит довольно стандартно и похожа на sasrec или pinnerformer: представляем пользователя в виде последовательности событий (item, action), и в тех местах, где следующим событием идет положительное взаимодействие с айтемом, предсказываем, что это за айтем. а вот для ранжирования новизна достаточно серьезная: чтобы сделать модель target-aware (см. deep interest network от alibaba), понадобилось сделать более хитрую последовательность, в которой чередуются токены айтемов и действий: item_1, action_1, item_2, action_2, …. из айтем-токенов предсказывается, какое с ними произойдет действие. еще говорят, что на практике можно решать в этом месте любую многоголовую мультизадачу. важно отметить, что авторы не учат единую модель сразу на генерацию кандидатов и ранжирование, а обучают две отдельные модели. другое нововведение — отказ от софтмакса и ffn в трансформере. утверждается, что софтмакс плох для выучивания «интенсивности» чего-либо в истории пользователя. те вещественные признаки, которые были выкинуты авторами, в основном её и касались. например, сколько раз пользователь лайкал автора видеоролика, сколько раз скипал и т. д. такие признаки очень важны для качества ранжирования. то, что отказ от софтмакса эту проблему решает, видно по результатам экспериментов — действительно есть значительное улучшение результатов ранжирования при такой модификации. в итоге hstu (hierarchical sequential transduction unit, так авторы окрестили свою архитектуру) показывает отличные результаты как на публичных, так и на внутренних датасетах. еще и работает гораздо быстрее, чем прошлый dlrm подход за счет авторегрессивности и нового энкодера. результаты в онлайне тоже очень хорошие — на billion-scale платформе short-form video (предполагаем, что это рилсы) получили +12.4% относительного прироста целевой метрики в a/b-тесте. тем не менее, итоговая архитектура, которую авторы измеряют и внедряют, с точки зрения количества параметров не очень большая, где-то сотни миллионов. а вот по размеру датасета и длине истории скейлинг получился очень хороший. @recsyschannel разбор подготовил ❣ кирилл хрыльченко actions speak louder than words: trillion-parameter sequential transducers for generative recommendations у нейросетевых рекомендательных систем есть одна большая проблема — они плохо масштабируются, в то время как в nlp и cv скейлинг по размеру нейросетевых энкодеров очень хороший. выделяют несколько причин этого явления: гигантский нестационарный словарь айтемов, гетерогенная природа признаков, а также очень большой объем данных. в сегодняшней статье авторы предлагают переформулировать задачу рекомендации в генеративной постановке. для начала, они представляют данные в виде последовательности событий. вещественные фичи (счетчики и проч.) выкидываются, из взаимодействий с айтемами формируется единая последовательность, и затем в нее добавляются события изменения статической информации, такие как смена локации или изменение любого другого контекста. архитектура для генерации кандидатов выглядит довольно стандартно и похожа на sasrec или pinnerformer: представляем пользователя в виде последовательности событий (item, action), и в тех местах, где следующим событием идет положительное взаимодействие с айтемом, предсказываем, что это за айтем. а вот для ранжирования новизна достаточно серьезная: чтобы сделать модель target-aware (см. deep interest network от alibaba), понадобилось сделать более хитрую последовательность, в которой чередуются токены айтемов и действий: item_1, action_1, item_2, action_2, …. из айтем-токенов предсказывается, какое с ними произойдет действие. еще говорят, что на практике можно решать в этом месте любую многоголовую мультизадачу. важно отметить, что авторы не учат единую модель сразу на генерацию кандидатов и ранжирование, а обучают две отдельные модели. другое нововведение — отказ от софтмакса и ffn в трансформере. утверждается, что софтмакс плох для выучивания «интенсивности» чего-либо в истории пользователя. те вещественные признаки, которые были выкинуты авторами, в основном её и касались. например, сколько раз пользователь лайкал автора видеоролика, сколько раз скипал и т. д. такие признаки очень важны для качества ранжирования. то, что отказ от софтмакса эту проблему решает, видно по результатам экспериментов — действительно есть значительное улучшение результатов ранжирования при такой модификации. в итоге hstu (hierarchical sequential transduction unit, так авторы окрестили свою архитектуру) показывает отличные результаты как на публичных, так и на внутренних датасетах. еще и работает гораздо быстрее, чем прошлый dlrm подход за счет авторегрессивности и нового энкодера. результаты в онлайне тоже очень хорошие — на billion-scale платформе short-form video (предполагаем, что это рилсы) получили +12.4% относительного прироста целевой метрики в a/b-тесте. тем не менее, итоговая архитектура, которую авторы измеряют и внедряют, с точки зрения количества параметров не очень большая, где-то сотни миллионов. а вот по размеру датасета и длине истории скейлинг получился очень хороший. @recsyschannel разбор подготовил ❣ кирилл хрыльченко">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-01T15:00:28+00:00" href="./posts/48.html">2024-11-01 15:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations </strong><br><br>У нейросетевых рекомендательных систем есть одна большая проблема — они плохо масштабируются, в то время как в NLP и CV скейлинг по размеру нейросетевых энкодеров очень хороший. Выделяют несколько причин этого явления: гигантский нестационарный словарь айтемов, гетерогенная природа признаков, а также очень большой объем данных.<br><br>В <a href="https://arxiv.org/abs/2402.17152" rel="nofollow noopener noreferrer">сегодняшней статье</a> авторы предлагают переформулировать задачу рекомендации в генеративной постановке. Для начала, они представляют данные в виде последовательности событий. Вещественные фичи (счетчики и проч.) выкидываются, из взаимодействий с айтемами формируется единая последовательность, и затем в нее добавляются события изменения статической информации, такие как смена локации или изменение любого другого контекста.<br><br>Архитектура для генерации кандидатов выглядит довольно стандартно и похожа на SASRec или Pinnerformer: представляем пользователя в виде последовательности событий (item, action), и в тех местах, где следующим событием идет положительное взаимодействие с айтемом, предсказываем, что это за айтем. <br><br>А вот для ранжирования новизна достаточно серьезная: чтобы сделать модель target-aware (см. <a href="https://arxiv.org/abs/1706.06978" rel="nofollow noopener noreferrer">Deep Interest Network </a>от Alibaba), понадобилось сделать более хитрую последовательность, в которой чередуются токены айтемов и действий: item_1, action_1, item_2, action_2, …. Из айтем-токенов предсказывается, какое с ними произойдет действие. Еще говорят, что на практике можно решать в этом месте любую многоголовую мультизадачу. Важно отметить, что авторы не учат единую модель сразу на генерацию кандидатов и ранжирование, а обучают две отдельные модели.<br><br>Другое нововведение — отказ от софтмакса и FFN в трансформере. Утверждается, что софтмакс плох для выучивания «интенсивности» чего-либо в истории пользователя. Те вещественные признаки, которые были выкинуты авторами, в основном её и касались. Например, сколько раз пользователь лайкал автора видеоролика, сколько раз скипал и т. д. Такие признаки очень важны для качества ранжирования. То, что отказ от софтмакса эту проблему решает, видно по результатам экспериментов — действительно есть значительное улучшение результатов ранжирования при такой модификации.<br><br>В итоге HSTU (Hierarchical Sequential Transduction Unit, так авторы окрестили свою архитектуру) показывает отличные результаты как на публичных, так и на внутренних датасетах. Еще и работает гораздо быстрее, чем прошлый DLRM подход за счет авторегрессивности и нового энкодера. Результаты в онлайне тоже очень хорошие — на billion-scale платформе short-form video (предполагаем, что это рилсы) получили +12.4% относительного прироста целевой метрики в A/B-тесте. Тем не менее, итоговая архитектура, которую авторы измеряют и внедряют, с точки зрения количества параметров не очень большая, где-то сотни миллионов. А вот по размеру датасета и длине истории скейлинг получился очень хороший. <br><br><em>@RecSysChannel</em><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Кирилл Хрыльченко<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/48_480.webp" srcset="../assets/media/thumbs/48_480.webp 480w, ../assets/media/48.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="48" data-image-index="0" /></div></div>
      <div class="actions">
        <span>11 141 просмотров · 34 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/48" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/48.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="39" data-search="и, по традиции, слайды прилагаются 👆 #yaacmrecsys @recsyschannel и, по традиции, слайды прилагаются 👆 #yaacmrecsys @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-23T09:55:30+00:00" href="./posts/39.html">2024-10-23 09:55 UTC</a></div>
      </div>
      <div class="post-body">И, по традиции, слайды прилагаются 👆<br><br>#YaACMRecSys<br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/39_480.webp" srcset="../assets/media/thumbs/39_480.webp 480w, ../assets/media/39.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/40_480.webp" srcset="../assets/media/thumbs/40_480.webp 480w, ../assets/media/40.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/41_480.webp" srcset="../assets/media/thumbs/41_480.webp 480w, ../assets/media/41.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/42_480.webp" srcset="../assets/media/thumbs/42_480.webp 480w, ../assets/media/42.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/43_480.webp" srcset="../assets/media/thumbs/43_480.webp 480w, ../assets/media/43.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/44_480.webp" srcset="../assets/media/thumbs/44_480.webp 480w, ../assets/media/44.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/45_480.webp" srcset="../assets/media/thumbs/45_480.webp 480w, ../assets/media/45.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/46_480.webp" srcset="../assets/media/thumbs/46_480.webp 480w, ../assets/media/46.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/47_480.webp" srcset="../assets/media/thumbs/47_480.webp 480w, ../assets/media/47.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="8" /></div></div>
      <div class="actions">
        <span>2 671 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/39" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/39.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="38" data-search="интересное с acm recsys 2024, часть 3 конференция завершилась, ребята вернулись домой, но продолжают делиться с нами обзорами интересных и актуальных статей, а мы и рады их опубликовать! сегодня в эфире — довольно подробный разбор статьи text2tracks: generative track retrieval for prompt-based music recommendation. авторы рассматривают задачу рекомендации музыки на основе текстовых запросов, например, “old school rock ballads to relax”, “songs to sing in the shower” и т. д. исследуется эффективность модели на широких запросах, не подразумевающих конкретного артиста или трека. рекомендовать трек по текстовому запросу можно разными путями. например, задать вопрос языковой модели, распарсить ответ и найти треки через поиск. это может привести к галлюцинациям или неоднозначности поиска — иногда совершенно разные треки могут иметь одно название. кроме того, предсказание может занимать много времени и требовать больших вычислительных ресурсов. авторы предлагают дообучить модель типа encoder-decoder (flan-t5-base), которая по текстовому входу смогла бы генерировать идентификатор трека напрямую, вдохновившись подходом differentiable search index. основной вопрос, на который дают ответ в статье — как лучше кодировать трек? для этого сравнивают несколько подходов: — трек кодируется случайным натуральным числом, которое подаётся на вход в виде текста. например “1001”, “111” — трек котируется как два числа: id артиста и id трека внутри артиста. то есть треки артиста 1 будут представляться как “1_1”, “1_2” … для топ 50к артистов добавляют отдельные токены с словарь. — каждый трек описывается списком id на основе иерархической кластеризации контентного (названия плейлистов с треком) или коллаборативных ембеддингов (word2vec). для каждого кластера добавляется отдельный токен. эти стратегии значительно сокращают количество токенов, необходимых для представления трека по сравнению с текстовым описанием. результат получился следующий: лучше всего себя показал второй подход (id артиста + id трека в нём). при этом хуже всего себя показали подходы с кластеризацией коллаборативных ембеддингов и id трека в виде натурального числа. в качестве основных бейзлайнов авторы используют popularity, bm25 и двухбашенный энкодер (all-mpnet-base-v2), который файнтюнят c multiple negatives ranking loss. сравнивают модели на трёх датасетах: mpd 100k, cpcd и редакционные плейлисты spotify. исследователи показывают, что их модель значительно лучше бейзлайнов на всех датасетах. в будущем они планируют изучить возможности моделей с архитектурой decoder-only и использование пользовательской истории для персонализации рекомендаций. @recsyschannel #yaacmrecsys обзор подготовил ❣ пётр зайдель интересное с acm recsys 2024, часть 3 конференция завершилась, ребята вернулись домой, но продолжают делиться с нами обзорами интересных и актуальных статей, а мы и рады их опубликовать! сегодня в эфире — довольно подробный разбор статьи text2tracks: generative track retrieval for prompt-based music recommendation . авторы рассматривают задачу рекомендации музыки на основе текстовых запросов, например, “old school rock ballads to relax”, “songs to sing in the shower” и т. д. исследуется эффективность модели на широких запросах, не подразумевающих конкретного артиста или трека. рекомендовать трек по текстовому запросу можно разными путями. например, задать вопрос языковой модели, распарсить ответ и найти треки через поиск. это может привести к галлюцинациям или неоднозначности поиска — иногда совершенно разные треки могут иметь одно название. кроме того, предсказание может занимать много времени и требовать больших вычислительных ресурсов. авторы предлагают дообучить модель типа encoder-decoder (flan-t5-base), которая по текстовому входу смогла бы генерировать идентификатор трека напрямую, вдохновившись подходом differentiable search index . основной вопрос, на который дают ответ в статье — как лучше кодировать трек? для этого сравнивают несколько подходов: — трек кодируется случайным натуральным числом, которое подаётся на вход в виде текста. например “1001”, “111” — трек котируется как два числа: id артиста и id трека внутри артиста. то есть треки артиста 1 будут представляться как “1_1”, “1_2” … для топ 50к артистов добавляют отдельные токены с словарь. — каждый трек описывается списком id на основе иерархической кластеризации контентного (названия плейлистов с треком) или коллаборативных ембеддингов (word2vec). для каждого кластера добавляется отдельный токен. эти стратегии значительно сокращают количество токенов, необходимых для представления трека по сравнению с текстовым описанием. результат получился следующий: лучше всего себя показал второй подход (id артиста + id трека в нём). при этом хуже всего себя показали подходы с кластеризацией коллаборативных ембеддингов и id трека в виде натурального числа. в качестве основных бейзлайнов авторы используют popularity, bm25 и двухбашенный энкодер (all-mpnet-base-v2), который файнтюнят c multiple negatives ranking loss. сравнивают модели на трёх датасетах: mpd 100k, cpcd и редакционные плейлисты spotify. исследователи показывают, что их модель значительно лучше бейзлайнов на всех датасетах. в будущем они планируют изучить возможности моделей с архитектурой decoder-only и использование пользовательской истории для персонализации рекомендаций. @recsyschannel #yaacmrecsys обзор подготовил ❣ пётр зайдель">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-23T09:54:49+00:00" href="./posts/38.html">2024-10-23 09:54 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересное с ACM RecSys 2024, часть 3</strong><br><br>Конференция завершилась, ребята вернулись домой, но продолжают делиться с нами обзорами интересных и актуальных статей, а мы и рады их опубликовать! Сегодня в эфире — довольно подробный разбор статьи <a href="https://roegen-recsys2024.github.io/papers/enrico_text2tracks.pdf" rel="nofollow noopener noreferrer">Text2Tracks: Generative Track Retrieval for Prompt-based Music Recommendation</a>.<br><br>Авторы рассматривают задачу рекомендации музыки на основе текстовых запросов, например, “old school rock ballads to relax”, “songs to sing in the shower” и т. д. Исследуется эффективность модели на широких запросах, не подразумевающих конкретного артиста или трека. Рекомендовать трек по текстовому запросу можно разными путями. Например, задать вопрос языковой модели, распарсить ответ и найти треки через поиск. Это может привести к галлюцинациям или неоднозначности поиска — иногда совершенно разные треки могут иметь одно название. Кроме того, предсказание может занимать много времени и требовать больших вычислительных ресурсов.<br><br>Авторы предлагают дообучить модель типа encoder-decoder (flan-t5-base), которая по текстовому входу смогла бы генерировать идентификатор трека напрямую, вдохновившись подходом <a href="https://arxiv.org/abs/2202.06991" rel="nofollow noopener noreferrer">differentiable search index</a>. Основной вопрос, на который дают ответ в статье — как лучше кодировать трек? Для этого сравнивают несколько подходов:<br>— Трек кодируется случайным натуральным числом, которое подаётся на вход в виде текста. Например “1001”, “111”<br>— Трек котируется как два числа: ID артиста и ID трека внутри артиста. То есть треки артиста 1 будут представляться как “1_1”, “1_2” … Для топ 50к артистов добавляют отдельные токены с словарь.<br>— Каждый трек описывается списком ID на основе иерархической кластеризации контентного (названия плейлистов с треком) или коллаборативных ембеддингов (word2vec). Для каждого кластера добавляется отдельный токен.<br><br>Эти стратегии значительно сокращают количество токенов, необходимых для представления трека по сравнению с текстовым описанием. Результат получился следующий: лучше всего себя показал второй подход (ID артиста + ID трека в нём). При этом хуже всего себя показали подходы с кластеризацией коллаборативных ембеддингов и ID трека в виде натурального числа. <br><br>В качестве основных бейзлайнов авторы используют popularity, bm25 и двухбашенный энкодер (all-mpnet-base-v2), который файнтюнят c multiple negatives ranking loss. Сравнивают модели на трёх датасетах: MPD 100k, CPCD и редакционные плейлисты Spotify. Исследователи показывают, что их модель значительно лучше бейзлайнов на всех датасетах. В будущем они планируют изучить возможности моделей с архитектурой decoder-only и использование пользовательской истории для персонализации рекомендаций.<br><br>@RecSysChannel #YaACMRecSys<br>Обзор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Пётр Зайдель</div>
      <div class="actions">
        <span>2 513 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/38" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/38.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="34" data-search="и постеры к посту выше 👆 #yaacmrecsys @recsyschannel и постеры к посту выше 👆 #yaacmrecsys @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-17T16:29:11+00:00" href="./posts/34.html">2024-10-17 16:29 UTC</a></div>
      </div>
      <div class="post-body">И постеры к посту выше 👆<br><br>#YaACMRecSys<br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/34_480.webp" srcset="../assets/media/thumbs/34_480.webp 480w, ../assets/media/34.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="34" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/35_480.webp" srcset="../assets/media/thumbs/35_480.webp 480w, ../assets/media/35.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="34" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/36_480.webp" srcset="../assets/media/thumbs/36_480.webp 480w, ../assets/media/36.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="34" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/37_480.webp" srcset="../assets/media/thumbs/37_480.webp 480w, ../assets/media/37.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="34" data-image-index="3" /></div></div>
      <div class="actions">
        <span>2 035 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/34" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/34.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="33" data-search="интересное с acm recsys 2024, часть 2 а мы продолжаем делиться классными докладами с acm recsys — оставайтесь с нами и приглашайте друзей подписываться, чтобы не пропустить самое интересное 👀 ranking across different content types: the robust beauty of multinomial blending простая, но разумная продуктовая идея от amazon music: дать возможность продактам задавать пропорции по типу контента. для этого есть две модели: одна ранжирует карусели, а другая — контент внутри каруселей. когда карусели отранжированы, их группируют по типам контента, сэмплируют тип пропорционально весам, заданным продактам, и выбирают самую релевантную карусель из типа, выпавшего в сэмплировании. в а/б тесте этот подход сравнили с системой, которая работает на mmr-like алгоритме и получили отличный рост метрик. раньше для ранжирования авторы использовали linear thompson sampling, теперь — нейронка, которая обучается в онлайн-режиме на сабсэмпле логов с задержкой в десятки секунд. сейчас они активно пробуют sequential-модели, но пока не в проде. aie: auction information enhanced framework for ctr prediction in online advertising довольно интересный фреймворк. авторы добавили отшкалированный cpc как вес позитива в log loss, и получили рост метрик (выразившийся в деньгах) в а/б тесте. к сожалению, автор не подсказал, какими были теоретические предпосылки — судя по всему сработала какая-то очень общая интуиция. в оффлайне используют в основном auc и csauc, которые обычно нормально конвертируются в онлайн-метрики. enhancing performance and scalability of large-scale recommendation systems with jagged flash attention постер о jagged flash attention — это когда вы не используете пэдлинг в историях пользователей, а вместо этого упаковываете её в два тензора: непрерывную историю и размеры историй. авторы обещают код в опенсорсе в ближайшее время. сообщают об ускорении на инференсе, но не рассказали, на каких размерах батчей и длинах истории получены цифры. на графиках с ускорением обучения всегда пэдят до максимальной длины, а не до максимальной длины в батче, а значит, цифры завышены. но в целом история очень полезная. sliding window training: utilizing historical recommender systems data for foundation models исследователи в netflix учат базовую модель для downstream-тасков. по сути это sasrec — предсказывают next item. на разных эпохах используют разные длины истории (фиксированные на всю эпоху). для каждого пользователя выбирают одно рандомное окно указанной длины в эпоху. на вход подают просто id, action type используют только в loss, где смешивают loss’ы на разный action type с разными весами. истрия пользователя состоит из разных позитивов: клики, просмотры и т. п. авторы никак не дообучают модель в downstream-тасках, а просто подают на вход верхней модели полученные эмбеддинги. lookahead и action type во входе модели не пробовали. размерность эмбеда — 64. loss представляет собой честный softmax по всей базе. @recsyschannel #yaacmrecsys находками делился ❣ николай савушкин интересное с acm recsys 2024, часть 2 а мы продолжаем делиться классными докладами с acm recsys — оставайтесь с нами и приглашайте друзей подписываться, чтобы не пропустить самое интересное 👀 ranking across different content types: the robust beauty of multinomial blending простая, но разумная продуктовая идея от amazon music: дать возможность продактам задавать пропорции по типу контента. для этого есть две модели: одна ранжирует карусели, а другая — контент внутри каруселей. когда карусели отранжированы, их группируют по типам контента, сэмплируют тип пропорционально весам, заданным продактам, и выбирают самую релевантную карусель из типа, выпавшего в сэмплировании. в а/б тесте этот подход сравнили с системой, которая работает на mmr-like алгоритме и получили отличный рост метрик. раньше для ранжирования авторы использовали linear thompson sampling, теперь — нейронка, которая обучается в онлайн-режиме на сабсэмпле логов с задержкой в десятки секунд. сейчас они активно пробуют sequential-модели, но пока не в проде. aie: auction information enhanced framework for ctr prediction in online advertising довольно интересный фреймворк . авторы добавили отшкалированный cpc как вес позитива в log loss, и получили рост метрик (выразившийся в деньгах) в а/б тесте. к сожалению, автор не подсказал, какими были теоретические предпосылки — судя по всему сработала какая-то очень общая интуиция. в оффлайне используют в основном auc и csauc, которые обычно нормально конвертируются в онлайн-метрики. enhancing performance and scalability of large-scale recommendation systems with jagged flash attention постер о jagged flash attention — это когда вы не используете пэдлинг в историях пользователей, а вместо этого упаковываете её в два тензора: непрерывную историю и размеры историй. авторы обещают код в опенсорсе в ближайшее время. сообщают об ускорении на инференсе, но не рассказали, на каких размерах батчей и длинах истории получены цифры. на графиках с ускорением обучения всегда пэдят до максимальной длины, а не до максимальной длины в батче, а значит, цифры завышены. но в целом история очень полезная. sliding window training: utilizing historical recommender systems data for foundation models исследователи в netflix учат базовую модель для downstream-тасков. по сути это sasrec — предсказывают next item. на разных эпохах используют разные длины истории (фиксированные на всю эпоху). для каждого пользователя выбирают одно рандомное окно указанной длины в эпоху. на вход подают просто id, action type используют только в loss, где смешивают loss’ы на разный action type с разными весами. истрия пользователя состоит из разных позитивов: клики, просмотры и т. п. авторы никак не дообучают модель в downstream-тасках, а просто подают на вход верхней модели полученные эмбеддинги. lookahead и action type во входе модели не пробовали. размерность эмбеда — 64. loss представляет собой честный softmax по всей базе. @recsyschannel #yaacmrecsys находками делился ❣ николай савушкин">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-17T16:26:58+00:00" href="./posts/33.html">2024-10-17 16:26 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересное с ACM RecSys 2024, часть 2</strong><br><br>А мы продолжаем делиться классными докладами с ACM RecSys — оставайтесь с нами и приглашайте друзей подписываться, чтобы не пропустить самое интересное 👀<br><br><strong>Ranking Across Different Content Types: The Robust Beauty of Multinomial Blending</strong><br>Простая, но разумная <a href="https://arxiv.org/abs/2408.09168#:~:text=Ranking%20Across%20Different%20Content%20Types%3A%20The%20Robust%20Beauty%20of%20Multinomial%20Blending,-Jan%20Malte%20Lichtenberg&amp;text=An%20increasing%20number%20of%20media,entities%20of%20multiple%20content%20types." rel="nofollow noopener noreferrer">продуктовая идея</a> от Amazon Music: дать возможность продактам задавать пропорции по типу контента. Для этого есть две модели: одна ранжирует карусели, а другая — контент внутри каруселей. Когда карусели отранжированы, их группируют по типам контента, сэмплируют тип пропорционально весам, заданным продактам, и выбирают самую релевантную карусель из типа, выпавшего в сэмплировании. В А/Б тесте этот подход сравнили с системой, которая работает на MMR-like алгоритме и получили отличный рост метрик.<br><br>Раньше для ранжирования авторы использовали linear thompson sampling, теперь — нейронка, которая обучается в онлайн-режиме на сабсэмпле логов с задержкой в десятки секунд. Сейчас они активно пробуют sequential-модели, но пока не в проде.<br><br><strong>AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising</strong><br>Довольно <a href="https://arxiv.org/abs/2408.07907" rel="nofollow noopener noreferrer">интересный фреймворк</a>. Авторы добавили отшкалированный CPC как вес позитива в log loss, и получили рост метрик (выразившийся в деньгах) в А/Б тесте. К сожалению, автор не подсказал, какими были теоретические предпосылки — судя по всему сработала какая-то очень общая интуиция. <br><br>В оффлайне используют в основном AUC и csAUC, которые обычно нормально конвертируются в онлайн-метрики.<br><br><strong>Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention</strong><br><a href="https://arxiv.org/abs/2409.15373" rel="nofollow noopener noreferrer">Постер</a> о jagged flash attention — это когда вы не используете пэдлинг в историях пользователей, а вместо этого упаковываете её в два тензора: непрерывную историю и размеры историй.<br><br>Авторы обещают код в опенсорсе в ближайшее время. Сообщают об ускорении на инференсе, но не рассказали, на каких размерах батчей и длинах истории получены цифры. На графиках с ускорением обучения всегда пэдят до максимальной длины, а не до максимальной длины в батче, а значит, цифры завышены. Но в целом история очень полезная.<br><br><strong>Sliding Window Training: Utilizing Historical Recommender Systems Data for Foundation Models</strong><br>Исследователи в Netflix <a href="https://arxiv.org/abs/2409.14517" rel="nofollow noopener noreferrer">учат базовую модель</a> для downstream-тасков. По сути это sasrec — предсказывают next item. На разных эпохах используют разные длины истории (фиксированные на всю эпоху). Для каждого пользователя выбирают одно рандомное окно указанной длины в эпоху. На вход подают просто ID, action type используют только в loss, где смешивают loss’ы на разный action type с разными весами. Истрия пользователя состоит из разных позитивов: клики, просмотры и т. п.<br><br>Авторы никак не дообучают модель в downstream-тасках, а просто подают на вход верхней модели полученные эмбеддинги. Lookahead и action type во входе модели не пробовали. Размерность эмбеда — 64. Loss представляет собой честный softmax по всей базе.<br><br>@RecSysChannel #YaACMRecSys<br>Находками делился <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Николай Савушкин</div>
      <div class="actions">
        <span>1 964 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/33" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/33.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="29" data-search="а тут — непередаваемая атмосфера конференции и подборка постеров из сегодняшнего поста. #yaacmrecsys @recsyschannel а тут — непередаваемая атмосфера конференции и подборка постеров из сегодняшнего поста . #yaacmrecsys @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-16T14:35:45+00:00" href="./posts/29.html">2024-10-16 14:35 UTC</a></div>
      </div>
      <div class="post-body">А тут — непередаваемая атмосфера конференции и подборка постеров из сегодняшнего <a href="https://t.me/RecSysChannel/28" rel="nofollow noopener noreferrer">поста</a>.<br><br>#YaACMRecSys<br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/29_480.webp" srcset="../assets/media/thumbs/29_480.webp 480w, ../assets/media/29.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="29" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/30_480.webp" srcset="../assets/media/thumbs/30_480.webp 480w, ../assets/media/30.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="29" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/31_480.webp" srcset="../assets/media/thumbs/31_480.webp 480w, ../assets/media/31.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="29" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/32_480.webp" srcset="../assets/media/thumbs/32_480.webp 480w, ../assets/media/32.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="29" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 550 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/29" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/29.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="28" data-search="интересное с acm recsys 2024, часть 1 14 октября в бари стартовала конференция acm conference on recommender systems, которая собрала специалистов в области рекомендательных систем со всего мира — в том числе, и из яндекса. мы поговорили с ребятами, обсудили интересные доклады и постеры, которые они увидели, и спешим поделиться с вами. впереди — ещё больше впечатлений и свежих идей в постах с полей acm recsys! encouraging exploration in spotify search through query recommendations spotify рассказали о том, как внедрили саджесты запросов в поиск. они собирают запросы из разных источников: каталог (треки, артисты, альбомы, плейлисты), запросы других пользователей, запросы вида артист + mix/covers и запросы, сгенерированные llm по метаинформации. всё это отправляется в ранкер, обученный на поисковых логах, из которого пользователю показывают топ-4. результаты: +9% exploratory queries, они же — поиск нового контента, и +10% к средней длине запроса. do not wait: learning re-ranking model without user feedback at serving time in e-commerce идея статьи: если у нас есть реранжирующая функция и функция, приближающая reward по пользователю и списку, в рантайме можно «скорректировать» параметры ранжирующей функции в сторону максимизации оценивающей функции. такие корректировки можно применить несколько раз и получить ранжирующую модель, работающую лучше оригинальной. авторы утверждают, что вырастили число заказов на пользователя на 2%. клики при этом выросли всего на 0.08%, что звучит очень странно на фоне роста числа заказов. ранжирующая функция — представляет собой какой-то thompson sampling, а argmax находят с помощью &quot;reinforce like method&quot;. интересно, но практическая польза под вопросом. better generalization with semantic ids: a case study in ranking for recommendations нашумевшая статья от google deepmind. авторы предлагают закодировать контент документа в виде нескольких токенов с использованием vae и векторной квантизации — изначально подход предложили в другой статье. каждый документ представляют как набор токенов фиксированной длины. получают хитрый словарь, которым можно кодировать документы, где один документ = несколько токенов. утверждают, что работает не сильно хуже, чем обучаемые id (без коллизий), но матрица эмбеддингов при этом радикально меньше, а коллизии в ней имеют семантический смысл. подход работает лучше контентных эмбеддингов, так как векторы для токенов обучается e2e c верхней моделью на рекомендательную задачу. авторы также пробовали обучать небольшую голову поверх контентных эмбеддингов, но получилось хуже по качеству. кроме того, в силу иерархической природы токенов, на них можно обучать декодер, что было описано в ещё одной статье. @recsyschannel #yaacmrecsys находками делились ❣ николай савушкин и пётр зайдель интересное с acm recsys 2024, часть 1 14 октября в бари стартовала конференция acm conference on recommender systems, которая собрала специалистов в области рекомендательных систем со всего мира — в том числе, и из яндекса. мы поговорили с ребятами, обсудили интересные доклады и постеры, которые они увидели, и спешим поделиться с вами. впереди — ещё больше впечатлений и свежих идей в постах с полей acm recsys! encouraging exploration in spotify search through query recommendations spotify рассказали о том, как внедрили саджесты запросов в поиск. они собирают запросы из разных источников: каталог (треки, артисты, альбомы, плейлисты), запросы других пользователей, запросы вида артист + mix/covers и запросы, сгенерированные llm по метаинформации. всё это отправляется в ранкер, обученный на поисковых логах, из которого пользователю показывают топ-4. результаты: +9% exploratory queries, они же — поиск нового контента, и +10% к средней длине запроса. do not wait: learning re-ranking model without user feedback at serving time in e-commerce идея статьи: если у нас есть реранжирующая функция и функция, приближающая reward по пользователю и списку, в рантайме можно «скорректировать» параметры ранжирующей функции в сторону максимизации оценивающей функции. такие корректировки можно применить несколько раз и получить ранжирующую модель, работающую лучше оригинальной. авторы утверждают, что вырастили число заказов на пользователя на 2%. клики при этом выросли всего на 0.08%, что звучит очень странно на фоне роста числа заказов. ранжирующая функция — представляет собой какой-то thompson sampling, а argmax находят с помощью &amp;quot;reinforce like method&amp;quot;. интересно, но практическая польза под вопросом. better generalization with semantic ids: a case study in ranking for recommendations нашумевшая статья от google deepmind. авторы предлагают закодировать контент документа в виде нескольких токенов с использованием vae и векторной квантизации — изначально подход предложили в другой статье. каждый документ представляют как набор токенов фиксированной длины. получают хитрый словарь, которым можно кодировать документы, где один документ = несколько токенов. утверждают, что работает не сильно хуже, чем обучаемые id (без коллизий), но матрица эмбеддингов при этом радикально меньше, а коллизии в ней имеют семантический смысл. подход работает лучше контентных эмбеддингов, так как векторы для токенов обучается e2e c верхней моделью на рекомендательную задачу. авторы также пробовали обучать небольшую голову поверх контентных эмбеддингов, но получилось хуже по качеству. кроме того, в силу иерархической природы токенов, на них можно обучать декодер, что было описано в ещё одной статье. @recsyschannel #yaacmrecsys находками делились ❣ николай савушкин и пётр зайдель">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-16T14:33:44+00:00" href="./posts/28.html">2024-10-16 14:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересное с ACM RecSys 2024, часть 1</strong><br><br>14 октября в Бари стартовала конференция <a href="https://recsys.acm.org/" rel="nofollow noopener noreferrer">ACM Conference on Recommender Systems,</a> которая собрала специалистов в области рекомендательных систем со всего мира — в том числе, и из Яндекса. Мы поговорили с ребятами, обсудили интересные доклады и постеры, которые они увидели, и спешим поделиться с вами. Впереди — ещё больше впечатлений и свежих идей в постах с полей ACM RecSys!<br><br><strong>Encouraging Exploration in Spotify Search through Query Recommendations</strong><br>Spotify <a href="https://dl.acm.org/doi/10.1145/3640457.3688035" rel="nofollow noopener noreferrer">рассказали</a> о том, как внедрили саджесты запросов в поиск. Они собирают запросы из разных источников: каталог (треки, артисты, альбомы, плейлисты), запросы других пользователей, запросы вида артист + mix/covers и запросы, сгенерированные LLM по метаинформации. Всё это отправляется в ранкер, обученный на поисковых логах, из которого пользователю показывают топ-4. Результаты: +9% exploratory queries, они же — поиск нового контента, и +10% к средней длине запроса.<br><br><strong>Do Not Wait: Learning Re-Ranking Model Without User Feedback At Serving Time in E-Commerce</strong><br>Идея <a href="https://dl.acm.org/doi/fullHtml/10.1145/3640457.3688165" rel="nofollow noopener noreferrer">статьи:</a> если у нас есть реранжирующая функция и функция, приближающая reward по пользователю и списку, в рантайме можно «скорректировать» параметры ранжирующей функции в сторону максимизации оценивающей функции. Такие корректировки можно применить несколько раз и получить ранжирующую модель, работающую лучше оригинальной. <br><br>Авторы утверждают, что вырастили число заказов на пользователя на 2%. Клики при этом выросли всего на 0.08%, что звучит очень странно на фоне роста числа заказов. Ранжирующая функция — представляет собой какой-то thompson sampling, а Argmax находят с помощью &quot;reinforce like method&quot;. Интересно, но практическая польза под вопросом.<br><br><strong>Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations</strong><br>Нашумевшая <a href="https://dl.acm.org/doi/10.1145/3640457.3688190" rel="nofollow noopener noreferrer">статья от Google DeepMind.</a> Авторы предлагают закодировать контент документа в виде нескольких токенов с использованием VAE и векторной квантизации — изначально подход предложили в <a href="https://arxiv.org/abs/2203.01941" rel="nofollow noopener noreferrer">другой статье.</a> Каждый документ представляют как набор токенов фиксированной длины. Получают хитрый словарь, которым можно кодировать документы, где  один документ = несколько токенов. Утверждают, что работает не сильно хуже, чем обучаемые ID (без коллизий), но матрица эмбеддингов при этом радикально меньше, а коллизии в ней имеют семантический смысл.<br><br>Подход работает лучше контентных эмбеддингов, так как векторы для токенов обучается e2e c верхней моделью на рекомендательную задачу. Авторы также пробовали обучать небольшую голову поверх контентных эмбеддингов, но получилось хуже по качеству. Кроме того, в силу иерархической природы токенов, на них можно обучать декодер, что было описано в ещё <a href="https://arxiv.org/pdf/2305.05065" rel="nofollow noopener noreferrer">одной статье.</a><br><br>@RecSysChannel #YaACMRecSys<br>Находками делились <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Николай Савушкин и Пётр Зайдель</div>
      <div class="actions">
        <span>1 736 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/28" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/28.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="27" data-search="density weighting for multi-interest personalized recommendation сегодняшняя статья от google посвящена репрезентации юзера в виде нескольких векторов, каждый из которых отображает некоторый интерес пользователя. авторы отмечают, что использование нескольких представлений пользователя (multiple user representations, mur) вместо одного представления (single user representation, sur) показало свою эффективность. однако при таком подходе огромную роль играет неравномерное распределение интересов пользователя. mur фокусируется на головных, самых популярных интересах, из-за чего возникает просадка на более редких, хвостовых. чтобы решить эту проблему, авторы предлагают схему итеративного взвешивания плотности (iterative density weighting scheme, idw). она должна помочь справиться с дисбалансом данных и улучшить рекомендации для хвостовых элементов. idw корректирует представление предметов в пространстве, уменьшая влияние дисбалансированных данных и улучшая кластеризацию элементов. вот как устроена idw: 1. модель анализирует плотность предметов в пространстве представлений — то есть то, насколько близко друг к другу они находятся. плотность рассчитывается для каждого предмета, чтобы понять, каких элементов слишком много (высокая плотность) и каких мало (низкая плотность). 2. на основе плотности модель корректирует веса предметов — элементы с высокой плотностью получают меньшие веса, а с низкой плотностью — большие. это позволяет модели меньше фокусироваться на популярных предметах и больше — на редких. 3. idw — это итеративный процесс. на каждом этапе веса пересчитываются с учётом изменённой структуры представлений предметов. этот процесс повторяется до тех пор, пока модель не стабилизируется и не достигнет сбалансированного состояния. 4. после корректировки весов для предметов, модель дополнительно оптимизируется, чтобы улучшить рекомендации для хвостовых элементов, не снижая производительность для популярных предметов. по результатам экспериментов на бенчмарках — movielens 1m, kindle store, а также clothing, shoes and jewelry — схема idw показала значительное улучшение рекомендаций. в метрике hr@20 для movielens 1m модель с idw достигла 82,65% против 80,82% у обычной mur, а в ndcg@20 — 49,67% против 47,72% у mur. на датасете kindle store hr@20 составил 65.24% с idw против 64,66% у mur, а ndcg@20 — 32.25%, тогда как у mur было 31,16%. на датасете clothing, shoes and jewelry метрика hr@20 у idw составила 37,34% (33.92% у mur), а ndcg@20 — 16.33% (14.90% у mur). @recsyschannel разбор подготовил ❣ степан макаренко density weighting for multi-interest personalized recommendation сегодняшняя статья от google посвящена репрезентации юзера в виде нескольких векторов, каждый из которых отображает некоторый интерес пользователя. авторы отмечают, что использование нескольких представлений пользователя (multiple user representations, mur) вместо одного представления (single user representation, sur) показало свою эффективность. однако при таком подходе огромную роль играет неравномерное распределение интересов пользователя. mur фокусируется на головных, самых популярных интересах, из-за чего возникает просадка на более редких, хвостовых. чтобы решить эту проблему, авторы предлагают схему итеративного взвешивания плотности (iterative density weighting scheme, idw). она должна помочь справиться с дисбалансом данных и улучшить рекомендации для хвостовых элементов. idw корректирует представление предметов в пространстве, уменьшая влияние дисбалансированных данных и улучшая кластеризацию элементов. вот как устроена idw: 1. модель анализирует плотность предметов в пространстве представлений — то есть то, насколько близко друг к другу они находятся. плотность рассчитывается для каждого предмета, чтобы понять, каких элементов слишком много (высокая плотность) и каких мало (низкая плотность). 2. на основе плотности модель корректирует веса предметов — элементы с высокой плотностью получают меньшие веса, а с низкой плотностью — большие. это позволяет модели меньше фокусироваться на популярных предметах и больше — на редких. 3. idw — это итеративный процесс. на каждом этапе веса пересчитываются с учётом изменённой структуры представлений предметов. этот процесс повторяется до тех пор, пока модель не стабилизируется и не достигнет сбалансированного состояния. 4. после корректировки весов для предметов, модель дополнительно оптимизируется, чтобы улучшить рекомендации для хвостовых элементов, не снижая производительность для популярных предметов. по результатам экспериментов на бенчмарках — movielens 1m, kindle store, а также clothing, shoes and jewelry — схема idw показала значительное улучшение рекомендаций. в метрике hr@20 для movielens 1m модель с idw достигла 82,65% против 80,82% у обычной mur, а в ndcg@20 — 49,67% против 47,72% у mur. на датасете kindle store hr@20 составил 65.24% с idw против 64,66% у mur, а ndcg@20 — 32.25%, тогда как у mur было 31,16%. на датасете clothing, shoes and jewelry метрика hr@20 у idw составила 37,34% (33.92% у mur), а ndcg@20 — 16.33% (14.90% у mur). @recsyschannel разбор подготовил ❣ степан макаренко">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T09:56:10+00:00" href="./posts/27.html">2024-10-04 09:56 UTC</a></div>
      </div>
      <div class="post-body"><strong>Density Weighting for Multi-Interest Personalized Recommendation</strong><br><br>Сегодняшняя <a href="https://arxiv.org/abs/2308.01563" rel="nofollow noopener noreferrer">статья от Google</a> посвящена репрезентации юзера в виде нескольких векторов, каждый из которых отображает некоторый интерес пользователя. <br><br>Авторы отмечают, что использование нескольких представлений пользователя (multiple user representations, MUR) вместо одного представления (single user representation, SUR) показало свою эффективность. Однако при таком подходе огромную роль играет неравномерное распределение интересов пользователя. MUR фокусируется на головных, самых популярных интересах, из-за чего возникает просадка на более редких, хвостовых. <br><br>Чтобы решить эту проблему, авторы предлагают схему итеративного взвешивания плотности (iterative density weighting scheme, IDW). Она должна помочь справиться с дисбалансом данных и улучшить рекомендации для хвостовых элементов. IDW корректирует представление предметов в пространстве, уменьшая влияние дисбалансированных данных и улучшая кластеризацию элементов. Вот как устроена IDW:<br><br>1. Модель анализирует плотность предметов в пространстве представлений — то есть то, насколько близко друг к другу они находятся. Плотность рассчитывается для каждого предмета, чтобы понять, каких элементов слишком много (высокая плотность) и каких мало (низкая плотность).<br><br>2. На основе плотности модель корректирует веса предметов — элементы с высокой плотностью получают меньшие веса, а с низкой плотностью — большие. Это позволяет модели меньше фокусироваться на популярных предметах и больше — на редких.<br><br>3. IDW — это итеративный процесс. На каждом этапе веса пересчитываются с учётом изменённой структуры представлений предметов. Этот процесс повторяется до тех пор, пока модель не стабилизируется и не достигнет сбалансированного состояния.<br><br>4. После корректировки весов для предметов, модель дополнительно оптимизируется, чтобы улучшить рекомендации для хвостовых элементов, не снижая производительность для популярных предметов.<br><br>По результатам экспериментов на бенчмарках — MovieLens 1M, Kindle Store, а также Clothing, Shoes and Jewelry — схема IDW показала значительное улучшение рекомендаций. В метрике HR@20 для MovieLens 1M модель с IDW достигла 82,65% против 80,82% у обычной MUR, а в NDCG@20 — 49,67% против 47,72% у MUR. <br><br>На датасете Kindle Store HR@20 составил 65.24% с IDW против 64,66% у MUR, а NDCG@20 — 32.25%, тогда как у MUR было 31,16%. <br><br>На датасете Clothing, Shoes and Jewelry метрика HR@20 у IDW составила 37,34% (33.92% у MUR), а NDCG@20 — 16.33% (14.90% у MUR).<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Степан Макаренко</div>
      <div class="actions">
        <span>2 137 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/27" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/27.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="26" data-search="efficient retrieval with learned similarities сегодня обсуждаем статью от microsoft и meta* об эффективном retrieval с обучаемыми функциями близости. исторически нишу функций близости в retrieval занимали косинусные близости — скалярные произведения над нормализованными векторами. но в последнее время популярность стали набирать обучаемые функции близости. например, они допускают сопоставление одному запросу нескольких эмбеддингов, чтобы лучше улавливать редкие и противоречивые интересы пользователей. также можно использовать нейросети над векторами запроса и векторами айтема и делать многие другие интересные вещи. однако с эффективностью этих решений есть проблемы. чтобы повысить эффективность обучаемых функций близости, в статье используют mixture-of-logits как универсальный аппроксиматор и предлагают методы его ускорения для получения достаточно точной аппроксимации топ-k соседей. в экспериментах подход авторов обгоняет бейслайны почти в 100 раз по времени работы и при этом достигает 99% полноты/рекола. по сути обучаемые функции близости — это попытка повысить экспрессивность функции, моделирующей релевантность. в идеале у нас есть большая матрица релевантностей, где каждому запросу сопоставлена вероятность. при переходе к обучаемым эмбеддингам мы оцениваем релевантность как dot product и пытаемся оценить логарифм матрицы низкоранговым разложением. если изначальная матрица имела высокий ранг, мы не получим точного разложения. простым решением является увеличение размерности эмбеддингов, но оно может привести к проблемам с памятью и оверфиту. а mol позволяет аппроксимировать матрицу релевантности, сохранив ее ранг. также авторы предлагают методы ускорения этой конструкции: несколько вариаций алгоритмов, о которых можно узнать больше из полного текста статьи, и оптимизацию gpu-кернелов вкупе с использованием более масштабных датасетов, что сделает разницу между традиционными подходами и обучаемыми функциями близости ещё более выраженной. @recsyschannel разбор подготовил ❣ сергей макеев — meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф efficient retrieval with learned similarities сегодня обсуждаем статью от microsoft и meta* об эффективном retrieval с обучаемыми функциями близости. исторически нишу функций близости в retrieval занимали косинусные близости — скалярные произведения над нормализованными векторами. но в последнее время популярность стали набирать обучаемые функции близости. например, они допускают сопоставление одному запросу нескольких эмбеддингов, чтобы лучше улавливать редкие и противоречивые интересы пользователей. также можно использовать нейросети над векторами запроса и векторами айтема и делать многие другие интересные вещи. однако с эффективностью этих решений есть проблемы. чтобы повысить эффективность обучаемых функций близости, в статье используют mixture-of-logits как универсальный аппроксиматор и предлагают методы его ускорения для получения достаточно точной аппроксимации топ-k соседей. в экспериментах подход авторов обгоняет бейслайны почти в 100 раз по времени работы и при этом достигает 99% полноты/рекола. по сути обучаемые функции близости — это попытка повысить экспрессивность функции, моделирующей релевантность. в идеале у нас есть большая матрица релевантностей, где каждому запросу сопоставлена вероятность. при переходе к обучаемым эмбеддингам мы оцениваем релевантность как dot product и пытаемся оценить логарифм матрицы низкоранговым разложением. если изначальная матрица имела высокий ранг, мы не получим точного разложения. простым решением является увеличение размерности эмбеддингов, но оно может привести к проблемам с памятью и оверфиту. а mol позволяет аппроксимировать матрицу релевантности, сохранив ее ранг. также авторы предлагают методы ускорения этой конструкции: несколько вариаций алгоритмов, о которых можно узнать больше из полного текста статьи, и оптимизацию gpu-кернелов вкупе с использованием более масштабных датасетов, что сделает разницу между традиционными подходами и обучаемыми функциями близости ещё более выраженной. @recsyschannel разбор подготовил ❣ сергей макеев — meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-27T12:19:16+00:00" href="./posts/26.html">2024-09-27 12:19 UTC</a></div>
      </div>
      <div class="post-body"><strong>Efficient Retrieval with Learned Similarities</strong><br><br>Сегодня <a href="https://arxiv.org/abs/2407.15462" rel="nofollow noopener noreferrer">обсуждаем статью</a> от Microsoft и Meta* об эффективном retrieval с обучаемыми функциями близости. Исторически нишу функций близости в retrieval занимали косинусные близости — скалярные произведения над нормализованными векторами. Но в последнее время популярность стали набирать обучаемые функции близости. Например, они допускают сопоставление одному запросу нескольких эмбеддингов, чтобы лучше улавливать редкие и противоречивые интересы пользователей. Также можно использовать нейросети над векторами запроса и векторами айтема и делать многие другие интересные вещи. Однако с эффективностью этих решений есть проблемы. <br><br>Чтобы повысить эффективность обучаемых функций близости, в статье используют Mixture-of-Logits как универсальный аппроксиматор и предлагают методы его ускорения для получения достаточно точной аппроксимации топ-k соседей. В экспериментах подход авторов обгоняет бейслайны почти в 100 раз по времени работы и при этом достигает 99% полноты/рекола. <br><br>По сути обучаемые функции близости — это попытка повысить экспрессивность функции, моделирующей релевантность. В идеале у нас есть большая матрица релевантностей, где каждому запросу сопоставлена вероятность. При переходе к обучаемым эмбеддингам мы оцениваем релевантность как dot product и пытаемся оценить логарифм матрицы низкоранговым разложением. <br><br>Если изначальная матрица имела высокий ранг, мы не получим точного разложения. Простым решением является увеличение размерности эмбеддингов, но оно может привести к проблемам с памятью и оверфиту. А MoL позволяет аппроксимировать матрицу релевантности, сохранив ее ранг.<br><br>Также авторы предлагают методы ускорения этой конструкции: несколько вариаций алгоритмов, о которых можно узнать больше из полного текста статьи, и оптимизацию GPU-кернелов вкупе с использованием более масштабных датасетов, что сделает разницу между традиционными подходами и обучаемыми функциями близости ещё более выраженной.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<br><br>—<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/26_480.webp" srcset="../assets/media/thumbs/26_480.webp 480w, ../assets/media/26.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="26" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 158 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/26" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/26.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="25" data-search="diffusion model for slate recommendation разбираем свежую статью от spotify о диффузионных моделях для рекомендации слейтов. слейт — что-то вроде списка событий произвольной длины. самый простой пример слейта — плейлист с музыкой. особенность таких рекомендаций в том, что помимо генерации кандидатов для показа необходимо ещё и ранжирование, так мы получаем пачку, внутри которой объекты должны быть расположены в определённом порядке. в идеале с приносящем как можно больше удовольствия пользователю. ранжирование объектов — важная подзадача в рамках рекомендации слейтов, и для её решения авторы статьи используют отдельные модели, но в данной работе концентрируются на retrieval-части, рассказывая, чем хороши диффузионки. в качестве примеров похожих работ они ссылаются на 2 статьи от google, за 2015 и 2019 год, где для решения аналогичной задачи используется rl. проблема в том, что айтемы в слейте являются в rl-подходе независимыми событиями. это упрощает обучение, но такой подход не совсем корректен, так как зависимость между соседними айтемами в слейте все же есть, что приводит к проблемам с качеством генерации. исследователи из spotify утверждают, что генеративный подход (а именно — диффузионные модели) могут работать лучше, чем rl-like подходы. диффузионки могут сделать слейт разнообразнее благодаря неявному пониманию, что айтемы не должны быть слишком похожими. также авторы замечают, что диффузионные модели помогают бороться с popularity bias’ом и включать в подборки менее очевидные треки даже без явного обучения под эту задачу. также авторы делают conditioning на весь контекст — профиль и запрос пользователя. опционально в контекст добавляют отдельные айтемы из слейта, которые уже были предсказаны. по словам исследователей, такой метод даёт значимо лучшие результаты, чем rl и другие привычные решения в сфере рекомендаций слейтов. @recsyschannel разбор подготовил ❣ владимир байкалов diffusion model for slate recommendation разбираем свежую статью от spotify о диффузионных моделях для рекомендации слейтов. слейт — что-то вроде списка событий произвольной длины. самый простой пример слейта — плейлист с музыкой. особенность таких рекомендаций в том, что помимо генерации кандидатов для показа необходимо ещё и ранжирование, так мы получаем пачку, внутри которой объекты должны быть расположены в определённом порядке. в идеале с приносящем как можно больше удовольствия пользователю. ранжирование объектов — важная подзадача в рамках рекомендации слейтов, и для её решения авторы статьи используют отдельные модели, но в данной работе концентрируются на retrieval-части, рассказывая, чем хороши диффузионки. в качестве примеров похожих работ они ссылаются на 2 статьи от google, за 2015 и 2019 год, где для решения аналогичной задачи используется rl. проблема в том, что айтемы в слейте являются в rl-подходе независимыми событиями. это упрощает обучение, но такой подход не совсем корректен, так как зависимость между соседними айтемами в слейте все же есть, что приводит к проблемам с качеством генерации. исследователи из spotify утверждают, что генеративный подход (а именно — диффузионные модели) могут работать лучше, чем rl-like подходы. диффузионки могут сделать слейт разнообразнее благодаря неявному пониманию, что айтемы не должны быть слишком похожими. также авторы замечают, что диффузионные модели помогают бороться с popularity bias’ом и включать в подборки менее очевидные треки даже без явного обучения под эту задачу. также авторы делают conditioning на весь контекст — профиль и запрос пользователя. опционально в контекст добавляют отдельные айтемы из слейта, которые уже были предсказаны. по словам исследователей, такой метод даёт значимо лучшие результаты, чем rl и другие привычные решения в сфере рекомендаций слейтов. @recsyschannel разбор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-20T08:48:57+00:00" href="./posts/25.html">2024-09-20 08:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Diffusion Model for Slate Recommendation</strong><br><br>Разбираем свежую <a href="https://www.arxiv.org/abs/2408.06883" rel="nofollow noopener noreferrer">статью от Spotify</a> о диффузионных моделях для рекомендации слейтов. Слейт — что-то вроде списка событий произвольной длины. Самый простой пример слейта — плейлист с музыкой. Особенность таких рекомендаций в том, что помимо генерации кандидатов для показа необходимо ещё и ранжирование, так мы получаем пачку, внутри которой объекты должны быть расположены в определённом порядке. В идеале с приносящем как можно больше удовольствия пользователю. <br><br>Ранжирование объектов — важная подзадача в рамках рекомендации слейтов, и для её решения авторы статьи используют отдельные модели, но в данной работе концентрируются на retrieval-части, рассказывая, чем хороши диффузионки. В качестве примеров похожих работ они ссылаются на 2 статьи от Google, за <a href="https://arxiv.org/abs/1512.01124" rel="nofollow noopener noreferrer">2015</a> и <a href="https://arxiv.org/abs/1905.12767" rel="nofollow noopener noreferrer">2019</a> год, где для решения аналогичной задачи используется RL. Проблема в том, что айтемы в слейте являются в RL-подходе независимыми событиями. Это упрощает обучение, но такой подход не совсем корректен, так как зависимость между соседними айтемами в слейте все же есть, что приводит к проблемам с качеством генерации.<br><br>Исследователи из Spotify утверждают, что генеративный подход (а именно — диффузионные модели) могут работать лучше, чем RL-like подходы. Диффузионки могут сделать слейт разнообразнее благодаря неявному пониманию, что айтемы не должны быть слишком похожими. Также авторы замечают, что диффузионные модели помогают бороться с popularity bias’ом и включать в подборки менее очевидные треки даже без явного обучения под эту задачу. Также авторы делают conditioning на весь контекст — профиль и запрос пользователя. Опционально в контекст добавляют отдельные айтемы из слейта, которые уже были предсказаны.<br><br>По словам исследователей, такой метод даёт значимо лучшие результаты, чем RL и другие привычные решения в сфере рекомендаций слейтов.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/25_480.webp" srcset="../assets/media/thumbs/25_480.webp 480w, ../assets/media/25.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="25" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 034 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/25" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/25.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="24" data-search="multi-objective learning to rank by model distillation свежая статья от airbnb о том, как совместить дистилляцию и мультитаск-обучение. задача ранжирования заключается не только в предсказании конверсии, но и её исхода. например, человек может вернуть заказ или оставить отзыв. также он может обратиться в сервис, если возникнут проблемы. это важно в контексте долгосрочного роста платформы. по словам авторов, традиционные подходы к ранжированию страдают из-за дисбаланса данных, которых для разных задач может быть разное количество — много для ctr и конверсий, но мало для возвратов, обращений в сервис и т. д. учесть эти факторы сложно. решение — end-to-end multi-objective, совмещенный с дистилляцией. важно, чтобы при этом инференс и обучение не занимали слишком много времени. модели объединяют через дистилляцию, после чего добавляют механизм самодистилляции — он даёт лучшую воспроизводимость и помогает побороть cold start при переобучении. так у авторов получилось решать ad-hoc бизнес-задачи, связанные с недифференцированными функционалами. лоссы с разных подзадач объединяют с помощью скаляризации. отдельные лейблы, возникающие из-за того, что задач несколько, агрегируются в один, т. н. софт-лейбл. при такой постановке происходит шеринг нижних слоев (возможно, не только их). это оказывает положительное влияние на задачи с разреженными данными, но может работать плохо, если между задачами слабая корреляция — так часто бывает в маркетплейсах. даже несмотря на это, по словам авторов, выгода от шеринга есть. подбирать веса, с которыми суммируются лоссы, при таком подходе дорого, плюс есть риск переобучения. если подбирать веса каждый раз, когда обновляется какая-то модель — затраты вырастут. избавиться от онлайн-тюнинга весов и сбалансировать обучение на цели с разным количеством данных помогает дистилляция, а дальнейшая самодистилляция закрепляет и усиливает эффект. исследователи получили рост метрики ndcg на 1,1% в офлайн-экспериментах и +0,37% бронирований (cvr) в a/b-тестах. @recsyschannel разбор подготовил ❣ сергей макеев multi-objective learning to rank by model distillation свежая статья от airbnb о том, как совместить дистилляцию и мультитаск-обучение. задача ранжирования заключается не только в предсказании конверсии, но и её исхода. например, человек может вернуть заказ или оставить отзыв. также он может обратиться в сервис, если возникнут проблемы. это важно в контексте долгосрочного роста платформы. по словам авторов, традиционные подходы к ранжированию страдают из-за дисбаланса данных, которых для разных задач может быть разное количество — много для ctr и конверсий, но мало для возвратов, обращений в сервис и т. д. учесть эти факторы сложно. решение — end-to-end multi-objective, совмещенный с дистилляцией. важно, чтобы при этом инференс и обучение не занимали слишком много времени. модели объединяют через дистилляцию, после чего добавляют механизм самодистилляции — он даёт лучшую воспроизводимость и помогает побороть cold start при переобучении. так у авторов получилось решать ad-hoc бизнес-задачи, связанные с недифференцированными функционалами. лоссы с разных подзадач объединяют с помощью скаляризации. отдельные лейблы, возникающие из-за того, что задач несколько, агрегируются в один, т. н. софт-лейбл. при такой постановке происходит шеринг нижних слоев (возможно, не только их). это оказывает положительное влияние на задачи с разреженными данными, но может работать плохо, если между задачами слабая корреляция — так часто бывает в маркетплейсах. даже несмотря на это, по словам авторов, выгода от шеринга есть. подбирать веса, с которыми суммируются лоссы, при таком подходе дорого, плюс есть риск переобучения. если подбирать веса каждый раз, когда обновляется какая-то модель — затраты вырастут. избавиться от онлайн-тюнинга весов и сбалансировать обучение на цели с разным количеством данных помогает дистилляция, а дальнейшая самодистилляция закрепляет и усиливает эффект. исследователи получили рост метрики ndcg на 1,1% в офлайн-экспериментах и +0,37% бронирований (cvr) в a/b-тестах. @recsyschannel разбор подготовил ❣ сергей макеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-06T08:09:46+00:00" href="./posts/24.html">2024-09-06 08:09 UTC</a></div>
      </div>
      <div class="post-body"><strong>Multi-objective Learning to Rank by Model Distillation</strong><br><br>Свежая <a href="https://arxiv.org/abs/2407.07181" rel="nofollow noopener noreferrer">статья</a> от Airbnb о том, как совместить дистилляцию и мультитаск-обучение. Задача ранжирования заключается не только в предсказании конверсии, но и её исхода. Например, человек может вернуть заказ или оставить отзыв. Также он может обратиться в сервис, если возникнут проблемы. Это важно в контексте долгосрочного роста платформы.<br><br>По словам авторов, традиционные подходы к ранжированию страдают из-за дисбаланса данных, которых для разных задач может быть разное количество — много для CTR и конверсий, но мало для возвратов, обращений в сервис и т. д. Учесть эти факторы сложно.<br><br>Решение — end-to-end multi-objective, совмещенный с дистилляцией. Важно, чтобы при этом инференс и обучение не занимали слишком много времени. Модели объединяют через дистилляцию, после чего добавляют механизм самодистилляции — он даёт лучшую воспроизводимость и помогает побороть cold start при переобучении. Так у авторов получилось решать ad-hoc бизнес-задачи, связанные с недифференцированными функционалами.<br><br>Лоссы с разных подзадач объединяют с помощью скаляризации. Отдельные лейблы, возникающие из-за того, что задач несколько, агрегируются в один, т. н. софт-лейбл. При такой постановке происходит шеринг нижних слоев (возможно, не только их). Это оказывает положительное влияние на задачи с разреженными данными, но может работать плохо, если между задачами слабая корреляция — так часто бывает в маркетплейсах. Даже несмотря на это, по словам авторов, выгода от шеринга есть.<br><br>Подбирать веса, с которыми суммируются лоссы, при таком подходе дорого, плюс есть риск переобучения. Если подбирать веса каждый раз, когда обновляется какая-то модель — затраты вырастут. Избавиться от онлайн-тюнинга весов и сбалансировать обучение на цели с разным количеством данных помогает дистилляция, а дальнейшая самодистилляция закрепляет и усиливает эффект. Исследователи получили рост метрики nDCG на 1,1% в офлайн-экспериментах и +0,37% бронирований (CVR) в A/B-тестах.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Сергей Макеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/24_480.webp" srcset="../assets/media/thumbs/24_480.webp 480w, ../assets/media/24.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="24" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 563 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/24" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/24.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="23" data-search="linr: model based neural retrieval on gpus at linkedin в свежей статье от linkedin рассказывается о linr, первом индустриальном алгоритме генерации кандидатов, работающем на gpu. linr может поддерживать индексы, включающие миллиарды потенциальных кандидатов. авторы рассказали, как они разрабатывали свой обучаемый дифференцируемый индекс и с какими трудностями им пришлось столкнуться. в linr построение индекса рассматривается как процесс обучения, из-за чего и представления объектов, и веса, с помощью которых происходит формирование выдачи, интегрируются в одну модель. отличительным аспектом статьи является использование честного knn для формирования выдачи вместо широко распространенного ann (approximate nearest neighbor). в работе также описывается способ интегрирования фильтраций, основанных на логических правилах, в стадию скоринга объектов индекса, что позволяет повысить качество финальной выдачи. в статье авторы предлагают три версии алгоритма для генерации кандидатов: — скоринг всех объектов с последующей фильтрацией. первое предложенное решение, которое может работать неоптимально, особенно в случаях с большим процентом отфильтрованных объектов (low-pass-rate сценарии). — предварительная фильтрация объектов с последующим скорингом. улучшенная версия первого подхода, которая решает его проблемы и увеличивает метрики качества выдачи. — дополнительное улучшение второго подхода с использованием квантизации. предлагается использовать две стадии выбора кандидатов после фильтрации: первичный выбор подмножества объектов на основе квантизованных представлений и более гранулярная фильтрация оставшихся объектов для получения финальной выдачи. внедрение linr позволило увеличить количество ежедневных уникальных пользователей на 3%. об особенностях архитектуры модели, квантизации, инфраструктуре, экспериментах, результатах и других фишках можно подробнее прочитать в самой статье, а я хотел бы остановиться на главных тезисах, которые исследователи постулируют в работе: — с оффлайн инференсом и обновлением модели вы теряете свежих кандидатов, а следовательно, и качество. real-time обновление linr увеличило качество всего пайплайна на 6%. — предварительная фильтрация, в отличие от пост-фильтрации, которая может тратить слоты кандидатов на нерелевантные объекты, помогает повысить качество модели. — по умолчанию tf или pytorch не приспособлены для реализации retrieval-моделей, из-за чего такие решения будут довольно медленными без дополнительных оптимизаций. — имплементация собственного cuda-ядра для второй и третьей версий модели позволила получить значительное преимущество в скорости (к сожалению, авторы не поделились кодом самого ядра). @recsyschannel разбор подготовил ❣ владимир байкалов linr: model based neural retrieval on gpus at linkedin в свежей статье от linkedin рассказывается о linr, первом индустриальном алгоритме генерации кандидатов, работающем на gpu. linr может поддерживать индексы, включающие миллиарды потенциальных кандидатов. авторы рассказали, как они разрабатывали свой обучаемый дифференцируемый индекс и с какими трудностями им пришлось столкнуться. в linr построение индекса рассматривается как процесс обучения, из-за чего и представления объектов, и веса, с помощью которых происходит формирование выдачи, интегрируются в одну модель. отличительным аспектом статьи является использование честного knn для формирования выдачи вместо широко распространенного ann (approximate nearest neighbor). в работе также описывается способ интегрирования фильтраций, основанных на логических правилах, в стадию скоринга объектов индекса, что позволяет повысить качество финальной выдачи. в статье авторы предлагают три версии алгоритма для генерации кандидатов: — скоринг всех объектов с последующей фильтрацией . первое предложенное решение, которое может работать неоптимально, особенно в случаях с большим процентом отфильтрованных объектов (low-pass-rate сценарии). — предварительная фильтрация объектов с последующим скорингом . улучшенная версия первого подхода, которая решает его проблемы и увеличивает метрики качества выдачи. — дополнительное улучшение второго подхода с использованием квантизации . предлагается использовать две стадии выбора кандидатов после фильтрации: первичный выбор подмножества объектов на основе квантизованных представлений и более гранулярная фильтрация оставшихся объектов для получения финальной выдачи. внедрение linr позволило увеличить количество ежедневных уникальных пользователей на 3% . об особенностях архитектуры модели, квантизации, инфраструктуре, экспериментах, результатах и других фишках можно подробнее прочитать в самой статье , а я хотел бы остановиться на главных тезисах, которые исследователи постулируют в работе: — с оффлайн инференсом и обновлением модели вы теряете свежих кандидатов, а следовательно, и качество. real-time обновление linr увеличило качество всего пайплайна на 6%. — предварительная фильтрация, в отличие от пост-фильтрации, которая может тратить слоты кандидатов на нерелевантные объекты, помогает повысить качество модели. — по умолчанию tf или pytorch не приспособлены для реализации retrieval-моделей, из-за чего такие решения будут довольно медленными без дополнительных оптимизаций. — имплементация собственного cuda-ядра для второй и третьей версий модели позволила получить значительное преимущество в скорости (к сожалению, авторы не поделились кодом самого ядра). @recsyschannel разбор подготовил ❣ владимир байкалов">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-08-30T11:27:17+00:00" href="./posts/23.html">2024-08-30 11:27 UTC</a></div>
      </div>
      <div class="post-body"><strong>LiNR: Model Based Neural Retrieval on GPUs at LinkedIn</strong><br><br>В свежей <a href="https://arxiv.org/abs/2407.13218" rel="nofollow noopener noreferrer">статье</a> от LinkedIn рассказывается о LiNR, первом индустриальном алгоритме генерации кандидатов, работающем на GPU. LiNR может поддерживать индексы, включающие миллиарды потенциальных кандидатов. Авторы рассказали, как они разрабатывали свой обучаемый дифференцируемый индекс и с какими трудностями им пришлось столкнуться. <br><br>В LiNR построение индекса рассматривается как процесс обучения, из-за чего и представления объектов, и веса, с помощью которых происходит формирование выдачи, интегрируются в одну модель. Отличительным аспектом статьи является использование честного KNN для формирования выдачи вместо широко распространенного ANN (Approximate Nearest Neighbor). В работе также описывается способ интегрирования фильтраций, основанных на логических правилах, в стадию скоринга объектов индекса, что позволяет повысить качество финальной выдачи. <br><br>В статье авторы предлагают три версии алгоритма для генерации кандидатов:<br>— <strong>Скоринг всех объектов с последующей фильтрацией</strong>. Первое предложенное решение, которое может работать неоптимально, особенно в случаях с большим процентом отфильтрованных объектов (low-pass-rate сценарии).<br>— <strong>Предварительная фильтрация объектов с последующим скорингом</strong>. Улучшенная версия первого подхода, которая решает его проблемы и увеличивает метрики качества выдачи.<br>— <strong>Дополнительное улучшение второго подхода с использованием квантизации</strong>. Предлагается использовать две стадии выбора кандидатов после фильтрации: первичный выбор подмножества объектов на основе квантизованных представлений и более гранулярная фильтрация оставшихся объектов для получения финальной выдачи.<br><br>Внедрение LiNR позволило увеличить количество ежедневных уникальных пользователей на <strong>3%</strong>. Об особенностях архитектуры модели, квантизации, инфраструктуре, экспериментах, результатах и других фишках можно подробнее прочитать в <a href="https://arxiv.org/pdf/2407.13218" rel="nofollow noopener noreferrer">самой статье</a>, а я хотел бы остановиться на главных тезисах, которые исследователи постулируют в работе: <br><br>— С оффлайн инференсом и обновлением модели вы теряете свежих кандидатов, а следовательно, и качество. Real-time обновление LiNR увеличило качество <strong>всего пайплайна</strong> на 6%.<br>— Предварительная фильтрация, в отличие от пост-фильтрации, которая может тратить слоты кандидатов на нерелевантные объекты, помогает повысить качество модели.<br>— По умолчанию TF или PyTorch не приспособлены для реализации retrieval-моделей, из-за чего такие решения будут довольно медленными без дополнительных оптимизаций.<br>— Имплементация собственного CUDA-ядра для второй и третьей версий модели позволила получить значительное преимущество в скорости (к сожалению, авторы не поделились кодом самого ядра).<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владимир Байкалов<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/23_480.webp" srcset="../assets/media/thumbs/23_480.webp 480w, ../assets/media/23.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="23" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 175 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/23" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/23.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="22" data-search="законы масштабирования в больших моделях последовательных рекомендаций авторы из wechat и tencent разбирались, работают ли законы масштабирования нейросетей для рекомендательных систем. главный вопрос — есть ли улучшение качества рекомендаций при увеличении количества обучаемых параметров? короткий ответ — да. известно, что рост количества параметров моделей иногда коррелирует с улучшением качества решаемых задач. больше всего работ посвящено законам масштабирования в языковых моделях. в них определяется эмпирическая зависимость функции потерь на отложенной выборке от характеристик обучения. обычно рассматривают параметры энкодеров и/или декодеров. для nlp зависимость в логарифмических координатах получается линейной. в работе об sr авторы масштабировали декодер трансформера и вносили изменения в стратегии обучения, чтобы получить закон масштабирования для рекомендательных систем: — для слоёв в начале последовательности декодер-блоков применяли больший dropout-rate, а для слоёв на вершине — меньший, что позволило избежать оверфита. — сначала обучались с adam до полной сходимости, а потом брали чекпоинты, с которых продолжали обучение при помощи sgd, потому что несмотря на лучшую сходимость, итоговый минимум у adam получался хуже. историю взаимодействий форматировали как хронологическую последовательность id айтемов. то есть задача решалась так же, как в случае с языковыми моделями. исследователи не брали другую информацию (например, текст айтема), так как хотели изучить работу закона с т. з. поведения пользователя. модели увеличивали до 0,8b параметров, сравнивая эффекты в разных диапазонах размеров. оказалось, закон масштабирования работает для sr-моделей даже в сценариях с ограниченным количеством данных. авторы показали преимущество больших моделей и на сложных задачах рекомендаций: cold start, long tail, определяли траектории пользователей и смотрели, что происходит при мультидоменном трансфере — во всех случаях масштабирование улучшало результаты. @recsyschannel разбор подготовил ❣ артем матвеев законы масштабирования в больших моделях последовательных рекомендаций авторы из wechat и tencent разбирались , работают ли законы масштабирования нейросетей для рекомендательных систем. главный вопрос — есть ли улучшение качества рекомендаций при увеличении количества обучаемых параметров? короткий ответ — да. известно, что рост количества параметров моделей иногда коррелирует с улучшением качества решаемых задач. больше всего работ посвящено законам масштабирования в языковых моделях. в них определяется эмпирическая зависимость функции потерь на отложенной выборке от характеристик обучения. обычно рассматривают параметры энкодеров и/или декодеров. для nlp зависимость в логарифмических координатах получается линейной. в работе об sr авторы масштабировали декодер трансформера и вносили изменения в стратегии обучения, чтобы получить закон масштабирования для рекомендательных систем: — для слоёв в начале последовательности декодер-блоков применяли больший dropout-rate, а для слоёв на вершине — меньший, что позволило избежать оверфита. — сначала обучались с adam до полной сходимости, а потом брали чекпоинты, с которых продолжали обучение при помощи sgd, потому что несмотря на лучшую сходимость, итоговый минимум у adam получался хуже. историю взаимодействий форматировали как хронологическую последовательность id айтемов. то есть задача решалась так же, как в случае с языковыми моделями. исследователи не брали другую информацию (например, текст айтема), так как хотели изучить работу закона с т. з. поведения пользователя. модели увеличивали до 0,8b параметров, сравнивая эффекты в разных диапазонах размеров. оказалось, закон масштабирования работает для sr-моделей даже в сценариях с ограниченным количеством данных. авторы показали преимущество больших моделей и на сложных задачах рекомендаций: cold start, long tail, определяли траектории пользователей и смотрели, что происходит при мультидоменном трансфере — во всех случаях масштабирование улучшало результаты. @recsyschannel разбор подготовил ❣ артем матвеев">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-08-16T14:59:28+00:00" href="./posts/22.html">2024-08-16 14:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>Законы масштабирования в больших моделях последовательных рекомендаций</strong><br><br>Авторы из WeChat и Tencent <a href="https://arxiv.org/pdf/2311.11351" rel="nofollow noopener noreferrer">разбирались</a>, работают ли законы масштабирования нейросетей для рекомендательных систем. Главный вопрос — есть ли улучшение качества рекомендаций при увеличении количества обучаемых параметров? Короткий ответ — да.<br><br>Известно, что рост количества параметров моделей иногда коррелирует с улучшением качества решаемых задач. Больше всего работ посвящено законам масштабирования в языковых моделях. В них определяется эмпирическая зависимость функции потерь на отложенной выборке от характеристик обучения. Обычно рассматривают параметры энкодеров и/или декодеров. Для NLP зависимость в логарифмических координатах получается линейной. <br><br>В работе об SR авторы масштабировали декодер трансформера и вносили изменения в стратегии обучения, чтобы получить закон масштабирования для рекомендательных систем:<br>— Для слоёв в начале последовательности декодер-блоков применяли больший dropout-rate, а для слоёв на вершине — меньший, что позволило избежать оверфита. <br>— Сначала обучались с Adam до полной сходимости, а потом брали чекпоинты, с которых продолжали обучение при помощи SGD, потому что несмотря на лучшую сходимость, итоговый минимум у Adam получался хуже.<br><br>Историю взаимодействий форматировали как хронологическую последовательность ID айтемов. То есть задача решалась так же, как в случае с языковыми моделями. Исследователи не брали другую информацию (например, текст айтема), так как хотели изучить работу закона с т. з. поведения пользователя. Модели увеличивали до 0,8B параметров, сравнивая эффекты в разных диапазонах размеров. <br><br>Оказалось, закон масштабирования работает для SR-моделей даже в сценариях с ограниченным количеством данных. Авторы показали преимущество больших моделей и на сложных задачах рекомендаций: cold start, long tail, определяли траектории пользователей и смотрели, что происходит при мультидоменном трансфере — во всех случаях масштабирование улучшало результаты.<br><br>@RecSysChannel<br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Артем Матвеев<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/22_480.webp" srcset="../assets/media/thumbs/22_480.webp 480w, ../assets/media/22.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="22" data-image-index="0" /></div></div>
      <div class="actions">
        <span>11 302 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/RecSysChannel/22" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/22.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 66, "media": [{"kind": "photo", "path": "../assets/media/66.jpg", "thumb": "../assets/media/thumbs/66_480.webp", "size": 100606, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/67.jpg", "thumb": "../assets/media/thumbs/67_480.webp", "size": 119074, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/68.jpg", "thumb": "../assets/media/thumbs/68_480.webp", "size": 118558, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/69.jpg", "thumb": "../assets/media/thumbs/69_480.webp", "size": 133352, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/70.jpg", "thumb": "../assets/media/thumbs/70_480.webp", "size": 134016, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/71.jpg", "thumb": "../assets/media/thumbs/71_480.webp", "size": 114576, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/72.jpg", "thumb": "../assets/media/thumbs/72_480.webp", "size": 124270, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/73.jpg", "thumb": "../assets/media/thumbs/73_480.webp", "size": 114421, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/74.jpg", "thumb": "../assets/media/thumbs/74_480.webp", "size": 114818, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/75.jpg", "thumb": "../assets/media/thumbs/75_480.webp", "size": 82143, "mime": "image/jpeg", "name": null}]}, {"id": 65, "media": [{"kind": "photo", "path": "../assets/media/65.jpg", "thumb": "../assets/media/thumbs/65_480.webp", "size": 48678, "mime": "image/jpeg", "name": null}]}, {"id": 64, "media": [{"kind": "photo", "path": "../assets/media/64.jpg", "thumb": "../assets/media/thumbs/64_480.webp", "size": 80418, "mime": "image/jpeg", "name": null}]}, {"id": 63, "media": []}, {"id": 62, "media": [{"kind": "photo", "path": "../assets/media/62.jpg", "thumb": "../assets/media/thumbs/62_480.webp", "size": 91856, "mime": "image/jpeg", "name": null}]}, {"id": 61, "media": []}, {"id": 60, "media": [{"kind": "photo", "path": "../assets/media/60.jpg", "thumb": "../assets/media/thumbs/60_480.webp", "size": 82084, "mime": "image/jpeg", "name": null}]}, {"id": 59, "media": []}, {"id": 58, "media": []}, {"id": 57, "media": []}, {"id": 56, "media": [{"kind": "photo", "path": "../assets/media/56.jpg", "thumb": "../assets/media/thumbs/56_480.webp", "size": 91149, "mime": "image/jpeg", "name": null}]}, {"id": 55, "media": [{"kind": "photo", "path": "../assets/media/55.jpg", "thumb": "../assets/media/thumbs/55_480.webp", "size": 64536, "mime": "image/jpeg", "name": null}]}, {"id": 54, "media": [{"kind": "photo", "path": "../assets/media/54.jpg", "thumb": "../assets/media/thumbs/54_480.webp", "size": 31504, "mime": "image/jpeg", "name": null}]}, {"id": 53, "media": [{"kind": "photo", "path": "../assets/media/53.jpg", "thumb": "../assets/media/thumbs/53_480.webp", "size": 72511, "mime": "image/jpeg", "name": null}]}, {"id": 52, "media": [{"kind": "photo", "path": "../assets/media/52.jpg", "thumb": "../assets/media/thumbs/52_480.webp", "size": 24221, "mime": "image/jpeg", "name": null}]}, {"id": 51, "media": [{"kind": "photo", "path": "../assets/media/51.jpg", "thumb": "../assets/media/thumbs/51_480.webp", "size": 64477, "mime": "image/jpeg", "name": null}]}, {"id": 49, "media": [{"kind": "photo", "path": "../assets/media/49.jpg", "thumb": "../assets/media/thumbs/49_480.webp", "size": 73875, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/50.jpg", "thumb": "../assets/media/thumbs/50_480.webp", "size": 61194, "mime": "image/jpeg", "name": null}]}, {"id": 48, "media": [{"kind": "photo", "path": "../assets/media/48.jpg", "thumb": "../assets/media/thumbs/48_480.webp", "size": 24221, "mime": "image/jpeg", "name": null}]}, {"id": 39, "media": [{"kind": "photo", "path": "../assets/media/39.jpg", "thumb": "../assets/media/thumbs/39_480.webp", "size": 99933, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/40.jpg", "thumb": "../assets/media/thumbs/40_480.webp", "size": 147221, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/41.jpg", "thumb": "../assets/media/thumbs/41_480.webp", "size": 143207, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/42.jpg", "thumb": "../assets/media/thumbs/42_480.webp", "size": 95502, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/43.jpg", "thumb": "../assets/media/thumbs/43_480.webp", "size": 98438, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/44.jpg", "thumb": "../assets/media/thumbs/44_480.webp", "size": 89700, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/45.jpg", "thumb": "../assets/media/thumbs/45_480.webp", "size": 86053, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/46.jpg", "thumb": "../assets/media/thumbs/46_480.webp", "size": 132521, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/47.jpg", "thumb": "../assets/media/thumbs/47_480.webp", "size": 77685, "mime": "image/jpeg", "name": null}]}, {"id": 38, "media": []}, {"id": 34, "media": [{"kind": "photo", "path": "../assets/media/34.jpg", "thumb": "../assets/media/thumbs/34_480.webp", "size": 126980, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/35.jpg", "thumb": "../assets/media/thumbs/35_480.webp", "size": 83571, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/36.jpg", "thumb": "../assets/media/thumbs/36_480.webp", "size": 43903, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/37.jpg", "thumb": "../assets/media/thumbs/37_480.webp", "size": 65633, "mime": "image/jpeg", "name": null}]}, {"id": 33, "media": []}, {"id": 29, "media": [{"kind": "photo", "path": "../assets/media/29.jpg", "thumb": "../assets/media/thumbs/29_480.webp", "size": 216737, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/30.jpg", "thumb": "../assets/media/thumbs/30_480.webp", "size": 194953, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/31.jpg", "thumb": "../assets/media/thumbs/31_480.webp", "size": 158449, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/32.jpg", "thumb": "../assets/media/thumbs/32_480.webp", "size": 84968, "mime": "image/jpeg", "name": null}]}, {"id": 28, "media": []}, {"id": 27, "media": []}, {"id": 26, "media": [{"kind": "photo", "path": "../assets/media/26.jpg", "thumb": "../assets/media/thumbs/26_480.webp", "size": 54527, "mime": "image/jpeg", "name": null}]}, {"id": 25, "media": [{"kind": "photo", "path": "../assets/media/25.jpg", "thumb": "../assets/media/thumbs/25_480.webp", "size": 69162, "mime": "image/jpeg", "name": null}]}, {"id": 24, "media": [{"kind": "photo", "path": "../assets/media/24.jpg", "thumb": "../assets/media/thumbs/24_480.webp", "size": 58747, "mime": "image/jpeg", "name": null}]}, {"id": 23, "media": [{"kind": "photo", "path": "../assets/media/23.jpg", "thumb": "../assets/media/thumbs/23_480.webp", "size": 24221, "mime": "image/jpeg", "name": null}]}, {"id": 22, "media": [{"kind": "photo", "path": "../assets/media/22.jpg", "thumb": "../assets/media/thumbs/22_480.webp", "size": 90482, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "Рекомендательная [RecSys Channel]", "username": "RecSysChannel", "channel": "RecSysChannel", "last_sync_utc": "2026-02-04T08:06:57Z", "posts_count": 104, "last_seen_message_id": 217, "stats": {"new": 104, "updated": 2, "media_downloaded": 104}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
